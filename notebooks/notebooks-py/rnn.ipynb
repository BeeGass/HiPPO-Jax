{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.ops\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax import optim\n",
    "from flax.linen.recurrent import RNNCellBase\n",
    "\n",
    "import optax\n",
    "\n",
    "import numpy as np  # convention: original numpy\n",
    "\n",
    "from typing import Any, Callable, Sequence, Optional, Tuple, Union\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 5\n",
    "rng, key2, key3, key4, key5 = jax.random.split(key, num=num_copies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_batch(nest, batch_size: Optional[int]):\n",
    "    \"\"\"Adds a batch dimension at axis 0 to the leaves of a nested structure.\"\"\"\n",
    "    broadcast = lambda x: jnp.broadcast_to(x, (batch_size,) + x.shape)\n",
    "\n",
    "    return jax.tree_map(broadcast, nest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(RNNCellBase):\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "\n",
    "    @nn.compact()\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            W_xh = x_{t} @ W_{xh} - multiply the previous hidden state with\n",
    "            W_hh = H_{t-1} @ W_{hh} + b_{h} - this a linear layer\n",
    "\n",
    "            H_{t} = f_{w}(H_{t-1}, x)\n",
    "            H_{t} = tanh(H_{t-1} @ W_{hh}) + (x_{t} @ W_{xh})\n",
    "\n",
    "        Args:\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        ht_1, _ = carry\n",
    "\n",
    "        h_t = self.rnn_update(input, ht_1)\n",
    "\n",
    "        return (h_t, h_t), h_t\n",
    "\n",
    "    def rnn_update(self, input, ht_1):\n",
    "\n",
    "        W_hh = nn.Dense(self.hidden_size)(ht_1)\n",
    "        W_xh = nn.Dense(self.hidden_size)(input)\n",
    "        h_t = nn.relu(W_hh + W_xh)  # H_{t} = tanh(H_{t-1} @ W_{hh}) + (x_{t} @ W_{xh})\n",
    "\n",
    "        return h_t\n",
    "\n",
    "    def initial_state(self, batch_size: Optional[int]):\n",
    "        state = jnp.zeros([self.hidden_size])\n",
    "        if batch_size is not None:\n",
    "            state = add_batch(state, batch_size)\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(RNNCellBase):\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "\n",
    "    @nn.compact()\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            i_{t} = sigmoid((W_{ii} @ x_{t} + b_{ii}) + (W_{hi} @ h_{t-1} + b_{hi}))\n",
    "            f_{t} = sigmoid((W_{if} @ x_{t} + b_{if}) + (W_{hf} @ h_{t-1} + b_{hf}))\n",
    "            g_{t} = tanh((W_{ig} @ x_{t} + b_{ig}) + (W_{hg} @ h_{t-1} + b_{hg}))\n",
    "            o_{t} = sigmoid((W_{io} @ x_{t} + b_{io}) + (W_{ho} @ h_{t-1} + b_{ho}))\n",
    "            c_{t} = f_{t} * c_{t-1} + i_{t} * g_{t}\n",
    "            h_{t} = o_{t} * tanh(c_{t})\n",
    "\n",
    "        Args:\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        ht_1, ct_1 = carry\n",
    "\n",
    "        c_t, h_t = self.rnn_update(input, ht_1, ct_1)\n",
    "\n",
    "        return (h_t, c_t), h_t\n",
    "\n",
    "    def rnn_update(self, input, ht_1, ct_1):\n",
    "\n",
    "        i_ta = nn.Dense(self.hidden_size)(input)\n",
    "        i_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        i_t = nn.sigmoid(i_ta + i_tb)  # input gate\n",
    "\n",
    "        o_ta = nn.Dense(self.hidden_size)(input)\n",
    "        o_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        o_t = nn.sigmoid(o_ta + o_tb)  # output gate\n",
    "\n",
    "        f_ia = nn.Dense(self.hidden_size)(\n",
    "            input\n",
    "        )  # b^{f}_{i} + \\sum\\limits_{j} U^{f}_{i, j} x^{t}_{j}\n",
    "        f_ib = nn.Dense(self.hidden_size)(\n",
    "            ht_1\n",
    "        )  # \\sum\\limits_{j} W^{f}_{i, j} h^{(t-1)}_{j}\n",
    "        f_i = nn.sigmoid(f_ia + f_ib)  # forget gate\n",
    "\n",
    "        g_ia = nn.Dense(self.hidden_size)(\n",
    "            input\n",
    "        )  # b^{g}_{i} + \\sum\\limits_{j} U^{g}_{i, j} x^{t}_{j}\n",
    "        g_ib = nn.Dense(self.hidden_size)(\n",
    "            ht_1\n",
    "        )  # \\sum\\limits_{j} W^{g}_{i, j} h^{(t-1)}_{j}\n",
    "        g_i = nn.tanh(g_ia + g_ib)  # (external) input gate\n",
    "\n",
    "        c_t = (f_i * ct_1) + (i_t * g_i)  # internal cell state update\n",
    "\n",
    "        h_t = o_t * nn.tanh(c_t)  # hidden state update\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "    def initial_state(self, batch_size: Optional[int]):\n",
    "        state = jnp.zeros([self.hidden_size])\n",
    "        if batch_size is not None:\n",
    "            state = add_batch(state, batch_size)\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(RNNCellBase):\n",
    "    hidden_size: int\n",
    "\n",
    "    @nn.compact()\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            z_t = sigmoid((W_{iz} @ x_{t} + b_{iz}) + (W_{hz} @ h_{t-1} + b_{hz}))\n",
    "            r_t = sigmoid((W_{ir} @ x_{t} + b_{ir}) + (W_{hr} @ h_{t-1} + b_{hr}))\n",
    "            g_t = tanh(((W_{ig} @ x_{t} + b_{ig}) + r_t) * (W_{hg} @ h_{t-1} + b_{hg}))\n",
    "            h_t = (z_t * h_{t-1}) + ((1 - z_t) * g_i)\n",
    "\n",
    "        Args:\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        ht_1 = carry\n",
    "\n",
    "        h_t = self.rnn_update(input, ht_1)\n",
    "\n",
    "        return (h_t, h_t), h_t\n",
    "\n",
    "    def rnn_update(self, input, ht_1):\n",
    "\n",
    "        z_ta = nn.Dense(self.hidden_size)(input)\n",
    "        z_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        z_t = nn.sigmoid(z_ta + z_tb)  # reset gate\n",
    "\n",
    "        r_ta = nn.Dense(self.hidden_size)(input)\n",
    "        r_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        r_t = nn.sigmoid(r_ta + r_tb)  # update gate\n",
    "\n",
    "        g_ta = nn.Dense(self.hidden_size)(input)\n",
    "        g_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        g_t = nn.tanh((g_ta + r_t) * g_tb)  # (external) input gate\n",
    "\n",
    "        h_t = ((1 - z_t) * ht_1) + (z_t * g_t)  # internal cell state update\n",
    "\n",
    "        return h_t\n",
    "\n",
    "    def initial_state(self, batch_size: Optional[int]):\n",
    "        state = jnp.zeros([self.hidden_size])\n",
    "        if batch_size is not None:\n",
    "            state = add_batch(state, batch_size)\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPOCell(nn.module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        RNN update function\n",
    "        τ(h, x) = (1 - g(h, x)) ◦ h + g(h, x) ◦ tanh(Lτ (h, x))\n",
    "        g(h, x) = σ(Lg(h,x))\n",
    "\n",
    "    Args:\n",
    "        hippo (HiPPO): # hippo model object\n",
    "        cell (RNNCellBase): choice of RNN cell object\n",
    "            - RNNCell\n",
    "            - LSTMCell\n",
    "            - GRUCell\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "\n",
    "    hippo: HiPPO\n",
    "    cell: RNNCellBase\n",
    "\n",
    "    @nn.compact()\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            RNN update function\n",
    "            τ(h, x) = (1 - g(h, x)) ◦ h + g(h, x) ◦ tanh(Lτ (h, x))\n",
    "            g(h, x) = σ(Lg(h,x))\n",
    "\n",
    "        Args:\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "\n",
    "        _, h_t = self.cell(carry, input)\n",
    "\n",
    "        y_t = nn.Dense(self.output_size)(h_t)  # f_t in the paper\n",
    "\n",
    "        c_t = self.hippo(y_t, init_state=None, kernel=False)\n",
    "\n",
    "        return (h_t, c_t), h_t\n",
    "\n",
    "    def initial_state(self, batch_size: Optional[int]):\n",
    "        state = jnp.zeros([self.hidden_size])\n",
    "        if batch_size is not None:\n",
    "            state = add_batch(state, batch_size)\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refer to https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/recurrent.py#L714-L762\n",
    "# also refer to https://dm-haiku.readthedocs.io/en/latest/api.html?highlight=DeepRNN#deeprnn\n",
    "class _DeepRNN(RNNCellBase):\n",
    "    layers: Sequence[Any]\n",
    "    skip_connections: bool\n",
    "    hidden_to_output_layer: bool\n",
    "    name: Optional[str]\n",
    "\n",
    "    def setup(self):\n",
    "        if self.skip_connections:\n",
    "            for layer in self.layers:\n",
    "                if not (isinstance(layer, RNNCellBase) or isinstance(layer, HiPPOCell)):\n",
    "                    raise ValueError(\n",
    "                        f\"{self.name} layer {layer} is not a RNNCellBase or HiPPOCell\"\n",
    "                    )\n",
    "\n",
    "    def __call__(self, carry, inputs):\n",
    "        current_carry = carry\n",
    "        next_states = []\n",
    "        h_t_outputs = []\n",
    "        c_t_outputs = []\n",
    "        state_idx = 0\n",
    "        h_t, c_t = current_carry  # c_t may actually be h_t in which case dont use it\n",
    "        (\n",
    "            h_t_copy,\n",
    "            c_t_copy,\n",
    "        ) = current_carry  # c_t may actually be h_t in which case dont use it\n",
    "        concat = lambda *args: jnp.concatenate(args, axis=-1)\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if self.skip_connections and idx > 0:\n",
    "                skip_h_t = jax.tree_map(concat, h_t, h_t_copy)\n",
    "                skip_c_t = jax.tree_map(concat, c_t, c_t_copy)\n",
    "                current_carry = (skip_h_t, skip_c_t)\n",
    "\n",
    "            if isinstance(layer, RNNCellBase) or isinstance(layer, HiPPOCell):\n",
    "                current_carry, next_state = layer(current_carry, inputs[state_idx])\n",
    "                if self.hidden_to_output_layer:\n",
    "                    next_state = nn.Dense(next_state.shape[0])(next_state)\n",
    "\n",
    "                h_t, c_t = current_carry\n",
    "                h_t_outputs.append(h_t)\n",
    "                c_t_outputs.append(c_t)\n",
    "                next_states.append(next_state)\n",
    "                state_idx += 1\n",
    "\n",
    "            else:\n",
    "                current_carry = layer(current_carry)\n",
    "\n",
    "        if self.skip_connections:\n",
    "            skip_h_t_out = jax.tree_map(concat, *h_t_outputs)\n",
    "            skip_c_t_out = jax.tree_map(concat, *c_t_outputs)\n",
    "            out = (skip_h_t_out, skip_c_t_out)\n",
    "        else:\n",
    "            out = current_carry\n",
    "\n",
    "        return out, tuple(next_states)\n",
    "\n",
    "    def initial_state(self, batch_size: Optional[int]):\n",
    "        return tuple(\n",
    "            layer.initial_state_and_carry(batch_size)\n",
    "            for layer in self.layers\n",
    "            if isinstance(layer, RNNCellBase)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRNN(_DeepRNN):\n",
    "    r\"\"\"Wraps a sequence of cores and callables as a single core.\n",
    "        >>> deep_rnn = hk.DeepRNN([\n",
    "        ...     LSTMCell(hidden_size=4),\n",
    "        ...     jax.nn.relu,\n",
    "        ...     LSTMCell(hidden_size=2),\n",
    "        ... ])\n",
    "    The state of a :class:`DeepRNN` is a tuple with one element per\n",
    "    :class:`RNNCore`. If no layers are :class:`RNNCore`\\ s, the state is an empty\n",
    "    tuple.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers: Sequence[Any],\n",
    "        skip_connections: Optional[bool] = False,\n",
    "        hidden_to_output_layer: Optional[bool] = False,\n",
    "        name: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            layers,\n",
    "            skip_connections=skip_connections,\n",
    "            hidden_to_output_layer=hidden_to_output_layer,\n",
    "            name=name,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Training Types\n",
    "refer to [this](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one2one(params, init_carry, input, sequence_length, cell):\n",
    "    _, h_t = cell.apply(params, init_carry, input)\n",
    "    y_t = nn.Dense(input.shape[0])(h_t)  # output\n",
    "    return y_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one2many(params, init_carry, input, T_y, cell, tf_bool=True):\n",
    "    output = []\n",
    "    if not tf_bool:\n",
    "        for i in range(len(T_y)):\n",
    "            if i == 0:\n",
    "                carry, h_t = cell.apply(params, init_carry, input[i])\n",
    "            else:\n",
    "                carry, h_t = cell.apply(params, carry, output[i - 1])\n",
    "\n",
    "            y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "            output.append(y_t)\n",
    "    else:\n",
    "        for i in range(len(input)):\n",
    "            carry, h_t = cell.apply(params, init_carry, input[i])\n",
    "            y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "            output.append(y_t)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many2one(params, init_carry, input, T_x, cell, tf_bool=True):\n",
    "    y_t = None\n",
    "    carry = init_carry\n",
    "    for i in range(len(T_x)):\n",
    "        carry, h_t = cell.apply(params, carry, input[i])\n",
    "        if i == (len(T_x) - 1):\n",
    "            y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "\n",
    "    return y_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many2many(params, init_carry, input, T_xy, cell, tf_bool=True):\n",
    "    output = []\n",
    "    carry = init_carry\n",
    "    for i in range(len(T_xy)):\n",
    "        carry, h_t = cell.apply(params, carry, input[i])\n",
    "        y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "        output.append(y_t)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many2many(params, init_carry, input, T_x, T_y, cell, tf_bool=True):\n",
    "    assert T_x != T_y, \"T_x and T_y must be different\"\n",
    "    output = []\n",
    "    carry = init_carry\n",
    "    for i in range(len(T_x)):\n",
    "        carry, h_t = cell.apply(params, carry, input[i])\n",
    "\n",
    "    for i in range(len(T_y)):\n",
    "        carry, h_t = cell.apply(params, carry, input[i])\n",
    "        y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "        output.append(y_t)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data_2_id(iterable):\n",
    "    \"\"\"\n",
    "    provides mapping to and from ids\n",
    "    \"\"\"\n",
    "    id_2_data = {}\n",
    "    data_2_id = {}\n",
    "\n",
    "    for id, elem in enumerate(iterable):\n",
    "        id_2_data[id] = elem\n",
    "\n",
    "    for id, elem in enumerate(iterable):\n",
    "        data_2_id[elem] = id\n",
    "\n",
    "    return (id_2_data, data_2_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(i, n):\n",
    "    \"\"\"\n",
    "    create vector of size n with 1 at index i\n",
    "    \"\"\"\n",
    "    x = defaultdict(lambda: jnp.zeros(n))\n",
    "    return x[].at[i].set(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(char):\n",
    "    return one_hot(data_2_id[char], len(data_2_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(predictions, id_2_data):\n",
    "    return id_2_data[int(jnp.argmax(predictions))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check [this](https://github.com/deepmind/optax/blob/master/examples/quick_start.ipynb) out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_optimizer_fn(name, starting_learning_rate):\n",
    "    # refer to https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules\n",
    "    optim = None\n",
    "\n",
    "    if name == \"sgd\":\n",
    "        optim = optax.sgd(starting_learning_rate)\n",
    "    elif name == \"adam\":\n",
    "        optim = optax.adam(starting_learning_rate)\n",
    "    elif name == \"adagrad\":\n",
    "        optim = optax.adagrad(starting_learning_rate)\n",
    "    elif name == \"rmsprop\":\n",
    "        optim = optax.rmsprop(starting_learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"optimizer name not recognized\")\n",
    "\n",
    "    return optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_scheduler_fn(\n",
    "    start_learning_rate, steps, decay_rate, init_value, end_val, name\n",
    "):\n",
    "    # refer to https://optax.readthedocs.io/en/latest/api.html#schedules\n",
    "    scheduler = None\n",
    "\n",
    "    if name == \"constant\":\n",
    "        scheduler = optax.constant_schedule(init_value)\n",
    "\n",
    "    elif name == \"exp_decay\":\n",
    "        scheduler = optax.exponential_decay(\n",
    "            init_value=start_learning_rate, transition_steps=1000, decay_rate=0.99\n",
    "        )\n",
    "    elif name == \"linear\":\n",
    "        scheduler = optax.linear_schedule(init_value=init_value, end_value=end_val)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"scheduler name not recognized\")\n",
    "\n",
    "    return scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add transformations\n",
    "# refer to https://optax.readthedocs.io/en/latest/api.html#optax-transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # A simple update loop.\n",
    "#     for _ in range(1000):\n",
    "#     grads = jax.grad(compute_loss)(params, xs, ys)\n",
    "#     updates, opt_state = gradient_transform.update(grads, opt_state)\n",
    "#     params = optax.apply_updates(params, updates)\n",
    "\n",
    "#     assert jnp.allclose(params, target_params), \\\n",
    "#     'Optimization should retrieve the target params used to generate the data.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimizer_fn(start_learning_rate, params, num_weights, x, y):\n",
    "#     optimizer = optax.adam(start_learning_rate)\n",
    "#     # Obtain the `opt_state` that contains statistics for the optimizer.\n",
    "#     params = {'w': jnp.ones((num_weights,))}\n",
    "#     opt_state = optimizer.init(params)\n",
    "\n",
    "#     compute_loss = lambda params, x, y: optax.l2_loss(params['w'].dot(x), y)\n",
    "#     grads = jax.grad(compute_loss)(params, xs, ys)\n",
    "\n",
    "#     updates, opt_state = optimizer.update(grads, opt_state)\n",
    "#     params = optax.apply_updates(params, updates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    state_size: int\n",
    "    vocab_size: int\n",
    "    batch_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        L = self.vocab_size\n",
    "        N = self.state_size\n",
    "\n",
    "        hippo = HiPPO(\n",
    "            N=N,\n",
    "            max_length=L,\n",
    "            measure=\"legs\",\n",
    "            step=1.0 / L,\n",
    "            GBT_alpha=0.5,\n",
    "            seq_L=L,\n",
    "            v=\"v\",\n",
    "            lambda_n=1.0,\n",
    "            fourier_type=\"fru\",\n",
    "            alpha=0.0,\n",
    "            beta=1.0,\n",
    "        )\n",
    "\n",
    "        cell1 = LSTMCell()\n",
    "        cell2 = LSTMCell()\n",
    "\n",
    "        self.deep_cell = DeepRNN(\n",
    "            layers=[\n",
    "                HiPPOCell(hippo=hippo, cell=cell1),\n",
    "                nn.relu,\n",
    "                HiPPOCell(hippo=hippo, cell=cell2),\n",
    "            ],\n",
    "            skip_connections=True,\n",
    "            hidden_to_output_layer=True,\n",
    "            name=\"CharRNN\",\n",
    "        )\n",
    "\n",
    "    def __call__(self, carry, i):\n",
    "        input = one_hot(i, self.vocab_size)\n",
    "        carries, next_states = self.deep_cell(carry, input)\n",
    "        predictions = nn.softmax(nn.Dense(self.vocab_size)(next_states[-1]))\n",
    "        return next_states, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('jax-pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbeba54f39f0ad863615cd2814766ffd78084a8276e5c331aa23b4d5ff4f068c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
