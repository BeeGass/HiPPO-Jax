{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.ops\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax import optim\n",
    "from flax.linen.recurrent import RNNCellBase\n",
    "\n",
    "import optax\n",
    "\n",
    "import numpy as np  # convention: original numpy\n",
    "\n",
    "from typing import Any, Callable, Sequence, Optional, Tuple, Union\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 5\n",
    "rng, key2, key3, key4, key5 = jax.random.split(key, num=num_copies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(RNNCellBase):\n",
    "    @nn.compact()\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            W_xh = x_{t} @ W_{xh} - multiply the previous hidden state with\n",
    "            W_hh = H_{t-1} @ W_{hh} + b_{h} - this a linear layer\n",
    "\n",
    "            H_{t} = f_{w}(H_{t-1}, x)\n",
    "            H_{t} = tanh(H_{t-1} @ W_{hh}) + (x_{t} @ W_{xh})\n",
    "\n",
    "        Args:\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        ht_1 = carry\n",
    "\n",
    "        h_t = self.rnn_update(input, ht_1)\n",
    "\n",
    "        return h_t, h_t\n",
    "\n",
    "    def rnn_update(self, input, ht_1):\n",
    "\n",
    "        W_hh = nn.Dense(ht_1.shape[0])(ht_1)\n",
    "        W_xh = nn.Dense(input.shape[0])(input)\n",
    "        h_t = jnp.relu(W_hh + W_xh)  # H_{t} = tanh(H_{t-1} @ W_{hh}) + (x_{t} @ W_{xh})\n",
    "\n",
    "        return h_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(RNNCellBase):\n",
    "    @nn.compact()\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            i_{t} = sigmoid((W_{ii} @ x_{t} + b_{ii}) + (W_{hi} @ h_{t-1} + b_{hi}))\n",
    "            f_{t} = sigmoid((W_{if} @ x_{t} + b_{if}) + (W_{hf} @ h_{t-1} + b_{hf}))\n",
    "            g_{t} = tanh((W_{ig} @ x_{t} + b_{ig}) + (W_{hg} @ h_{t-1} + b_{hg}))\n",
    "            o_{t} = sigmoid((W_{io} @ x_{t} + b_{io}) + (W_{ho} @ h_{t-1} + b_{ho}))\n",
    "            c_{t} = f_{t} * c_{t-1} + i_{t} * g_{t}\n",
    "            h_{t} = o_{t} * tanh(c_{t})\n",
    "\n",
    "        Args:\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        ht_1, ct_1 = carry\n",
    "\n",
    "        c_t, h_t = self.rnn_update(input, ht_1, ct_1)\n",
    "\n",
    "        return (h_t, c_t), h_t\n",
    "\n",
    "    def rnn_update(self, input, ht_1, ct_1):\n",
    "\n",
    "        i_ta = nn.Dense(input.shape[0])(input)\n",
    "        i_tb = nn.Dense(ht_1.shape[0])(ht_1)\n",
    "        i_t = jnp.sigmoid(i_ta + i_tb)  # input gate\n",
    "\n",
    "        o_ta = nn.Dense(input.shape[0])(input)\n",
    "        o_tb = nn.Dense(ht_1.shape[0])(ht_1)\n",
    "        o_t = jnp.sigmoid(o_ta + o_tb)  # output gate\n",
    "\n",
    "        f_ia = nn.Dense(input.shape[0])(\n",
    "            input\n",
    "        )  # b^{f}_{i} + \\sum\\limits_{j} U^{f}_{i, j} x^{t}_{j}\n",
    "        f_ib = nn.Dense(ht_1.shape[0])(\n",
    "            ht_1\n",
    "        )  # \\sum\\limits_{j} W^{f}_{i, j} h^{(t-1)}_{j}\n",
    "        f_i = jnp.sigmoid(f_ia + f_ib)  # forget gate\n",
    "\n",
    "        g_ia = nn.Dense(input.shape[0])(\n",
    "            input\n",
    "        )  # b^{g}_{i} + \\sum\\limits_{j} U^{g}_{i, j} x^{t}_{j}\n",
    "        g_ib = nn.Dense(ht_1.shape[0])(\n",
    "            ht_1\n",
    "        )  # \\sum\\limits_{j} W^{g}_{i, j} h^{(t-1)}_{j}\n",
    "        g_i = jnp.tanh(g_ia + g_ib)  # (external) input gate\n",
    "\n",
    "        c_t = (f_i * ct_1) + (i_t * g_i)  # internal cell state update\n",
    "\n",
    "        h_t = o_t * jnp.tanh(c_t)  # hidden state update\n",
    "\n",
    "        return h_t, c_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(RNNCellBase):\n",
    "    @nn.compact()\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            z_t = sigmoid((W_{iz} @ x_{t} + b_{iz}) + (W_{hz} @ h_{t-1} + b_{hz}))\n",
    "            r_t = sigmoid((W_{ir} @ x_{t} + b_{ir}) + (W_{hr} @ h_{t-1} + b_{hr}))\n",
    "            g_t = tanh(((W_{ig} @ x_{t} + b_{ig}) + r_t) * (W_{hg} @ h_{t-1} + b_{hg}))\n",
    "            h_t = (z_t * h_{t-1}) + ((1 - z_t) * g_i)\n",
    "\n",
    "        Args:\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        ht_1 = carry\n",
    "\n",
    "        h_t = self.rnn_update(input, ht_1)\n",
    "\n",
    "        return h_t, h_t\n",
    "\n",
    "    def rnn_update(self, input, ht_1):\n",
    "\n",
    "        z_ta = nn.Dense(input.shape[0])(input)\n",
    "        z_tb = nn.Dense(ht_1.shape[0])(ht_1)\n",
    "        z_t = jnp.sigmoid(z_ta + z_tb)  # reset gate\n",
    "\n",
    "        r_ta = nn.Dense(input.shape[0])(input)\n",
    "        r_tb = nn.Dense(ht_1.shape[0])(ht_1)\n",
    "        r_t = jnp.sigmoid(r_ta + r_tb)  # update gate\n",
    "\n",
    "        g_ta = nn.Dense(input.shape[0])(input)\n",
    "        g_tb = nn.Dense(ht_1.shape[0])(ht_1)\n",
    "        g_t = jnp.tanh((g_ta + r_t) * g_tb)  # (external) input gate\n",
    "\n",
    "        h_t = ((1 - z_t) * ht_1) + (z_t * g_t)  # internal cell state update\n",
    "\n",
    "        return h_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPOCell(nn.module):\n",
    "    '''\n",
    "        Description:\n",
    "            RNN update function\n",
    "            τ(h, x) = (1 - g(h, x)) ◦ h + g(h, x) ◦ tanh(Lτ (h, x)) \n",
    "            g(h, x) = σ(Lg(h,x))\n",
    "\n",
    "        Args:\n",
    "            hippo (HiPPO): # hippo model object\n",
    "            cell (RNNCellBase): choice of RNN cell object\n",
    "                - RNNCell \n",
    "                - LSTMCell\n",
    "                - GRUCell\n",
    "    '''\n",
    "    hippo: HiPPO\n",
    "    cell: RNNCellBase\n",
    "        \n",
    "    @nn.compact() \n",
    "    def __call__(self, carry, input):\n",
    "        '''\n",
    "        Description:\n",
    "            RNN update function\n",
    "            τ(h, x) = (1 - g(h, x)) ◦ h + g(h, x) ◦ tanh(Lτ (h, x)) \n",
    "            g(h, x) = σ(Lg(h,x))\n",
    "            \n",
    "        Args:\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "            \n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        '''\n",
    "        \n",
    "        _, h_t = self.cell(carry, input)\n",
    "        \n",
    "        y_t = nn.Dense(input.shape[0])(h_t) # f_t in the paper\n",
    "        \n",
    "        c_t = self.hippo(y_t, init_state=None, kernel=False)\n",
    "        \n",
    "        return (h_t, c_t), h_t\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refer to https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/recurrent.py#L714-L762\n",
    "# also refer to https://dm-haiku.readthedocs.io/en/latest/api.html?highlight=DeepRNN#deeprnn\n",
    "class _DeepRNN():\n",
    "    layers: Sequence[Any]\n",
    "    skip_connections: bool\n",
    "    name: Optional[str]\n",
    "    \n",
    "    def setup(self):\n",
    "        if self.skip_connections:\n",
    "            for layer in self.layers:\n",
    "                assert isinstance(layer, nn.Module), \"layer must be a nn.Module or an instance of a subclass of nn.Module\"\n",
    "                \n",
    "    \n",
    "    def __call__(self, carry, input):\n",
    "        current_inputs = input\n",
    "        ht_1, ct_1 = carry\n",
    "        next_states = []\n",
    "        outputs = []\n",
    "        state_idx = 0\n",
    "        concat = lambda *args: jnp.concatenate(args, axis=-1)\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "        if self.skip_connections and idx > 0:\n",
    "            current_inputs = jax.tree_map(concat, inputs, current_inputs)\n",
    "\n",
    "        if isinstance(layer, RNNCore):\n",
    "            current_inputs, next_state = layer(current_inputs, state[state_idx])\n",
    "            outputs.append(current_inputs)\n",
    "            next_states.append(next_state)\n",
    "            state_idx += 1\n",
    "        else:\n",
    "            current_inputs = layer(current_inputs)\n",
    "\n",
    "        if self.skip_connections:\n",
    "        out = jax.tree_map(concat, *outputs)\n",
    "        else:\n",
    "        out = current_inputs\n",
    "\n",
    "        return out, tuple(next_states)\n",
    "\n",
    "    def initial_state(self, batch_size: Optional[int]):\n",
    "        return tuple(\n",
    "            layer.initial_state(batch_size)\n",
    "            for layer in self.layers\n",
    "            if isinstance(layer, RNNCore))\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Training Types\n",
    "refer to [this](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one2one(params, init_carry, input, sequence_length, cell):\n",
    "    _, h_t = cell.apply(params, init_carry, input)\n",
    "    y_t = nn.Dense(input.shape[0])(h_t)  # output\n",
    "    return y_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one2many(params, init_carry, input, T_y, cell, tf_bool=True):\n",
    "    output = []\n",
    "    if not tf_bool:\n",
    "        for i in range(len(T_y)):\n",
    "            if i == 0:\n",
    "                carry, h_t = cell.apply(params, init_carry, input[i])\n",
    "            else:\n",
    "                carry, h_t = cell.apply(params, carry, output[i - 1])\n",
    "\n",
    "            y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "            output.append(y_t)\n",
    "    else:\n",
    "        for i in range(len(input)):\n",
    "            carry, h_t = cell.apply(params, init_carry, input[i])\n",
    "            y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "            output.append(y_t)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many2one(params, init_carry, input, T_x, cell, tf_bool=True):\n",
    "    y_t = None\n",
    "    carry = init_carry\n",
    "    for i in range(len(T_x)):\n",
    "        carry, h_t = cell.apply(params, carry, input[i])\n",
    "        if i == (len(T_x) - 1):\n",
    "            y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "\n",
    "    return y_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many2many(params, init_carry, input, T_xy, cell, tf_bool=True):\n",
    "    output = []\n",
    "    carry = init_carry\n",
    "    for i in range(len(T_xy)):\n",
    "        carry, h_t = cell.apply(params, carry, input[i])\n",
    "        y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "        output.append(y_t)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many2many(params, init_carry, input, T_x, T_y, cell, tf_bool=True):\n",
    "    assert T_x != T_y, \"T_x and T_y must be different\"\n",
    "    output = []\n",
    "    carry = init_carry\n",
    "    for i in range(len(T_x)):\n",
    "        carry, h_t = cell.apply(params, carry, input[i])\n",
    "        \n",
    "    for i in range(len(T_y)):\n",
    "        carry, h_t = cell.apply(params, carry, input[i])\n",
    "        y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "        output.append(y_t)\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data_2_id(iterable):\n",
    "    \"\"\"\n",
    "    provides mapping to and from ids\n",
    "    \"\"\"\n",
    "    id_2_data = {}\n",
    "    data_2_id = {}\n",
    "\n",
    "    for id, elem in enumerate(iterable):\n",
    "        id_2_data[id] = elem\n",
    "\n",
    "    for id, elem in enumerate(iterable):\n",
    "        data_2_id[elem] = id\n",
    "\n",
    "    return (id_2_data, data_2_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(i, n):\n",
    "    \"\"\"\n",
    "    create vector of size n with 1 at index i\n",
    "    \"\"\"\n",
    "    x = defaultdict(lambda: jnp.zeros(n))\n",
    "    return x[].at[i].set(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(char):\n",
    "    return one_hot(data_2_id[char], len(data_2_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(predictions, id_2_data):\n",
    "    return id_2_data[int(jnp.argmax(predictions))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check [this](https://github.com/deepmind/optax/blob/master/examples/quick_start.ipynb) out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_optimizer_fn(name, starting_learning_rate):\n",
    "    # refer to https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules\n",
    "    optim = None\n",
    "\n",
    "    if name == \"sgd\":\n",
    "        optim = optax.sgd(starting_learning_rate)\n",
    "    elif name == \"adam\":\n",
    "        optim = optax.adam(starting_learning_rate)\n",
    "    elif name == \"adagrad\":\n",
    "        optim = optax.adagrad(starting_learning_rate)\n",
    "    elif name == \"rmsprop\":\n",
    "        optim = optax.rmsprop(starting_learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"optimizer name not recognized\")\n",
    "\n",
    "    return optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_scheduler_fn(start_learning_rate, steps, decay_rate, init_value, end_val):\n",
    "    # refer to https://optax.readthedocs.io/en/latest/api.html#schedules\n",
    "    scheduler = None\n",
    "\n",
    "    if name == \"constant\":\n",
    "        scheduler = optax.constant_schedule(value)\n",
    "\n",
    "    elif name == \"exp_decay\":\n",
    "        scheduler = optax.exponential_decay(\n",
    "            init_value=start_learning_rate, transition_steps=1000, decay_rate=0.99\n",
    "        )\n",
    "    elif name == \"linear\":\n",
    "        scheduler = optax.linear_schedule(\n",
    "            init_value=init_value, end_value = end_val, transition_steps\n",
    "        )\n",
    "    \n",
    "    return scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add transformations\n",
    "# refer to https://optax.readthedocs.io/en/latest/api.html#optax-transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # A simple update loop.\n",
    "#     for _ in range(1000):\n",
    "#     grads = jax.grad(compute_loss)(params, xs, ys)\n",
    "#     updates, opt_state = gradient_transform.update(grads, opt_state)\n",
    "#     params = optax.apply_updates(params, updates)\n",
    "\n",
    "#     assert jnp.allclose(params, target_params), \\\n",
    "#     'Optimization should retrieve the target params used to generate the data.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimizer_fn(start_learning_rate, params, num_weights, x, y):\n",
    "#     optimizer = optax.adam(start_learning_rate)\n",
    "#     # Obtain the `opt_state` that contains statistics for the optimizer.\n",
    "#     params = {'w': jnp.ones((num_weights,))}\n",
    "#     opt_state = optimizer.init(params)\n",
    "    \n",
    "#     compute_loss = lambda params, x, y: optax.l2_loss(params['w'].dot(x), y)\n",
    "#     grads = jax.grad(compute_loss)(params, xs, ys)\n",
    "    \n",
    "#     updates, opt_state = optimizer.update(grads, opt_state)\n",
    "#     params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jax-pytorch-s4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef2b914a37cedc76fc9872a46094ffe13a6a8170158ba89febe80f19b5718ff4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
