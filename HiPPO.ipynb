{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiPPO Matrices\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GpuDevice(id=0, process_index=0)]"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import packages\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import linen as jnn\n",
    "\n",
    "from jax.nn.initializers import lecun_normal, uniform\n",
    "from jax.numpy.linalg import eig, inv, matrix_power\n",
    "from jax.scipy.signal import convolve\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from scipy import linalg as la\n",
    "from scipy import signal\n",
    "from scipy import special as ss\n",
    "\n",
    "import math\n",
    "\n",
    "## setup JAX to use TPUs if available\n",
    "try:\n",
    "    url = 'http:' + os.environ['TPU_NAME'].split(':')[1] + ':8475/requestversion/tpu_driver_nightly'\n",
    "    resp = requests.post(url)\n",
    "    jax.config.FLAGS.jax_xla_backend = 'tpu_driver'\n",
    "    jax.config.FLAGS.jax_backend_target = os.environ['TPU_NAME']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 5\n",
    "rng, key2, key3, key4, key5 = jax.random.split(key, num=num_copies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_SSM(rng, N):\n",
    "    a_r, b_r, c_r = jax.random.split(rng, 3)\n",
    "    A = jax.random.uniform(a_r, (N, N))\n",
    "    B = jax.random.uniform(b_r, (N, 1))\n",
    "    C = jax.random.uniform(c_r, (1, N))\n",
    "    return A, B, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate The HiPPO Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Legendre (LegT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Legendre (LegT) - vectorized\n",
    "def build_LegT_V(N, lambda_n=1):\n",
    "    q = jnp.arange(N, dtype=jnp.float64)\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    case = jnp.power(-1.0, (n-k))\n",
    "    A = None\n",
    "    B = None\n",
    "    \n",
    "    if lambda_n == 1:\n",
    "        A_base = -jnp.sqrt(2*n+1) * jnp.sqrt(2*k+1)\n",
    "        pre_D = jnp.sqrt(jnp.diag(2*q+1))\n",
    "        B = D = jnp.diag(pre_D)[:, None]\n",
    "        A = jnp.where(k <= n, A_base, A_base * case) # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "        \n",
    "    elif lambda_n == 2: #(jnp.sqrt(2*n+1) * jnp.power(-1, n)):\n",
    "        A_base = -(2*n+1)\n",
    "        B = jnp.diag((2*q+1) * jnp.power(-1, n))[:, None]\n",
    "        A = jnp.where(k <= n, A_base * case, A_base) # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Legendre (LegT) - non-vectorized\n",
    "def build_LegT(N, legt_type=\"legt\"):\n",
    "    Q = jnp.arange(N, dtype=jnp.float64)\n",
    "    pre_R = (2*Q + 1)\n",
    "    k, n = jnp.meshgrid(Q, Q)\n",
    "        \n",
    "    if legt_type == \"legt\":\n",
    "        R = jnp.sqrt(pre_R)\n",
    "        A = R[:, None] * jnp.where(n < k, (-1.)**(n-k), 1) * R[None, :]\n",
    "        B = R[:, None]\n",
    "        A = -A\n",
    "\n",
    "        # Halve again for timescale correctness\n",
    "        # A, B = A/2, B/2\n",
    "        #A *= 0.5\n",
    "        #B *= 0.5\n",
    "        \n",
    "    elif legt_type == \"lmu\":\n",
    "        R = pre_R[:, None]\n",
    "        A = jnp.where(n < k, -1, (-1.)**(n-k+1)) * R\n",
    "        B = (-1.)**Q[:, None] * R\n",
    "        \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      " [[-1. -1. -1. -1. -1.]\n",
      " [ 3. -3. -3. -3. -3.]\n",
      " [-5.  5. -5. -5. -5.]\n",
      " [ 7. -7.  7. -7. -7.]\n",
      " [-9.  9. -9.  9. -9.]]\n",
      "v:\n",
      " [[-1. -1. -1. -1. -1.]\n",
      " [ 3. -3. -3. -3. -3.]\n",
      " [-5.  5. -5. -5. -5.]\n",
      " [ 7. -7.  7. -7. -7.]\n",
      " [-9.  9. -9.  9. -9.]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.virtualenvs/jax-pytorch/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2065: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in arange is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"arange\")\n"
     ]
    }
   ],
   "source": [
    "nv_LegT_A, nv_LegT_B = build_LegT(N=N, legt_type=\"lmu\")\n",
    "LegT_A, LegT_B = build_LegT_V(N=N, lambda_n=2)\n",
    "print(f\"nv:\\n\", nv_LegT_A)\n",
    "print(f\"v:\\n\", LegT_A)\n",
    "# print(f\"nv:\\n\", nv_LegT_B)\n",
    "# print(f\"v:\\n\", LegT_B)\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_LegT_A, LegT_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_LegT_B, LegT_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Laguerre (LagT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Laguerre (LagT) - non-vectorized\n",
    "def build_LagT(alpha, beta, N):\n",
    "    A = -jnp.eye(N) * (1 + beta) / 2 - jnp.tril(jnp.ones((N, N)), -1)\n",
    "    B = ss.binom(alpha + jnp.arange(N), jnp.arange(N))[:, None]\n",
    "\n",
    "    L = jnp.exp(.5 * (ss.gammaln(jnp.arange(N)+alpha+1) - ss.gammaln(jnp.arange(N)+1)))\n",
    "    A = (1./L[:, None]) * A * L[None, :]\n",
    "    B = (1./L[:, None]) * B * jnp.exp(-.5 * ss.gammaln(1-alpha)) * beta**((1-alpha)/2)\n",
    "    \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Laguerre (LagT) - non-vectorized\n",
    "def build_LagT_V(alpha, beta, N):\n",
    "    L = jnp.exp(.5 * (ss.gammaln(jnp.arange(N)+alpha+1) - ss.gammaln(jnp.arange(N)+1)))\n",
    "    inv_L = 1./L[:, None]\n",
    "    pre_A = (jnp.eye(N) * ((1 + beta) / 2)) + jnp.tril(jnp.ones((N, N)), -1)\n",
    "    pre_B = ss.binom(alpha + jnp.arange(N), jnp.arange(N))[:, None]\n",
    "    \n",
    "    A = -inv_L * pre_A * L[None, :]\n",
    "    B =  jnp.exp(-.5 * ss.gammaln(1-alpha)) * jnp.power(beta, (1-alpha)/2) * inv_L * pre_B \n",
    "    \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      "[[-1. -0. -0. -0. -0.]\n",
      " [-1. -1. -0. -0. -0.]\n",
      " [-1. -1. -1. -0. -0.]\n",
      " [-1. -1. -1. -1. -0.]\n",
      " [-1. -1. -1. -1. -1.]]\n",
      "v:\n",
      "[[-1. -0. -0. -0. -0.]\n",
      " [-1. -1. -0. -0. -0.]\n",
      " [-1. -1. -1. -0. -0.]\n",
      " [-1. -1. -1. -1. -0.]\n",
      " [-1. -1. -1. -1. -1.]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_LagT_A, nv_LagT_B = build_LagT(alpha=0, beta=1, N=N)\n",
    "LagT_A, LagT_B = build_LagT_V(alpha=0, beta=1, N=N)\n",
    "print(f\"nv:\\n{nv_LagT_A}\")\n",
    "print(f\"v:\\n{LagT_A}\")\n",
    "# print(f\"nv:\\n{nv_LagT_B}\")\n",
    "# print(f\"v:\\n{LagT_B}\")\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_LagT_A, LagT_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_LagT_B, LagT_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Legendre (LegS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled Legendre (LegS) vectorized\n",
    "def build_LegS_V(N):\n",
    "    q = jnp.arange(N, dtype=jnp.float64)\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    pre_D = jnp.sqrt(jnp.diag(2*q+1))\n",
    "    B = D = jnp.diag(pre_D)[:, None]\n",
    "    \n",
    "    A_base = (-jnp.sqrt(2*n+1)) * jnp.sqrt(2*k+1)\n",
    "    case_2 = (n+1)/(2*n+1)\n",
    "    \n",
    "    A = jnp.where(n > k, A_base, 0.0) # if n > k, then A_base is used, otherwise 0\n",
    "    A = jnp.where(n == k, (A_base * case_2), A) # if n == k, then A_base is used, otherwise A\n",
    "    \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled Legendre (LegS), non-vectorized\n",
    "def build_LegS(N):\n",
    "    q = jnp.arange(N, dtype=jnp.float64)  # q represents the values 1, 2, ..., N each column has\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    r = 2 * q + 1\n",
    "    M = -(jnp.where(n >= k, r, 0) - jnp.diag(q)) # represents the state matrix M \n",
    "    D = jnp.sqrt(jnp.diag(2 * q + 1)) # represents the diagonal matrix D $D := \\text{diag}[(2n+1)^{\\frac{1}{2}}]^{N-1}_{n=0}$\n",
    "    A = D @ M @ jnp.linalg.inv(D)\n",
    "    B = jnp.diag(D)[:, None]\n",
    "    B = B.copy() # Otherwise \"UserWarning: given NumPY array is not writeable...\" after torch.as_tensor(B)\n",
    "    \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      "[[-1.         0.         0.         0.         0.       ]\n",
      " [-1.7324219 -1.9997292  0.         0.         0.       ]\n",
      " [-2.2363281 -3.873207  -3.0015717  0.         0.       ]\n",
      " [-2.6464844 -4.58337   -5.919281  -4.00074    0.       ]\n",
      " [-3.        -5.194336  -6.7089844 -7.9365234 -4.9987793]]\n",
      "v:\n",
      "[[-1.         0.         0.         0.         0.       ]\n",
      " [-1.7320508 -2.         0.         0.         0.       ]\n",
      " [-2.2360678 -3.872983  -2.9999995  0.         0.       ]\n",
      " [-2.6457512 -4.5825753 -5.916079  -4.         0.       ]\n",
      " [-3.        -5.196152  -6.7082033 -7.937254  -5.       ]]\n",
      "A Comparison:\n",
      "  False\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_LegS_A, nv_LegS_B = build_LegS(N=N)\n",
    "LegS_A, LegS_B = build_LegS_V(N=N)\n",
    "print(f\"nv:\\n{nv_LegS_A}\")\n",
    "print(f\"v:\\n{LegS_A}\")\n",
    "# print(f\"nv:\\n{nv_LegS_B}\")\n",
    "# print(f\"v:\\n{LegS_B}\")\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_LegS_A, LegS_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_LegS_B, LegS_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourier Basis OPs and functions - vectorized\n",
    "def build_Fourier_V(N, fourier_type='fru'):    \n",
    "    q = jnp.arange((N//2)*2, dtype=jnp.float64)\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    \n",
    "    n_odd = n % 2 == 0\n",
    "    k_odd = k % 2 == 0\n",
    "    \n",
    "    case_1 = (n==k) & (n==0)\n",
    "    case_2_3 = ((k==0) & (n_odd)) | ((n==0) & (k_odd))\n",
    "    case_4 = (n_odd) & (k_odd)\n",
    "    case_5 = (n-k==1) & (k_odd)\n",
    "    case_6 = (k-n==1) & (n_odd)\n",
    "    \n",
    "    A = None\n",
    "    B = None\n",
    "    \n",
    "    if fourier_type == \"fru\": # Fourier Recurrent Unit (FRU) - vectorized\n",
    "        A = jnp.diag(jnp.stack([jnp.zeros(N//2), jnp.zeros(N//2)], axis=-1).reshape(-1))\n",
    "        B = jnp.zeros(A.shape[1], dtype=jnp.float64)\n",
    "        q = jnp.arange((N//2)*2, dtype=jnp.float64)\n",
    "        \n",
    "        A = jnp.where(case_1, -1.0, \n",
    "                    jnp.where(case_2_3, -jnp.sqrt(2),\n",
    "                                jnp.where(case_4, -2, \n",
    "                                        jnp.where(case_5, jnp.pi * (n//2), \n",
    "                                                    jnp.where(case_6, -jnp.pi * (k//2), 0.0)))))\n",
    "        \n",
    "        B = B.at[::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "        \n",
    "    elif fourier_type == \"fout\": # truncated Fourier (FouT) - vectorized\n",
    "        A = jnp.diag(jnp.stack([jnp.zeros(N//2), jnp.zeros(N//2)], axis=-1).reshape(-1))\n",
    "        B = jnp.zeros(A.shape[1], dtype=jnp.float64)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "        n_odd = n % 2 == 0\n",
    "        k_odd = k % 2 == 0\n",
    "        \n",
    "        A = jnp.where(case_1, -1.0, \n",
    "                    jnp.where(case_2_3, -jnp.sqrt(2),\n",
    "                                jnp.where(case_4, -2, \n",
    "                                        jnp.where(case_5, jnp.pi * (n//2), \n",
    "                                                    jnp.where(case_6, -jnp.pi * (k//2), 0.0)))))\n",
    "        \n",
    "        B = B.at[::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "        \n",
    "        A = 2 * A\n",
    "        B = 2 * B\n",
    "        \n",
    "    elif fourier_type == \"fourd\":\n",
    "        A = jnp.diag(jnp.stack([jnp.zeros(N//2), jnp.zeros(N//2)], axis=-1).reshape(-1))\n",
    "        B = jnp.zeros(A.shape[1], dtype=jnp.float64)\n",
    "        \n",
    "        A = jnp.where(case_1, -1.0, \n",
    "                    jnp.where(case_2_3, -jnp.sqrt(2),\n",
    "                                jnp.where(case_4, -2, \n",
    "                                        jnp.where(case_5, 2 * jnp.pi * (n//2), \n",
    "                                                    jnp.where(case_6, 2 * -jnp.pi * (k//2), 0.0)))))\n",
    "        \n",
    "        B = B.at[::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "        \n",
    "        A = 0.5 * A\n",
    "        B = 0.5 * B\n",
    "        \n",
    "    \n",
    "    \n",
    "    B = B[:, None]\n",
    "        \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Fourier(N, fourier_type='fru'):\n",
    "    freqs = jnp.arange(N//2)\n",
    "    \n",
    "    if fourier_type == \"fru\": # Fourier Recurrent Unit (FRU) - non-vectorized\n",
    "        d = jnp.stack([jnp.zeros(N//2), freqs], axis=-1).reshape(-1)[1:]\n",
    "        A = jnp.pi*(-jnp.diag(d, 1) + jnp.diag(d, -1))\n",
    "        \n",
    "        B = jnp.zeros(A.shape[1])\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "        \n",
    "        A = A - B[:, None] * B[None, :]\n",
    "        B = B[:, None]\n",
    "\n",
    "    elif fourier_type == \"fout\": # truncated Fourier (FouT) - non-vectorized\n",
    "        freqs *= 2\n",
    "        d = jnp.stack([jnp.zeros(N//2), freqs], axis=-1).reshape(-1)[1:]\n",
    "        A = jnp.pi*(-jnp.diag(d, 1) + jnp.diag(d, -1))\n",
    "        \n",
    "        B = jnp.zeros(A.shape[1])\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        # Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case\n",
    "        A = A - B[:, None] * B[None, :] * 2\n",
    "        B = B[:, None] * 2\n",
    "        \n",
    "    elif fourier_type == \"fourd\":\n",
    "        d = jnp.stack([jnp.zeros(N//2), freqs], axis=-1).reshape(-1)[1:]\n",
    "        A = jnp.pi*(-jnp.diag(d, 1) + jnp.diag(d, -1))\n",
    "        \n",
    "        B = jnp.zeros(A.shape[1])\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        # Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case\n",
    "        A = A - 0.5 * B[:, None] * B[None, :]\n",
    "        B = 0.5 * B[:, None]\n",
    "        \n",
    "            \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      "[[-1.         0.        -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999 -3.1415927]\n",
      " [ 0.         0.         3.1415927  0.       ]]\n",
      "v:\n",
      "[[-1.        -0.        -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -2.        -3.1415927]\n",
      " [ 0.         0.         3.1415927  0.       ]]\n",
      "A Comparison:\n",
      " True\n",
      "B Comparison:\n",
      " True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.virtualenvs/jax-pytorch/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:1942: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"zeros\")\n"
     ]
    }
   ],
   "source": [
    "nv_Fourier_A, nv_Fourier_B = build_Fourier(N=N, fourier_type='fru')\n",
    "Fourier_A, Fourier_B = build_Fourier_V(N=N, fourier_type='fru')\n",
    "print(f\"nv:\\n{nv_Fourier_A}\")\n",
    "print(f\"v:\\n{Fourier_A}\")\n",
    "# print(f\"nv:\\n{nv_Fourier_B}\")\n",
    "# print(f\"v:\\n{Fourier_B}\")\n",
    "print(f\"A Comparison:\\n {jnp.allclose(nv_Fourier_A, Fourier_A)}\")\n",
    "print(f\"B Comparison:\\n {jnp.allclose(nv_Fourier_B, Fourier_B)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_HiPPO(N, v='nv', measure=\"legs\", lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1):\n",
    "    \"\"\"\n",
    "        Instantiates the HiPPO matrix of a given order using a particular measure. \n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            v (str): choose between this repo's implementation or hazy research's implementation.\n",
    "            measure (str): \n",
    "                choose between \n",
    "                    - HiPPO w/ Translated Legendre (LegT) - legt\n",
    "                    - HiPPO w/ Translated Laguerre (LagT) - lagt\n",
    "                    - HiPPO w/ Scaled Legendre (LegS) - legs\n",
    "                    - HiPPO w/ Fourier basis - fourier\n",
    "                        - FRU: Fourier Recurrent Unit \n",
    "                        - FouT: Translated Fourier \n",
    "            lambda_n (int): The amount of tilt applied to the HiPPO-LegS basis, determines between LegS and LMU. \n",
    "            fourier_type (str): chooses between the following:\n",
    "                - FRU: Fourier Recurrent Unit - fru\n",
    "                - FouT: Translated Fourier - fout\n",
    "                - FourD: Fourier Decay - fourd\n",
    "            alpha (float): The order of the Laguerre basis.\n",
    "            beta (float): The scale of the Laguerre basis.\n",
    "            \n",
    "        Returns:\n",
    "            A (jnp.ndarray): The HiPPO matrix multiplied by -1.\n",
    "            B (jnp.ndarray): The other corresponding state space matrix. \n",
    "            \n",
    "    \"\"\"\n",
    "    A = None\n",
    "    B = None\n",
    "    if measure == \"legt\":\n",
    "        if v == 'nv':\n",
    "            A, B = build_LegT(N=N, lambda_n=lambda_n)\n",
    "        else:\n",
    "            A, B = build_LegT_V(N=N, lambda_n=lambda_n) \n",
    "        \n",
    "    elif measure == \"lagt\":\n",
    "        if v == 'nv':\n",
    "            A, B = build_LagT(alpha=alpha, beta=beta, N=N)\n",
    "        else:\n",
    "            A, B = build_LagT_V(alpha=alpha, beta=beta, N=N)\n",
    "        \n",
    "    elif measure == \"legs\":\n",
    "        if v == 'nv':\n",
    "            A, B = build_LegS(N=N)\n",
    "        else:\n",
    "            A, B = build_LegS_V(N=N)\n",
    "        \n",
    "    elif measure == \"fourier\":\n",
    "        if v == 'nv':\n",
    "            A, B = build_Fourier(N=N, fourier_type=fourier_type)\n",
    "        else:\n",
    "            A, B = build_Fourier_V(N=N, fourier_type=fourier_type)\n",
    "        \n",
    "    elif measure == \"random\":\n",
    "        A = jnp.random.randn(N, N) / N\n",
    "        B = jnp.random.randn(N, 1)\n",
    "        \n",
    "    elif measure == \"diagonal\":\n",
    "        A = -jnp.diag(jnp.exp(jnp.random.randn(N)))\n",
    "        B = jnp.random.randn(N, 1)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid HiPPO type\")\n",
    "    \n",
    "    A_copy = A.copy()\n",
    "    B_copy = B.copy()\n",
    "    \n",
    "    return -jnp.array(A_copy), B_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      " [[ 1.        -0.        -0.        -0.        -0.       ]\n",
      " [ 1.7324219  1.9997292 -0.        -0.        -0.       ]\n",
      " [ 2.2363281  3.873207   3.0015717 -0.        -0.       ]\n",
      " [ 2.6464844  4.58337    5.919281   4.00074   -0.       ]\n",
      " [ 3.         5.194336   6.7089844  7.9365234  4.9987793]]\n",
      "v:\n",
      " [[ 1.        -0.        -0.        -0.        -0.       ]\n",
      " [ 1.7320508  2.        -0.        -0.        -0.       ]\n",
      " [ 2.2360678  3.872983   2.9999995 -0.        -0.       ]\n",
      " [ 2.6457512  4.5825753  5.916079   4.        -0.       ]\n",
      " [ 3.         5.196152   6.7082033  7.937254   5.       ]]\n",
      "A Comparison:\n",
      "  False\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_A, nv_B = make_HiPPO(N=N, v='nv', measure=\"legs\", lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1)\n",
    "v_A, v_B = make_HiPPO(N=N, v='v', measure=\"legs\", lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1)\n",
    "print(f\"nv:\\n\", nv_A)\n",
    "print(f\"v:\\n\", v_A)\n",
    "# print(f\"nv:\\n\", nv_B)\n",
    "# print(f\"v:\\n\", v_B)\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_A, v_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_B, v_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def discretize(A, B, C, step, alpha=0.5):\n",
    "#     '''\n",
    "#     - forward Euler corresponds to α = 0,\n",
    "#     - backward Euler corresponds to α = 1,\n",
    "#     - bilinear corresponds to α = 0.5,\n",
    "#     - Zero-order Hold corresponds to α > 1\n",
    "#     '''\n",
    "#     I = jnp.eye(A.shape[0])\n",
    "#     GBT = jnp.linalg.inv(I - ((step * alpha) * A))\n",
    "#     GBT_A = GBT @ (I + ((step * (1-alpha)) * A))\n",
    "#     GBT_B = (step * GBT) @ B\n",
    "    \n",
    "#     if alpha > 1: # Zero-order Hold\n",
    "#         GBT_A = jax.scipy.linalg.expm(step * A)\n",
    "#         GBT_B = (jnp.linalg.inv(A) @ (jax.scipy.linalg.expm(step * A) - I)) @ B \n",
    "    \n",
    "#     return GBT_A, GBT_B, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def discretize_b(A, B, A_stacked, B_stacked, N, max_length=1024, discretization='bilinear'):\n",
    "#     for t in range(1, max_length + 1):\n",
    "#         At = A / t\n",
    "#         Bt = B / t\n",
    "#         if discretization == 'forward':\n",
    "#             A_stacked[t - 1] = jnp.eye(N) + At\n",
    "#             B_stacked[t - 1] = Bt\n",
    "#         elif discretization == 'backward':\n",
    "#             A_stacked[t - 1] = la.solve_triangular(jnp.eye(N) - At, jnp.eye(N), lower=True)\n",
    "#             B_stacked[t - 1] = la.solve_triangular(jnp.eye(N) - At, Bt, lower=True)\n",
    "#         elif discretization == 'bilinear':\n",
    "            \n",
    "#             A_stacked[t - 1] = la.solve_triangular(jnp.eye(N) - At / 2, jnp.eye(N) + At / 2, lower=True)\n",
    "#             B_stacked[t - 1] = la.solve_triangular(jnp.eye(N) - At / 2, Bt, lower=True)\n",
    "#         else: # ZOH\n",
    "#             A_stacked[t - 1] = la.expm(A * (math.log(t + 1) - math.log(t)))\n",
    "#             B_stacked[t - 1] = la.solve_triangular(A, A_stacked[t - 1] @ B - B, lower=True)\n",
    "            \n",
    "#     return A_stacked, B_stacked\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collect_SSM_vars(A, B, C, u, alpha=0.5):\n",
    "#     L = u.shape[0]\n",
    "#     N = A.shape[0]\n",
    "#     Ab, Bb, Cb = discretize(A, B, C, step=1.0 / L, alpha=alpha)\n",
    "    \n",
    "#     return Ab, Bb, Cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scan_SSM(Ab, Bb, Cb, u, x0):\n",
    "#     '''\n",
    "#     This is for returning the discretized hidden state often needed for an RNN. \n",
    "#     '''\n",
    "#     def step(x_k_1, u_k):\n",
    "#         x_k = Ab @ x_k_1 + Bb @ u_k\n",
    "#         y_k = Cb @ x_k\n",
    "#         return x_k, y_k\n",
    "\n",
    "#     return jax.lax.scan(step, x0, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Old utilities for parallel scan implementation of Linear RNNs. \"\"\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "### Utilities\n",
    "\n",
    "\n",
    "def shift_up(a, s=None, drop=True, dim=0):\n",
    "    assert dim == 0\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(a[0, ...])\n",
    "    s = s.unsqueeze(dim)\n",
    "    if drop:\n",
    "        a = a[:-1, ...]\n",
    "    return torch.cat((s, a), dim=dim)\n",
    "\n",
    "def interleave(a, b, uneven=False, dim=0):\n",
    "    \"\"\" Interleave two tensors of same shape \"\"\"\n",
    "    # assert(a.shape == b.shape)\n",
    "    assert dim == 0 # TODO temporary to make handling uneven case easier\n",
    "    if dim < 0:\n",
    "        dim = N + dim\n",
    "    if uneven:\n",
    "        a_ = a[-1:, ...]\n",
    "        a = a[:-1, ...]\n",
    "    c = torch.stack((a, b), dim+1)\n",
    "    out_shape = list(a.shape)\n",
    "    out_shape[dim] *= 2\n",
    "    c = c.view(out_shape)\n",
    "    if uneven:\n",
    "        c = torch.cat((c, a_), dim=dim)\n",
    "    return c\n",
    "\n",
    "def batch_mult(A, u, has_batch=None):\n",
    "    \"\"\" Matrix mult A @ u with special case to save memory if u has additional batch dim\n",
    "\n",
    "    The batch dimension is assumed to be the second dimension\n",
    "    A : (L, ..., N, N)\n",
    "    u : (L, [B], ..., N)\n",
    "    has_batch: True, False, or None. If None, determined automatically\n",
    "\n",
    "    Output:\n",
    "    x : (L, [B], ..., N)\n",
    "      A @ u broadcasted appropriately\n",
    "    \"\"\"\n",
    "\n",
    "    if has_batch is None:\n",
    "        has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    if has_batch:\n",
    "        u = u.permute([0] + list(range(2, len(u.shape))) + [1])\n",
    "    else:\n",
    "        u = u.unsqueeze(-1)\n",
    "    v = (A @ u)\n",
    "    if has_batch:\n",
    "        v = v.permute([0] + [len(u.shape)-1] + list(range(1, len(u.shape)-1)))\n",
    "    else:\n",
    "        v = v[..., 0]\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "### Main unrolling functions\n",
    "\n",
    "def unroll(A, u):\n",
    "    \"\"\"\n",
    "    A : (..., N, N) # TODO I think this can't take batch dimension?\n",
    "    u : (L, ..., N)\n",
    "    output : x (..., N) # TODO a lot of these shapes are wrong\n",
    "    x[i, ...] = A^{i} @ u[0, ...] + ... + A @ u[i-1, ...] + u[i, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    m = u.new_zeros(u.shape[1:])\n",
    "    outputs = []\n",
    "    for u_ in torch.unbind(u, dim=0):\n",
    "        m = F.linear(m, A) + u_\n",
    "        outputs.append(m)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive(A, u):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of unroll. \"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_(A, u):\n",
    "        if u.shape[0] == 1:\n",
    "            return u\n",
    "\n",
    "        u_evens = u[0::2, ...]\n",
    "        u_odds = u[1::2, ...]\n",
    "\n",
    "        # u2 = F.linear(u_evens, A) + u_odds\n",
    "        u2 = (A @ u_evens.unsqueeze(-1)).squeeze(-1) + u_odds\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_odds = parallel_unroll_recursive_(A2, u2)\n",
    "        # x_evens = F.linear(shift_up(x_odds), A) + u_evens\n",
    "        x_evens = (A @ shift_up(x_odds).unsqueeze(-1)).squeeze(-1) + u_evens\n",
    "\n",
    "        x = interleave(x_evens, x_odds, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n)/math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N-u.shape[0],) + u.shape[1:] )), dim=0)\n",
    "\n",
    "    return parallel_unroll_recursive_(A, u)[:n, ...]\n",
    "\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive_br(A, u):\n",
    "    \"\"\" Same as parallel_unroll_recursive but uses bit reversal for locality. \"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_br_(A, u):\n",
    "        n = u.shape[0]\n",
    "        if n == 1:\n",
    "            return u\n",
    "\n",
    "        m = n//2\n",
    "        u_0 = u[:m, ...]\n",
    "        u_1 = u[m:, ...]\n",
    "\n",
    "        u2 = F.linear(u_0, A) + u_1\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_1 = parallel_unroll_recursive_br_(A2, u2)\n",
    "        x_0 = F.linear(shift_up(x_1), A) + u_0\n",
    "\n",
    "        # x = torch.cat((x_0, x_1), dim=0) # is there a way to do this with cat?\n",
    "        x = interleave(x_0, x_1, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n)/math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N-u.shape[0],) + u.shape[1:] )), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    x = parallel_unroll_recursive_br_(A, u)\n",
    "    return x[:n, ...]\n",
    "\n",
    "def parallel_unroll_iterative(A, u):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of unroll, implemented iteratively \"\"\"\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n)/math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N-u.shape[0],) + u.shape[1:] )), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    # Main recursive loop, flattened\n",
    "    us = [] # stores the u_0 terms in the recursive version\n",
    "    N_ = N\n",
    "    As = [] # stores the A matrices\n",
    "    for l in range(m):\n",
    "        N_ = N_ // 2\n",
    "        As.append(A)\n",
    "        u_0 = u[:N_, ...]\n",
    "        us.append(u_0)\n",
    "        u = F.linear(u_0, A) + u[N_:, ...]\n",
    "        A = A @ A\n",
    "    x_0 = []\n",
    "    x = u # x_1\n",
    "    for l in range(m-1, -1, -1):\n",
    "        x_0 = F.linear(shift_up(x), As[l]) + us[l]\n",
    "        x = interleave(x_0, x, dim=0)\n",
    "\n",
    "    return x[:n, ...]\n",
    "\n",
    "\n",
    "def variable_unroll_sequential(A, u, s=None, variable=True):\n",
    "    \"\"\" Unroll with variable (in time/length) transitions A.\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] @ s + A[i..1] @ u[0] + ... + A[i] @ u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        # s = F.linear(s, A_) + u_\n",
    "        # print(\"shapes\", A_.shape, s.shape, has_batch)\n",
    "        s = batch_mult(A_.unsqueeze(0), s.unsqueeze(0), has_batch)[0]\n",
    "        # breakpoint()\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def variable_unroll(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of variable_unroll. \"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        return variable_unroll_sequential(A, u, s, variable)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1  = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1  = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = A_1 @ A_0_\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll(A_10, u_10, s, variable, recurse_limit)\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "\n",
    "    x = interleave(x_0, x_1, uneven, dim=0) # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "def variable_unroll_general_sequential(A, u, s, op, variable=True):\n",
    "    \"\"\" Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        s = op(A_, s)\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "def variable_unroll_matrix_sequential(A, u, s=None, variable=True):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    return variable_unroll_general_sequential(A, u, s, op, variable=True)\n",
    "\n",
    "def variable_unroll_toeplitz_sequential(A, u, s=None, variable=True, pad=False):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        # print(\"shapes\", A.shape, u.shape)\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        # print(\"shapes\", A.shape, u.shape)\n",
    "        ret = variable_unroll_general_sequential(A, u, s, triangular_toeplitz_multiply_padded, variable=True)\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    return variable_unroll_general_sequential(A, u, s, triangular_toeplitz_multiply, variable=True)\n",
    "\n",
    "\n",
    "\n",
    "### General parallel scan functions with generic binary composition operators\n",
    "\n",
    "def variable_unroll_general(A, u, s, op, compose_op=None, sequential_op=None, variable=True, recurse_limit=16):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of variable_unroll.\n",
    "\n",
    "    compose is an optional function that defines how to compose A without multiplying by a leaf u\n",
    "    \"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        if sequential_op is None:\n",
    "            sequential_op = op\n",
    "        return variable_unroll_general_sequential(A, u, s, sequential_op, variable)\n",
    "\n",
    "    if compose_op is None:\n",
    "        compose_op = op\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1 = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1 = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = op(A_1, u_0_) # batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = compose_op(A_1, A_0_)\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll_general(A_10, u_10, s, op, compose_op, sequential_op, variable=variable, recurse_limit=recurse_limit)\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = op(A_0, x_0) # batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "\n",
    "    x = interleave(x_0, x_1, uneven, dim=0) # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "def variable_unroll_matrix(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "    op = lambda x, y: batch_mult(x, y, has_batch)\n",
    "    sequential_op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    matmul = lambda x, y: x @ y\n",
    "    return variable_unroll_general(A, u, s, op, compose_op=matmul, sequential_op=sequential_op, variable=variable, recurse_limit=recurse_limit)\n",
    "\n",
    "def variable_unroll_toeplitz(A, u, s=None, variable=True, recurse_limit=8, pad=False):\n",
    "    \"\"\" Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (L, [B], ..., N) same shape as u\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "    # Add the batch dimension to A if necessary\n",
    "    A_batch_dims = len(A.shape) - int(variable)\n",
    "    u_batch_dims = len(u.shape)-1\n",
    "    if u_batch_dims > A_batch_dims:\n",
    "        # assert u_batch_dims == A_batch_dims + 1\n",
    "        if variable:\n",
    "            while len(A.shape) < len(u.shape):\n",
    "                A = A.unsqueeze(1)\n",
    "        # else:\n",
    "        #     A = A.unsqueeze(0)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        # print(\"shapes\", A.shape, u.shape)\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        # print(\"shapes\", A.shape, u.shape)\n",
    "        op = triangular_toeplitz_multiply_padded\n",
    "        ret = variable_unroll_general(A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit)\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    op = triangular_toeplitz_multiply\n",
    "    ret = variable_unroll_general(A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit)\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "### Testing\n",
    "\n",
    "def test_correctness():\n",
    "    print(\"Testing Correctness\\n====================\")\n",
    "\n",
    "    # Test sequential unroll\n",
    "    L = 3\n",
    "    A = torch.Tensor([[1, 1], [1, 0]])\n",
    "    u = torch.ones((L, 2))\n",
    "    x = unroll(A, u)\n",
    "    assert torch.isclose(x, torch.Tensor([[1., 1.], [3., 2.], [6., 4.]])).all()\n",
    "\n",
    "    # Test utilities\n",
    "    assert torch.isclose(shift_up(x), torch.Tensor([[0., 0.], [1., 1.], [3., 2.]])).all()\n",
    "    assert torch.isclose(interleave(x, x), torch.Tensor([[1., 1.], [1., 1.], [3., 2.], [3., 2.], [6., 4.], [6., 4.]])).all()\n",
    "\n",
    "    # Test parallel unroll\n",
    "    x = parallel_unroll_recursive(A, u)\n",
    "    assert torch.isclose(x, torch.Tensor([[1., 1.], [3., 2.], [6., 4.]])).all()\n",
    "\n",
    "    # Powers\n",
    "    L = 12\n",
    "    A = torch.Tensor([[1, 0, 0], [2, 1, 0], [3, 3, 1]])\n",
    "    u = torch.ones((L, 3))\n",
    "    x = parallel_unroll_recursive(A, u)\n",
    "    print(\"recursive\", x)\n",
    "    x = parallel_unroll_recursive_br(A, u)\n",
    "    print(\"recursive_br\", x)\n",
    "    x = parallel_unroll_iterative(A, u)\n",
    "    print(\"iterative_br\", x)\n",
    "\n",
    "\n",
    "    A = A.repeat((L, 1, 1))\n",
    "    s = torch.zeros(3)\n",
    "    print(\"A shape\", A.shape)\n",
    "    x = variable_unroll_sequential(A, u, s)\n",
    "    print(\"variable_unroll\", x)\n",
    "    x = variable_unroll(A, u, s)\n",
    "    print(\"parallel_variable_unroll\", x)\n",
    "\n",
    "\n",
    "def generate_data(L, N, B=None, cuda=True):\n",
    "    A = torch.eye(N) + torch.normal(0, 1, size=(N, N)) / (N**.5) / L\n",
    "    u = torch.normal(0, 1, size=(L, B, N))\n",
    "\n",
    "\n",
    "    # device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    device = torch.device('cuda:0') if cuda else torch.device('cpu')\n",
    "    A = A.to(device)\n",
    "    u = u.to(device)\n",
    "    return A, u\n",
    "\n",
    "def test_stability():\n",
    "    print(\"Testing Stability\\n====================\")\n",
    "    L = 256\n",
    "    N = L // 2\n",
    "    B = 100\n",
    "    A, u = generate_data(L, N, B)\n",
    "\n",
    "    x = unroll(A, u)\n",
    "    x1 = parallel_unroll_recursive(A, u)\n",
    "    x2 = parallel_unroll_recursive_br(A, u)\n",
    "    x3 = parallel_unroll_iterative(A, u)\n",
    "    print(\"norm error\", torch.norm(x-x1))\n",
    "    print(\"norm error\", torch.norm(x-x2))\n",
    "    print(\"norm error\", torch.norm(x-x3))\n",
    "    # print(x-x1)\n",
    "    # print(x-x2)\n",
    "    # print(x-x3)\n",
    "    print(\"max error\", torch.max(torch.abs(x-x1)))\n",
    "    print(\"max error\", torch.max(torch.abs(x-x2)))\n",
    "    print(\"max error\", torch.max(torch.abs(x-x3)))\n",
    "\n",
    "    A = A.repeat((L, 1, 1))\n",
    "    x = variable_unroll_sequential(A, u)\n",
    "    x_ = variable_unroll(A, u)\n",
    "    # x_ = variable_unroll_matrix_sequential(A, u)\n",
    "    x_ = variable_unroll_matrix(A, u)\n",
    "    print(x-x_)\n",
    "    abserr = torch.abs(x-x_)\n",
    "    relerr = abserr/(torch.abs(x)+1e-8)\n",
    "    print(\"norm abs error\", torch.norm(abserr))\n",
    "    print(\"max abs error\", torch.max(abserr))\n",
    "    print(\"norm rel error\", torch.norm(relerr))\n",
    "    print(\"max rel error\", torch.max(relerr))\n",
    "\n",
    "def test_toeplitz():\n",
    "    from model.toeplitz import construct_toeplitz\n",
    "    def summarize(name, x, x_, showdiff=False):\n",
    "        print(name, \"stats\")\n",
    "        if showdiff:\n",
    "            print(x-x_)\n",
    "        abserr = torch.abs(x-x_)\n",
    "        relerr = abserr/(torch.abs(x)+1e-8)\n",
    "        print(\"  norm abs error\", torch.norm(abserr))\n",
    "        print(\"  max abs error\", torch.max(abserr))\n",
    "        print(\"  norm rel error\", torch.norm(relerr))\n",
    "        print(\"  max rel error\", torch.max(relerr))\n",
    "\n",
    "    print(\"Testing Toeplitz\\n====================\")\n",
    "    L = 512\n",
    "    N = L // 2\n",
    "    B = 100\n",
    "    A, u = generate_data(L, N, B)\n",
    "\n",
    "    A = A[..., 0]\n",
    "    A = construct_toeplitz(A)\n",
    "\n",
    "    # print(\"SHAPES\", A.shape, u.shape)\n",
    "\n",
    "    # Static A\n",
    "    x = unroll(A, u)\n",
    "    x_ = variable_unroll(A, u, variable=False)\n",
    "    summarize(\"nonvariable matrix original\", x, x_, showdiff=False)\n",
    "    x_ = variable_unroll_matrix(A, u, variable=False)\n",
    "    summarize(\"nonvariable matrix general\", x, x_, showdiff=False)\n",
    "    x_ = variable_unroll_toeplitz(A[..., 0], u, variable=False)\n",
    "    summarize(\"nonvariable toeplitz\", x, x_, showdiff=False)\n",
    "\n",
    "    # Sequential\n",
    "    A = A.repeat((L, 1, 1))\n",
    "    for _ in range(1):\n",
    "        x_ = variable_unroll_sequential(A, u)\n",
    "        summarize(\"variable unroll sequential\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_matrix_sequential(A, u)\n",
    "        summarize(\"variable matrix sequential\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_toeplitz_sequential(A[..., 0], u, pad=True)\n",
    "        summarize(\"variable toeplitz sequential\", x, x_, showdiff=False)\n",
    "\n",
    "    # Parallel\n",
    "    for _ in range(1):\n",
    "        x_ = variable_unroll(A, u)\n",
    "        summarize(\"variable matrix original\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_matrix(A, u)\n",
    "        summarize(\"variable matrix general\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_toeplitz(A[..., 0], u, pad=True, recurse_limit=8)\n",
    "        summarize(\"variable toeplitz\", x, x_, showdiff=False)\n",
    "\n",
    "def test_speed(variable=False, it=1):\n",
    "    print(\"Testing Speed\\n====================\")\n",
    "    N = 256\n",
    "    L = 1024\n",
    "    B = 100\n",
    "    A, u = generate_data(L, N, B)\n",
    "    As = A.repeat((L, 1, 1))\n",
    "\n",
    "    u.requires_grad=True\n",
    "    As.requires_grad=True\n",
    "    for _ in range(it):\n",
    "        x = unroll(A, u)\n",
    "        x = torch.sum(x)\n",
    "        x.backward()\n",
    "\n",
    "        x = parallel_unroll_recursive(A, u)\n",
    "        x = torch.sum(x)\n",
    "        x.backward()\n",
    "\n",
    "        # parallel_unroll_recursive_br(A, u)\n",
    "        # parallel_unroll_iterative(A, u)\n",
    "\n",
    "    for _ in range(it):\n",
    "        if variable:\n",
    "            x = variable_unroll_sequential(As, u, variable=True, recurse_limit=16)\n",
    "            x = torch.sum(x)\n",
    "            x.backward()\n",
    "            x = variable_unroll(As, u, variable=True, recurse_limit=16)\n",
    "            x = torch.sum(x)\n",
    "            x.backward()\n",
    "        else:\n",
    "            variable_unroll_sequential(A, u, variable=False, recurse_limit=16)\n",
    "            variable_unroll(A, u, variable=False, recurse_limit=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LegT(nn.Module):\n",
    "    def __init__(self, N, dt=1.0, discretization='bilinear'):\n",
    "        \"\"\"\n",
    "        N: the order of the HiPPO projection\n",
    "        dt: discretization step size - should be roughly inverse to the length of the sequence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        #A, B = transition('lmu', N)\n",
    "        A, B = make_HiPPO(N=self.N, v='v', measure='legt', lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1)\n",
    "        C = np.ones((1, N))\n",
    "        D = np.zeros((1,))\n",
    "        # dt, discretization options\n",
    "        A, B, _, _, _ = signal.cont2discrete((A, B, C, D), dt=dt, method=discretization)\n",
    "\n",
    "        B = B.squeeze(-1)\n",
    "\n",
    "        self.register_buffer('A', torch.Tensor(A)) # (N, N)\n",
    "        self.register_buffer('B', torch.Tensor(B)) # (N,)\n",
    "\n",
    "        # vals = np.linspace(0.0, 1.0, 1./dt)\n",
    "        vals = np.arange(0.0, 1.0, dt)\n",
    "        self.eval_matrix = torch.Tensor(ss.eval_legendre(np.arange(N)[:, None], 1 - 2 * vals).T)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = inputs * self.B # (length, ..., N)\n",
    "\n",
    "        c = torch.zeros(u.shape[1:])\n",
    "        cs = []\n",
    "        for f in inputs:\n",
    "            c = F.linear(c, self.A) + self.B * f\n",
    "            cs.append(c)\n",
    "        return torch.stack(cs, dim=0)\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        return (self.eval_matrix @ c.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LegS(nn.Module):\n",
    "    \"\"\" Vanilla HiPPO-LegS model (scale invariant instead of time invariant) \"\"\"\n",
    "    def __init__(self, N, max_length=1024, measure='legs', discretization='bilinear'):\n",
    "        \"\"\"\n",
    "        max_length: maximum sequence length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        A, B = make_HiPPO(N=self.N, v='v', measure=measure, lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1)\n",
    "        #A, B = transition(measure, N)\n",
    "        B = B.squeeze(-1)\n",
    "        A_stacked = np.empty((max_length, N, N), dtype=A.dtype)\n",
    "        B_stacked = np.empty((max_length, N), dtype=B.dtype)\n",
    "        for t in range(1, max_length + 1):\n",
    "            At = A / t\n",
    "            Bt = B / t\n",
    "            if discretization == 'forward':\n",
    "                A_stacked[t - 1] = np.eye(N) + At\n",
    "                B_stacked[t - 1] = Bt\n",
    "            elif discretization == 'backward':\n",
    "                # print(f\"BEFORE - Stacked A matrix:\\n{A_stacked[t - 1]}\")\n",
    "                # print(np.eye(N) - At)\n",
    "                A_stacked[t - 1] = la.solve_triangular(np.eye(N) - At, np.eye(N), lower=True)\n",
    "                print(f\"AFTER - Stacked A matrix:\\n{A_stacked[t - 1]}\")\n",
    "                B_stacked[t - 1] = la.solve_triangular(np.eye(N) - At, Bt, lower=True)\n",
    "            elif discretization == 'bilinear':\n",
    "                # print(f\"1st Term:\\n{np.eye(N) - At / 2}\")\n",
    "                # print(f\"1st Term Singular value\\n{np.linalg.svd(np.eye(N) - At / 2)[1]}\")\n",
    "                # print(f\"2nd Term:\\n{np.eye(N) + At / 2}\")\n",
    "                # print(f\"2nd Term Singular value\\n{np.linalg.svd(np.eye(N) + At / 2)[1]}\")\n",
    "                A_stacked[t - 1] = np.linalg.lstsq(np.eye(N) - At / 2,  np.eye(N) + At / 2, rcond=None)[0] # TODO: Referencing this: https://stackoverflow.com/questions/64527098/numpy-linalg-linalgerror-singular-matrix-error-when-trying-to-solve \n",
    "                #A_stacked[t - 1] = la.solve_triangular(np.eye(N) - At / 2, np.eye(N) + At / 2, lower=True)\n",
    "                # print(f\"AFTER - Stacked A matrix:\\n{A_stacked[t - 1]}\")\n",
    "                B_stacked[t - 1] = np.linalg.lstsq(np.eye(N) - At / 2,  Bt, rcond=None)[0]\n",
    "                #B_stacked[t - 1] = la.solve_triangular(np.eye(N) - At / 2, Bt, lower=True)\n",
    "            else: # ZOH\n",
    "                A_stacked[t - 1] = la.expm(A * (math.log(t + 1) - math.log(t)))\n",
    "                B_stacked[t - 1] = la.solve_triangular(A, A_stacked[t - 1] @ B - B, lower=True)\n",
    "        self.A_stacked = torch.Tensor(A_stacked.copy()) # (max_length, N, N)\n",
    "        self.B_stacked = torch.Tensor(B_stacked.copy()) # (max_length, N)\n",
    "        # print(f\"A_stacked:\\n{self.A_stacked.shape}\")\n",
    "        # print(f\"B_stacked:\\n{self.B_stacked.shape}\")\n",
    "\n",
    "        vals = np.linspace(0.0, 1.0, max_length)\n",
    "        self.eval_matrix = torch.from_numpy(np.asarray(((B[:, None] * ss.eval_legendre(np.arange(N)[:, None], 2 * vals - 1)).T)))\n",
    "        print(f\"eval_matrix:\\n{self.eval_matrix}\")\n",
    "        print(f\"eval_matrix:\\n{self.eval_matrix.shape}\")\n",
    "\n",
    "    def forward(self, inputs, fast=False):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "        result = None\n",
    "        \n",
    "        L = inputs.shape[0]\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = torch.transpose(inputs, 0, -2)\n",
    "        u = u * self.B_stacked[:L]\n",
    "        u = torch.transpose(u, 0, -2) # (length, ..., N)\n",
    "\n",
    "        if fast:\n",
    "            result = variable_unroll_matrix(self.A_stacked[:L], u)\n",
    "            \n",
    "        else:\n",
    "            result = variable_unroll_matrix_sequential(self.A_stacked[:L], u)\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        a = self.eval_matrix @ c.unsqueeze(-1)\n",
    "        return a.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO(jnn.Module):\n",
    "    '''\n",
    "    class that constructs HiPPO model using the defined measure. \n",
    "    \n",
    "    Args:\n",
    "        N (int): order of the HiPPO projection, aka the number of coefficients to describe the matrix\n",
    "        max_length (int): maximum sequence length to be input\n",
    "        measure (str): the measure used to define which way to instantiate the HiPPO matrix\n",
    "        step (float): step size used for descretization\n",
    "        GBT_alpha (float): represents which descretization transformation to use based off the alpha value\n",
    "        seq_L (int): length of the sequence to be used for training\n",
    "    '''\n",
    "    N: int \n",
    "    max_length: int \n",
    "    measure: str \n",
    "    step: float \n",
    "    GBT_alpha: float \n",
    "    seq_L: int \n",
    "    \n",
    "    def setup(self):\n",
    "        A, B = make_HiPPO(N=self.N, v='v', measure=\"legs\", lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1)\n",
    "        # self.A = self.param(\"A\", A, (self.N, self.N))\n",
    "        # self.B = self.param(\"B\", B, (self.N, 1))\n",
    "        # self.C = self.param(\"C\", lecun_normal(), (1, self.N))\n",
    "        # self.D = self.param(\"D\", jnn.initializers.ones, (1,))\n",
    "        \n",
    "        self.A = A\n",
    "        self.B = B.squeeze(-1)\n",
    "        self.C = jnp.ones((1, self.N))\n",
    "        self.D = jnp.zeros((1,))\n",
    "        \n",
    "        if self.measure == \"legt\":\n",
    "            L = self.seq_L\n",
    "            vals = jnp.arange(0.0, 1.0, L)\n",
    "            n = jnp.arange(self.N)[:, None]\n",
    "            x = 1 - 2 * vals\n",
    "            print(f\"legt eval shape:\\n{(ss.eval_legendre(n, x).T).shape}\")\n",
    "            print(f\"legt eval type:\\n{type(ss.eval_legendre(n, x).T)}\")\n",
    "            self.eval_matrix = ss.eval_legendre(n, x).T\n",
    "            \n",
    "        elif self.measure == \"legs\":\n",
    "            L = self.max_length\n",
    "            vals = jnp.linspace(0.0, 1.0, L)\n",
    "            n = jnp.arange(self.N)[:, None]\n",
    "            x =  2 * vals - 1\n",
    "            print(f\"legs eval shape:\\n{((B[:, None] * ss.eval_legendre(n,x)).T).shape}\")\n",
    "            print(f\"legs eval type:\\n{type((B[:, None] * ss.eval_legendre(n,x)).T)}\")\n",
    "            self.eval_matrix = (B[:, None] * ss.eval_legendre(n,x)).T\n",
    "        \n",
    "    def __call__(self, u, kernel=False):\n",
    "        if not kernel:\n",
    "            Ab, Bb, Cb, Db = self.collect_SSM_vars(self.A, self.B, self.C, self.D, u, alpha=self.GBT_alpha)\n",
    "            \n",
    "            print\n",
    "            print(f\"input from within the call:\\n{u[:, jnp.newaxis]}\")\n",
    "            c_k = self.scan_SSM(Ab, Bb, Cb, Db, u[:, jnp.newaxis], x0=jnp.zeros((self.N, )))[1]\n",
    "        else:\n",
    "            Ab, Bb, Cb, Db = self.discretize(self.A, self.B, self.C, self.D, step=self.step, alpha=self.GBT_alpha)\n",
    "            c_k = self.causal_convolution(u, self.K_conv(Ab, Bb, Cb, Db, L=self.max_length))\n",
    "            \n",
    "        return c_k\n",
    "    \n",
    "    def reconstruct(self, c):\n",
    "        '''\n",
    "        Uses coeffecients to reconstruct the signal\n",
    "        \n",
    "        Args: \n",
    "            c (jnp.ndarray): coefficients of the HiPPO projection\n",
    "            \n",
    "        Returns:\n",
    "            reconstructed signal\n",
    "        '''\n",
    "        a = self.eval_matrix @ jnp.expand_dims(c, -1)\n",
    "        return a.squeeze(-1)\n",
    "    \n",
    "    def discretize(self, A, B, C, D, step, alpha=0.5): # TODO this is broken, produces NaN values\n",
    "        '''\n",
    "        function used for descretizing the HiPPO matrix\n",
    "        \n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            step (float): step size used for discretization\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "                - forward Euler corresponds to α = 0,\n",
    "                - backward Euler corresponds to α = 1,\n",
    "                - bilinear corresponds to α = 0.5,\n",
    "                - Zero-order Hold corresponds to α > 1\n",
    "        '''\n",
    "        print(f\"A before descritize:\\n{A}\")\n",
    "        print(f\"B before descritize:\\n{B}\")\n",
    "        I = jnp.eye(A.shape[0])\n",
    "        GBT = jnp.linalg.inv(I - ((step * alpha) * A))\n",
    "        GBT_A = GBT @ (I + ((step * (1-alpha)) * A))\n",
    "        GBT_B = (step * GBT) @ B\n",
    "        \n",
    "        if alpha > 1: # Zero-order Hold\n",
    "            GBT_A = jax.scipy.linalg.expm(step * A)\n",
    "            GBT_B = (jnp.linalg.inv(A) @ (jax.scipy.linalg.expm(step * A) - I)) @ B \n",
    "        \n",
    "        print(f\"A after descritize:\\n{GBT_A}\")\n",
    "        print(f\"B after descritize:\\n{GBT_B}\")\n",
    "        \n",
    "        return GBT_A, GBT_B, C, D\n",
    "    \n",
    "    def collect_SSM_vars(self, A, B, C, D, u, alpha=0.5):\n",
    "        '''\n",
    "        turns the continous HiPPO matrix components into a discrete ones\n",
    "        \n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            u (jnp.ndarray): input signal\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "            \n",
    "        Returns:\n",
    "            Ab (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Bb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Cb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Db (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            \n",
    "            What is her name again\n",
    "            Mari\n",
    "        '''\n",
    "        L = u.shape[0]\n",
    "        assert L == self.seq_L\n",
    "        N = A.shape[0]\n",
    "        Ab, Bb, Cb, Db = self.discretize(A, B, C, D, step=1.0/L, alpha=alpha)\n",
    "    \n",
    "        return Ab, Bb, Cb, Db\n",
    "    \n",
    "    def scan_SSM(self, Ab, Bb, Cb, Db, u, x0):\n",
    "        '''\n",
    "        This is for returning the discretized hidden state often needed for an RNN. \n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            u (jnp.ndarray): the input sequence\n",
    "            x0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        '''\n",
    "        def step(x_k_1, u_k):\n",
    "            '''\n",
    "            Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "            Args:\n",
    "                x_k_1: previous hidden state\n",
    "                u_k: output from function f at, descritized, time step, k.\n",
    "            \n",
    "            Returns: \n",
    "                x_k: current hidden state\n",
    "                y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "            '''\n",
    "            print(f\"\\n\\nAb:\\n{Ab}\")\n",
    "            print(f\"Bb:\\n{Bb}\")\n",
    "            print(f\"Cb:\\n{Cb}\")\n",
    "            print(f\"Db:\\n{Db}\")\n",
    "            print(f\"u:\\n{u}\")\n",
    "            print(f\"x0:\\n{x0}\")\n",
    "            print(f\"x_k_1:\\n{x_k_1}\")\n",
    "            print(f\"u_k:\\n{u_k}\")\n",
    "\n",
    "            x_k = (Ab @ x_k_1) + (Bb @ u_k)\n",
    "            y_k = (Cb @ x_k) + (Db @ u_k)\n",
    "            \n",
    "            print(f\"x_k 1st term shape:\\n{(Ab @ x_k_1).shape}\")\n",
    "            print(f\"x_k 2nd term shape:\\n{(Bb @ u_k)}\")\n",
    "            \n",
    "            print(f\"y_k 1st term shape:\\n{(Cb @ x_k).shape}\")\n",
    "            print(f\"y_k 2nd term shape:\\n{(Db @ u_k).shape}\")\n",
    "            return x_k, y_k\n",
    "\n",
    "        return jax.lax.scan(step, x0, u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # N = 256\n",
    "    # L = 128\n",
    "    \n",
    "    N = 16\n",
    "    L = 8\n",
    "    \n",
    "    x = torch.randn(L, 1)\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    loss = nn.MSELoss()\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegT model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print('\\nTesting HiPPO LegT model')\n",
    "    hippo_legt = HiPPO_LegT(N, dt=1./L)\n",
    "    \n",
    "    y = hippo_legt(x)\n",
    "    \n",
    "    print(f\"h-y shape for LegT:\\n{y.shape}\")\n",
    "    z = hippo_legt.reconstruct(y)\n",
    "    print(f\"h-z shape for LegT:\\n{z.shape}\")\n",
    "\n",
    "    # mse = torch.mean((z[-1,0,:L].flip(-1) - x.squeeze(-1))**2)\n",
    "    # mse = torch.mean((z[-1,0,:L] - x.s}\")\n",
    "    mse = loss(z[-1,0,:L], x.squeeze(-1))\n",
    "    # mse = torch.mean((z[-1,0,:L] - x.squeeze(-1))**2)\n",
    "    print(f\"h-MSE shape:\\n{mse}\")\n",
    "    print(f\"end of test for HiPPO LegT model\")\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegS model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print('\\nTesting HiPPO LegS model')\n",
    "    # print(y.shape)\n",
    "    hippo_legs = HiPPO_LegS(N, max_length=L) #The Gu's\n",
    "    \n",
    "    y = hippo_legs(x)\n",
    "    \n",
    "    print(f\"h-y shape for LegS:\\n{y.shape}\")\n",
    "    print(f\"h-y for LegS:\\n{y}\")\n",
    "    \n",
    "    z = hippo_legs(x, fast=True)\n",
    "    \n",
    "    print(f\"h-reconstruction shape for LegS:\\n{hippo_legs.reconstruct(z).shape}\")\n",
    "    print(f\"h-reconstruction for LegS:\\n{hippo_legs.reconstruct(z)}\")\n",
    "    \n",
    "    # print(y-z)\n",
    "    print(f\"end of test for HiPPO LegT model\")\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test Generic HiPPO model --------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print('\\nTesting BRYANS HiPPO LegS model')\n",
    "    hippo_LegS_B = HiPPO(N=N,\n",
    "                         max_length=L, \n",
    "                         measure='legs', \n",
    "                         step=1.0/L, \n",
    "                         GBT_alpha=0.5, \n",
    "                         seq_L=L) # Bryan's\n",
    "    \n",
    "    x = jnp.asarray(x) # convert torch array to jax array\n",
    "    print(f\"input:\\n{x}\")\n",
    "    print(f\"input type:\\n{type(x)}\")\n",
    "    \n",
    "    params = hippo_LegS_B.init(key2, x)\n",
    "    \n",
    "    y_legs = hippo_LegS_B.apply(params, x)\n",
    "    \n",
    "    print(f\"U-y shape for LegS:\\n{y_legs.shape}\")\n",
    "    print(f\"U-y for LegS:\\n{y_legs}\")\n",
    "    \n",
    "    z_legs = hippo_LegS_B(x, fast=True)\n",
    "    \n",
    "    print(f\"U-reconstruction shape for LegS:\\n{hippo_LegS_B.reconstruct(z_legs).shape}\")\n",
    "    print(f\"U-reconstruction for LegS:\\n{hippo_LegS_B.reconstruct(z_legs)}\")\n",
    "    \n",
    "    print(f\"end of test for HiPPO LegT model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing HiPPO LegT model\n",
      "h-y shape for LegT:\n",
      "torch.Size([8, 1, 16])\n",
      "h-z shape for LegT:\n",
      "torch.Size([8, 1, 8])\n",
      "h-MSE shape:\n",
      "inf\n",
      "end of test for HiPPO LegT model\n",
      "\n",
      "Testing HiPPO LegS model\n",
      "eval_matrix:\n",
      "tensor([[ 1.0000, -1.7321,  2.2361, -2.6458,  3.0000, -3.3166,  3.6056, -3.8730,\n",
      "          4.1231, -4.3589,  4.5826, -4.7958,  5.0000, -5.1962,  5.3852, -5.5678],\n",
      "        [ 1.0000, -1.2372,  0.5932,  0.4242, -1.1983,  1.2778, -0.6189, -0.3970,\n",
      "          1.1854, -1.2936,  0.6599,  0.3521, -1.1626,  1.3075, -0.7039, -0.3025],\n",
      "        [ 1.0000, -0.7423, -0.5020,  1.1802, -0.4985, -0.7584,  1.1471, -0.2216,\n",
      "         -0.9582,  1.0417,  0.0666, -1.0990,  0.8746,  0.3501, -1.1745,  0.6561],\n",
      "        [ 1.0000, -0.2474, -1.0496,  0.5477,  0.9009, -0.8053, -0.6730,  0.9979,\n",
      "          0.3888, -1.1092, -0.0724,  1.1301, -0.2502, -1.0588,  0.5525,  0.9011],\n",
      "        [ 1.0000,  0.2474, -1.0496, -0.5477,  0.9009,  0.8053, -0.6730, -0.9979,\n",
      "          0.3888,  1.1092, -0.0724, -1.1301, -0.2502,  1.0588,  0.5525, -0.9011],\n",
      "        [ 1.0000,  0.7423, -0.5020, -1.1802, -0.4985,  0.7584,  1.1471,  0.2216,\n",
      "         -0.9582, -1.0417,  0.0666,  1.0990,  0.8746, -0.3501, -1.1745, -0.6561],\n",
      "        [ 1.0000,  1.2372,  0.5932, -0.4242, -1.1983, -1.2778, -0.6189,  0.3970,\n",
      "          1.1854,  1.2936,  0.6599, -0.3521, -1.1626, -1.3075, -0.7039,  0.3025],\n",
      "        [ 1.0000,  1.7321,  2.2361,  2.6458,  3.0000,  3.3166,  3.6056,  3.8730,\n",
      "          4.1231,  4.3589,  4.5826,  4.7958,  5.0000,  5.1962,  5.3852,  5.5678]])\n",
      "eval_matrix:\n",
      "torch.Size([8, 16])\n",
      "h-y shape for LegS:\n",
      "torch.Size([8, 1, 16])\n",
      "h-y for LegS:\n",
      "tensor([[[ 2.1469e-01,  1.2395e-01,  1.0450e-06, -2.4585e-06,  4.6433e-06,\n",
      "          -7.6998e-06,  1.1712e-05, -1.6774e-05,  2.2966e-05, -3.0352e-05,\n",
      "           3.9002e-05, -4.8986e-05,  6.0365e-05, -7.3188e-05,  8.7511e-05,\n",
      "          -1.0340e-04]],\n",
      "\n",
      "        [[ 8.6164e-02,  3.0048e-02, -1.9069e-02, -3.2312e-03,  1.3848e-05,\n",
      "          -2.2742e-05,  3.4002e-05, -4.7279e-05,  6.1745e-05, -7.5922e-05,\n",
      "           8.7454e-05, -9.2816e-05,  8.6990e-05, -6.3090e-05,  1.1941e-05,\n",
      "           7.8421e-05]],\n",
      "\n",
      "        [[-2.4216e-01, -3.5750e-01, -2.6187e-01, -1.0187e-01, -2.2826e-02,\n",
      "          -2.3413e-03,  6.4668e-05, -8.5773e-05,  1.0361e-04, -1.1223e-04,\n",
      "           1.0445e-04, -7.3975e-05,  2.0208e-05,  4.2730e-05, -7.0447e-05,\n",
      "          -3.6817e-05]],\n",
      "\n",
      "        [[ 1.3044e-01,  1.3073e-01,  8.3380e-02,  5.4337e-02,  2.4292e-02,\n",
      "           6.9298e-03,  1.2702e-03, -4.0592e-05,  1.3308e-04, -1.1545e-04,\n",
      "           6.5641e-05,  6.1042e-06, -6.2815e-05,  4.6935e-05,  5.3933e-05,\n",
      "           1.1711e-05]],\n",
      "\n",
      "        [[-7.3745e-02, -1.4966e-01, -1.5262e-01, -9.4327e-02, -4.5106e-02,\n",
      "          -1.6870e-02, -4.5261e-03, -1.0625e-03,  2.6936e-05, -8.0501e-05,\n",
      "          -1.3129e-05,  7.0262e-05, -3.3349e-05, -5.8344e-05, -2.0683e-05,\n",
      "          -2.4244e-06]],\n",
      "\n",
      "        [[-8.6938e-02, -1.6810e-01, -1.7650e-01, -1.2184e-01, -6.7309e-02,\n",
      "          -3.0235e-02, -1.0550e-02, -3.1436e-03, -5.1757e-04, -9.2839e-05,\n",
      "          -8.3169e-05,  3.4930e-05,  5.8974e-05,  2.3440e-05,  4.0292e-06,\n",
      "           2.7517e-07]],\n",
      "\n",
      "        [[ 8.8450e-02,  9.5476e-02,  7.2109e-02,  5.8030e-02,  3.8349e-02,\n",
      "           2.0606e-02,  9.5462e-03,  3.3055e-03,  1.0764e-03,  3.1451e-04,\n",
      "          -9.8421e-06, -5.2032e-05, -1.9304e-05, -3.2759e-06, -2.9314e-07,\n",
      "          -1.2164e-08]],\n",
      "\n",
      "        [[-2.4172e-01, -4.0433e-01, -4.2634e-01, -3.4428e-01, -2.3370e-01,\n",
      "          -1.3561e-01, -6.6745e-02, -2.8285e-02, -1.0051e-02, -2.8853e-03,\n",
      "          -6.7347e-04, -1.3044e-04, -2.0327e-05, -2.2828e-06, -1.7806e-07,\n",
      "           0.0000e+00]]])\n",
      "h-reconstruction shape for LegS:\n",
      "torch.Size([8, 1, 8])\n",
      "h-reconstruction for LegS:\n",
      "tensor([[[ 2.5257e-03,  6.1202e-02,  1.2255e-01,  1.8400e-01,  2.4544e-01,\n",
      "           3.0670e-01,  3.6799e-01,  4.2906e-01]],\n",
      "\n",
      "        [[ 2.2724e-03,  3.6239e-02,  6.9739e-02,  9.7055e-02,  1.1529e-01,\n",
      "           1.2182e-01,  1.1343e-01,  8.7413e-02]],\n",
      "\n",
      "        [[ 2.3052e-03,  2.6316e-02,  4.7652e-02,  4.6501e-02, -2.2362e-02,\n",
      "          -2.4615e-01, -7.6628e-01, -1.7930e+00]],\n",
      "\n",
      "        [[ 2.3008e-03,  2.0632e-02,  3.9418e-02,  5.5927e-02,  7.2159e-02,\n",
      "           1.1595e-01,  2.7985e-01,  7.8802e-01]],\n",
      "\n",
      "        [[ 2.3015e-03,  1.6694e-02,  3.2742e-02,  4.6984e-02,  5.0735e-02,\n",
      "           7.5159e-03, -2.3141e-01, -1.1360e+00]],\n",
      "\n",
      "        [[ 2.3017e-03,  1.3847e-02,  2.8140e-02,  4.0768e-02,  4.8377e-02,\n",
      "           1.9169e-02, -2.2426e-01, -1.4499e+00]],\n",
      "\n",
      "        [[ 2.3017e-03,  1.1745e-02,  2.4673e-02,  3.5770e-02,  4.6858e-02,\n",
      "           6.1400e-02,  1.4958e-01,  8.0460e-01]],\n",
      "\n",
      "        [[ 2.3017e-03,  1.0164e-02,  2.1883e-02,  3.1827e-02,  4.0752e-02,\n",
      "           2.1741e-02, -3.8143e-01, -4.3652e+00]]])\n",
      "end of test for HiPPO LegT model\n",
      "\n",
      "Testing BRYANS HiPPO LegS model\n",
      "input:\n",
      "[[-0.2146867 ]\n",
      " [-0.25820348]\n",
      " [ 0.8527709 ]\n",
      " [ 0.50247186]\n",
      " [ 0.17401066]\n",
      " [ 1.292949  ]\n",
      " [ 0.32262412]\n",
      " [ 1.7802873 ]]\n",
      "input type:\n",
      "<class 'jaxlib.xla_extension.DeviceArray'>\n",
      "legs eval shape:\n",
      "(8, 16, 16)\n",
      "legs eval type:\n",
      "<class 'jaxlib.xla_extension.DeviceArray'>\n",
      "A before descritize:\n",
      "[[ 1.        -0.        -0.        -0.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.       ]\n",
      " [ 1.7320508  2.        -0.        -0.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.       ]\n",
      " [ 2.2360678  3.872983   2.9999995 -0.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.       ]\n",
      " [ 2.6457512  4.5825753  5.916079   4.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.       ]\n",
      " [ 3.         5.196152   6.7082033  7.937254   5.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.       ]\n",
      " [ 3.3166246  5.744562   7.4161973  8.774963   9.949874   5.9999995\n",
      "  -0.        -0.        -0.        -0.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.       ]\n",
      " [ 3.6055512  6.244998   8.062257   9.5393915 10.816654  11.958261\n",
      "   7.0000005 -0.        -0.        -0.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.       ]\n",
      " [ 3.8729832  6.708204   8.660253  10.24695   11.61895   12.845232\n",
      "  13.964239   8.        -0.        -0.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.       ]\n",
      " [ 4.1231055  7.141428   9.219543  10.908711  12.369316  13.674793\n",
      "  14.866068  15.968719   9.        -0.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.       ]\n",
      " [ 4.3588986  7.549834   9.746793  11.532561  13.076696  14.456831\n",
      "  15.716232  16.88194   17.972198   9.999999  -0.        -0.\n",
      "  -0.        -0.        -0.        -0.       ]\n",
      " [ 4.5825753  7.937253  10.246949  12.124354  13.747726  15.198682\n",
      "  16.52271   17.748238  18.894442  19.974981  10.999999  -0.\n",
      "  -0.        -0.        -0.        -0.       ]\n",
      " [ 4.795831   8.306623  10.7238035 12.688577  14.387493  15.905972\n",
      "  17.291615  18.574173  19.773718  20.904543  21.977257  11.999998\n",
      "  -0.        -0.        -0.        -0.       ]\n",
      " [ 5.         8.660254  11.180339  13.228756  15.        16.583122\n",
      "  18.027756  19.364916  20.615528  21.794493  22.912876  23.979156\n",
      "  13.        -0.        -0.        -0.       ]\n",
      " [ 5.196152   8.999999  11.618949  13.747726  15.588457  17.233686\n",
      "  18.734993  20.12461   21.424284  22.6495    23.811758  24.919868\n",
      "  25.98076   13.999999  -0.        -0.       ]\n",
      " [ 5.3851647  9.327379  12.041594  14.247807  16.155495  17.86057\n",
      "  19.416487  20.856653  22.203602  23.473387  24.677923  25.826342\n",
      "  26.925823  27.982136  14.999999  -0.       ]\n",
      " [ 5.5677643  9.64365   12.449899  14.730919  16.703293  18.466185\n",
      "  20.07486   21.563858  22.95648   24.26932   25.5147    26.702057\n",
      "  27.838821  28.93095   29.983328  16.       ]]\n",
      "B before descritize:\n",
      "[1.        1.7320508 2.2360678 2.6457512 3.        3.3166246 3.6055512\n",
      " 3.8729832 4.1231055 4.3588986 4.5825753 4.795831  5.        5.196152\n",
      " 5.3851647 5.5677643]\n",
      "A after descritize:\n",
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "B after descritize:\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "input from within the call:\n",
      "[[[-0.2146867 ]]\n",
      "\n",
      " [[-0.25820348]]\n",
      "\n",
      " [[ 0.8527709 ]]\n",
      "\n",
      " [[ 0.50247186]]\n",
      "\n",
      " [[ 0.17401066]]\n",
      "\n",
      " [[ 1.292949  ]]\n",
      "\n",
      " [[ 0.32262412]]\n",
      "\n",
      " [[ 1.7802873 ]]]\n",
      "\n",
      "\n",
      "Ab:\n",
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "Bb:\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "Cb:\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "Db:\n",
      "[0.]\n",
      "u:\n",
      "[[[-0.2146867 ]]\n",
      "\n",
      " [[-0.25820348]]\n",
      "\n",
      " [[ 0.8527709 ]]\n",
      "\n",
      " [[ 0.50247186]]\n",
      "\n",
      " [[ 0.17401066]]\n",
      "\n",
      " [[ 1.292949  ]]\n",
      "\n",
      " [[ 0.32262412]]\n",
      "\n",
      " [[ 1.7802873 ]]]\n",
      "x0:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "x_k_1:\n",
      "Traced<ShapedArray(float32[16])>with<DynamicJaxprTrace(level=1/0)>\n",
      "u_k:\n",
      "Traced<ShapedArray(float32[1,1])>with<DynamicJaxprTrace(level=1/0)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.virtualenvs/jax-pytorch/lib/python3.8/site-packages/scipy/signal/_lti_conversion.py:460: LinAlgWarning: Ill-conditioned matrix (rcond=7.70383e-09): result may not be accurate.\n",
      "  ad = linalg.solve(ima, np.eye(a.shape[0]) + (1.0-alpha)*dt*a)\n",
      "/home/beegass/.virtualenvs/jax-pytorch/lib/python3.8/site-packages/scipy/signal/_lti_conversion.py:461: LinAlgWarning: Ill-conditioned matrix (rcond=7.70383e-09): result may not be accurate.\n",
      "  bd = linalg.solve(ima, dt*b)\n",
      "/home/beegass/.virtualenvs/jax-pytorch/lib/python3.8/site-packages/scipy/signal/_lti_conversion.py:464: LinAlgWarning: Ill-conditioned matrix (rcond=7.19654e-09): result may not be accurate.\n",
      "  cd = linalg.solve(ima.transpose(), c.transpose())\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (16,) and (1,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/beegass/Documents/Coding/S4/HiPPO.ipynb Cell 37'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000036?line=0'>1</a>\u001b[0m test()\n",
      "\u001b[1;32m/home/beegass/Documents/Coding/S4/HiPPO.ipynb Cell 36'\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000035?line=63'>64</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000035?line=64'>65</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput type:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(x)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000035?line=66'>67</a>\u001b[0m params \u001b[39m=\u001b[39m hippo_LegS_B\u001b[39m.\u001b[39;49minit(key2, x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000035?line=68'>69</a>\u001b[0m y_legs \u001b[39m=\u001b[39m hippo_LegS_B\u001b[39m.\u001b[39mapply(params, x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000035?line=70'>71</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mU-y shape for LegS:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00my_legs\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "\u001b[1;32m/home/beegass/Documents/Coding/S4/HiPPO.ipynb Cell 35'\u001b[0m in \u001b[0;36mHiPPO.__call__\u001b[0;34m(self, u, kernel)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=53'>54</a>\u001b[0m     \u001b[39mprint\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=54'>55</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput from within the call:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mu[:, jnp\u001b[39m.\u001b[39mnewaxis]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=55'>56</a>\u001b[0m     c_k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_SSM(Ab, Bb, Cb, Db, u[:, jnp\u001b[39m.\u001b[39;49mnewaxis], x0\u001b[39m=\u001b[39;49mjnp\u001b[39m.\u001b[39;49mzeros((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mN, )))[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=56'>57</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=57'>58</a>\u001b[0m     Ab, Bb, Cb, Db \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscretize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mB, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD, step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep, alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGBT_alpha)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "\u001b[1;32m/home/beegass/Documents/Coding/S4/HiPPO.ipynb Cell 35'\u001b[0m in \u001b[0;36mHiPPO.scan_SSM\u001b[0;34m(self, Ab, Bb, Cb, Db, u, x0)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=174'>175</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_k 2nd term shape:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m(Db \u001b[39m@\u001b[39m u_k)\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=175'>176</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x_k, y_k\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=177'>178</a>\u001b[0m \u001b[39mreturn\u001b[39;00m jax\u001b[39m.\u001b[39;49mlax\u001b[39m.\u001b[39;49mscan(step, x0, u)\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "\u001b[1;32m/home/beegass/Documents/Coding/S4/HiPPO.ipynb Cell 35'\u001b[0m in \u001b[0;36mHiPPO.scan_SSM.<locals>.step\u001b[0;34m(x_k_1, u_k)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=164'>165</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx_k_1:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mx_k_1\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=165'>166</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mu_k:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mu_k\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=167'>168</a>\u001b[0m x_k \u001b[39m=\u001b[39m (Ab \u001b[39m@\u001b[39m x_k_1) \u001b[39m+\u001b[39m (Bb \u001b[39m@\u001b[39;49m u_k)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=168'>169</a>\u001b[0m y_k \u001b[39m=\u001b[39m (Cb \u001b[39m@\u001b[39m x_k) \u001b[39m+\u001b[39m (Db \u001b[39m@\u001b[39m u_k)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=170'>171</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx_k 1st term shape:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m(Ab \u001b[39m@\u001b[39m x_k_1)\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/jax-pytorch/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:4555\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4553\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[1;32m   4554\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m-> 4555\u001b[0m \u001b[39mreturn\u001b[39;00m binary_op(\u001b[39mself\u001b[39;49m, other)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/jax-pytorch/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2767\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   2765\u001b[0m a \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39msqueeze(a, \u001b[39mtuple\u001b[39m(a_squeeze))\n\u001b[1;32m   2766\u001b[0m b \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39msqueeze(b, \u001b[39mtuple\u001b[39m(b_squeeze))\n\u001b[0;32m-> 2767\u001b[0m out \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39;49mdot_general(\n\u001b[1;32m   2768\u001b[0m   a, b, (((ndim(a) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m,), (ndim(b) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m b_is_mat,)), (a_batch, b_batch)),\n\u001b[1;32m   2769\u001b[0m   precision\u001b[39m=\u001b[39;49mprecision)\n\u001b[1;32m   2770\u001b[0m \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39mtranspose(out, perm)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/jax-pytorch/lib/python3.8/site-packages/jax/_src/lax/lax.py:2409\u001b[0m, in \u001b[0;36m_dot_general_shape_rule\u001b[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m   2406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m core\u001b[39m.\u001b[39msymbolic_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[1;32m   2407\u001b[0m   msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2408\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mshape, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2409\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[1;32m   2411\u001b[0m \u001b[39mreturn\u001b[39;00m _dot_general_shape_computation(lhs\u001b[39m.\u001b[39mshape, rhs\u001b[39m.\u001b[39mshape, dimension_numbers)\n",
      "\u001b[0;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (16,) and (1,)."
     ]
    }
   ],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('jax-pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbeba54f39f0ad863615cd2814766ffd78084a8276e5c331aa23b4d5ff4f068c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
