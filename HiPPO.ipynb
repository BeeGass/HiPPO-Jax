{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiPPO Matrices\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import packages\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import linen as jnn\n",
    "\n",
    "from jax.nn.initializers import lecun_normal, uniform\n",
    "from jax.numpy.linalg import eig, inv, matrix_power\n",
    "from jax.scipy.signal import convolve\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from scipy import linalg as la\n",
    "from scipy import signal\n",
    "from scipy import special as ss\n",
    "\n",
    "import math\n",
    "\n",
    "## setup JAX to use TPUs if available\n",
    "try:\n",
    "    url = 'http:' + os.environ['TPU_NAME'].split(':')[1] + ':8475/requestversion/tpu_driver_nightly'\n",
    "    resp = requests.post(url)\n",
    "    jax.config.FLAGS.jax_xla_backend = 'tpu_driver'\n",
    "    jax.config.FLAGS.jax_backend_target = os.environ['TPU_NAME']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/beegass/Documents/Coding/S4/HiPPO.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000038?line=0'>1</a>\u001b[0m rng, key2, key3, key4, key5 \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(key)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 2)"
     ]
    }
   ],
   "source": [
    "rng, key2, key3, key4, key5 = jax.random.split(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_SSM(rng, N):\n",
    "    a_r, b_r, c_r = jax.random.split(rng, 3)\n",
    "    A = jax.random.uniform(a_r, (N, N))\n",
    "    B = jax.random.uniform(b_r, (N, 1))\n",
    "    C = jax.random.uniform(c_r, (1, N))\n",
    "    return A, B, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate The HiPPO Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Legendre (LegT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Legendre (LegT) - vectorized\n",
    "def build_LegT_V(N, lambda_n=1):\n",
    "    q = jnp.arange(N, dtype=jnp.float64)\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    case = jnp.power(-1.0, (n-k))\n",
    "    A = None\n",
    "    B = None\n",
    "    \n",
    "    if lambda_n == 1:\n",
    "        A_base = -jnp.sqrt(2*n+1) * jnp.sqrt(2*k+1)\n",
    "        pre_D = jnp.sqrt(jnp.diag(2*q+1))\n",
    "        B = D = jnp.diag(pre_D)[:, None]\n",
    "        A = jnp.where(k <= n, A_base, A_base * case) # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "        \n",
    "    elif lambda_n == 2: #(jnp.sqrt(2*n+1) * jnp.power(-1, n)):\n",
    "        A_base = -(2*n+1)\n",
    "        B = jnp.diag((2*q+1) * jnp.power(-1, n))[:, None]\n",
    "        A = jnp.where(k <= n, A_base * case, A_base) # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Legendre (LegT) - non-vectorized\n",
    "def build_LegT(N, legt_type=\"legt\"):\n",
    "    Q = jnp.arange(N, dtype=jnp.float64)\n",
    "    pre_R = (2*Q + 1)\n",
    "    k, n = jnp.meshgrid(Q, Q)\n",
    "        \n",
    "    if legt_type == \"legt\":\n",
    "        R = jnp.sqrt(pre_R)\n",
    "        A = R[:, None] * jnp.where(n < k, (-1.)**(n-k), 1) * R[None, :]\n",
    "        B = R[:, None]\n",
    "        A = -A\n",
    "\n",
    "        # Halve again for timescale correctness\n",
    "        # A, B = A/2, B/2\n",
    "        #A *= 0.5\n",
    "        #B *= 0.5\n",
    "        \n",
    "    elif legt_type == \"lmu\":\n",
    "        R = pre_R[:, None]\n",
    "        A = jnp.where(n < k, -1, (-1.)**(n-k+1)) * R\n",
    "        B = (-1.)**Q[:, None] * R\n",
    "        \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      " [[-1. -1. -1. -1. -1.]\n",
      " [ 3. -3. -3. -3. -3.]\n",
      " [-5.  5. -5. -5. -5.]\n",
      " [ 7. -7.  7. -7. -7.]\n",
      " [-9.  9. -9.  9. -9.]]\n",
      "v:\n",
      " [[-1. -1. -1. -1. -1.]\n",
      " [ 3. -3. -3. -3. -3.]\n",
      " [-5.  5. -5. -5. -5.]\n",
      " [ 7. -7.  7. -7. -7.]\n",
      " [-9.  9. -9.  9. -9.]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_LegT_A, nv_LegT_B = build_LegT(N=N, legt_type=\"lmu\")\n",
    "LegT_A, LegT_B = build_LegT_V(N=N, lambda_n=2)\n",
    "print(f\"nv:\\n\", nv_LegT_A)\n",
    "print(f\"v:\\n\", LegT_A)\n",
    "# print(f\"nv:\\n\", nv_LegT_B)\n",
    "# print(f\"v:\\n\", LegT_B)\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_LegT_A, LegT_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_LegT_B, LegT_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Laguerre (LagT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Laguerre (LagT) - non-vectorized\n",
    "def build_LagT(alpha, beta, N):\n",
    "    A = -jnp.eye(N) * (1 + beta) / 2 - jnp.tril(jnp.ones((N, N)), -1)\n",
    "    B = ss.binom(alpha + jnp.arange(N), jnp.arange(N))[:, None]\n",
    "\n",
    "    L = jnp.exp(.5 * (ss.gammaln(jnp.arange(N)+alpha+1) - ss.gammaln(jnp.arange(N)+1)))\n",
    "    A = (1./L[:, None]) * A * L[None, :]\n",
    "    B = (1./L[:, None]) * B * jnp.exp(-.5 * ss.gammaln(1-alpha)) * beta**((1-alpha)/2)\n",
    "    \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Laguerre (LagT) - non-vectorized\n",
    "def build_LagT_V(alpha, beta, N):\n",
    "    L = jnp.exp(.5 * (ss.gammaln(jnp.arange(N)+alpha+1) - ss.gammaln(jnp.arange(N)+1)))\n",
    "    inv_L = 1./L[:, None]\n",
    "    pre_A = (jnp.eye(N) * ((1 + beta) / 2)) + jnp.tril(jnp.ones((N, N)), -1)\n",
    "    pre_B = ss.binom(alpha + jnp.arange(N), jnp.arange(N))[:, None]\n",
    "    \n",
    "    A = -inv_L * pre_A * L[None, :]\n",
    "    B =  jnp.exp(-.5 * ss.gammaln(1-alpha)) * jnp.power(beta, (1-alpha)/2) * inv_L * pre_B \n",
    "    \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      "[[-1. -0. -0. -0. -0.]\n",
      " [-1. -1. -0. -0. -0.]\n",
      " [-1. -1. -1. -0. -0.]\n",
      " [-1. -1. -1. -1. -0.]\n",
      " [-1. -1. -1. -1. -1.]]\n",
      "v:\n",
      "[[-1. -0. -0. -0. -0.]\n",
      " [-1. -1. -0. -0. -0.]\n",
      " [-1. -1. -1. -0. -0.]\n",
      " [-1. -1. -1. -1. -0.]\n",
      " [-1. -1. -1. -1. -1.]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_LagT_A, nv_LagT_B = build_LagT(alpha=0, beta=1, N=N)\n",
    "LagT_A, LagT_B = build_LagT_V(alpha=0, beta=1, N=N)\n",
    "print(f\"nv:\\n{nv_LagT_A}\")\n",
    "print(f\"v:\\n{LagT_A}\")\n",
    "# print(f\"nv:\\n{nv_LagT_B}\")\n",
    "# print(f\"v:\\n{LagT_B}\")\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_LagT_A, LagT_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_LagT_B, LagT_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Legendre (LegS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled Legendre (LegS) vectorized\n",
    "def build_LegS_V(N):\n",
    "    q = jnp.arange(N, dtype=jnp.float64)\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    pre_D = jnp.sqrt(jnp.diag(2*q+1))\n",
    "    B = D = jnp.diag(pre_D)[:, None]\n",
    "    \n",
    "    A_base = (-jnp.sqrt(2*n+1)) * jnp.sqrt(2*k+1)\n",
    "    case_2 = (n+1)/(2*n+1)\n",
    "    \n",
    "    A = jnp.where(n > k, A_base, 0.0) # if n > k, then A_base is used, otherwise 0\n",
    "    A = jnp.where(n == k, (A_base * case_2), A) # if n == k, then A_base is used, otherwise A\n",
    "    \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled Legendre (LegS), non-vectorized\n",
    "def build_LegS(N):\n",
    "    q = jnp.arange(N, dtype=jnp.float64)  # q represents the values 1, 2, ..., N each column has\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    r = 2 * q + 1\n",
    "    M = -(jnp.where(n >= k, r, 0) - jnp.diag(q)) # represents the state matrix M \n",
    "    D = jnp.sqrt(jnp.diag(2 * q + 1)) # represents the diagonal matrix D $D := \\text{diag}[(2n+1)^{\\frac{1}{2}}]^{N-1}_{n=0}$\n",
    "    A = D @ M @ jnp.linalg.inv(D)\n",
    "    B = jnp.diag(D)[:, None]\n",
    "    B = B.copy() # Otherwise \"UserWarning: given NumPY array is not writeable...\" after torch.as_tensor(B)\n",
    "    \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      "[[-1.         0.         0.         0.         0.       ]\n",
      " [-1.7320508 -1.9999999  0.         0.         0.       ]\n",
      " [-2.236068  -3.8729835 -3.         0.         0.       ]\n",
      " [-2.6457512 -4.582576  -5.9160795 -4.         0.       ]\n",
      " [-3.        -5.196152  -6.708204  -7.9372544 -5.       ]]\n",
      "v:\n",
      "[[-1.         0.         0.         0.         0.       ]\n",
      " [-1.7320508 -2.         0.         0.         0.       ]\n",
      " [-2.236068  -3.8729832 -3.         0.         0.       ]\n",
      " [-2.6457512 -4.5825753 -5.9160795 -4.         0.       ]\n",
      " [-3.        -5.196152  -6.7082043 -7.937254  -5.       ]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_LegS_A, nv_LegS_B = build_LegS(N=N)\n",
    "LegS_A, LegS_B = build_LegS_V(N=N)\n",
    "print(f\"nv:\\n{nv_LegS_A}\")\n",
    "print(f\"v:\\n{LegS_A}\")\n",
    "# print(f\"nv:\\n{nv_LegS_B}\")\n",
    "# print(f\"v:\\n{LegS_B}\")\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_LegS_A, LegS_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_LegS_B, LegS_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourier Basis OPs and functions - vectorized\n",
    "def build_Fourier_V(N, fourier_type='FRU'):    \n",
    "    q = jnp.arange((N//2)*2, dtype=jnp.float64)\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    \n",
    "    n_odd = n % 2 == 0\n",
    "    k_odd = k % 2 == 0\n",
    "    \n",
    "    case_1 = (n==k) & (n==0)\n",
    "    case_2_3 = ((k==0) & (n_odd)) | ((n==0) & (k_odd))\n",
    "    case_4 = (n_odd) & (k_odd)\n",
    "    case_5 = (n-k==1) & (k_odd)\n",
    "    case_6 = (k-n==1) & (n_odd)\n",
    "    \n",
    "    A = None\n",
    "    B = None\n",
    "    \n",
    "    if fourier_type == \"FRU\": # Fourier Recurrent Unit (FRU) - vectorized\n",
    "        A = jnp.diag(jnp.stack([jnp.zeros(N//2), jnp.zeros(N//2)], axis=-1).reshape(-1))\n",
    "        B = jnp.zeros(A.shape[1], dtype=jnp.float64)\n",
    "        q = jnp.arange((N//2)*2, dtype=jnp.float64)\n",
    "        \n",
    "        A = jnp.where(case_1, -1.0, \n",
    "                    jnp.where(case_2_3, -jnp.sqrt(2),\n",
    "                                jnp.where(case_4, -2, \n",
    "                                        jnp.where(case_5, jnp.pi * (n//2), \n",
    "                                                    jnp.where(case_6, -jnp.pi * (k//2), 0.0)))))\n",
    "        \n",
    "        B = B.at[::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "        \n",
    "    elif fourier_type == \"FouT\": # truncated Fourier (FouT) - vectorized\n",
    "        A = jnp.diag(jnp.stack([jnp.zeros(N//2), jnp.zeros(N//2)], axis=-1).reshape(-1))\n",
    "        B = jnp.zeros(A.shape[1], dtype=jnp.float64)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "        n_odd = n % 2 == 0\n",
    "        k_odd = k % 2 == 0\n",
    "        \n",
    "        A = jnp.where(case_1, -1.0, \n",
    "                    jnp.where(case_2_3, -jnp.sqrt(2),\n",
    "                                jnp.where(case_4, -2, \n",
    "                                        jnp.where(case_5, jnp.pi * (n//2), \n",
    "                                                    jnp.where(case_6, -jnp.pi * (k//2), 0.0)))))\n",
    "        \n",
    "        B = B.at[::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "        \n",
    "        A = 2 * A\n",
    "        B = 2 * B\n",
    "        \n",
    "    elif fourier_type == \"fourier_decay\":\n",
    "        A = jnp.diag(jnp.stack([jnp.zeros(N//2), jnp.zeros(N//2)], axis=-1).reshape(-1))\n",
    "        B = jnp.zeros(A.shape[1], dtype=jnp.float64)\n",
    "        \n",
    "        A = jnp.where(case_1, -1.0, \n",
    "                    jnp.where(case_2_3, -jnp.sqrt(2),\n",
    "                                jnp.where(case_4, -2, \n",
    "                                        jnp.where(case_5, 2 * jnp.pi * (n//2), \n",
    "                                                    jnp.where(case_6, 2 * -jnp.pi * (k//2), 0.0)))))\n",
    "        \n",
    "        B = B.at[::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "        \n",
    "        A = 0.5 * A\n",
    "        B = 0.5 * B\n",
    "        \n",
    "    \n",
    "    \n",
    "    B = B[:, None]\n",
    "        \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Fourier(N, fourier_type='FRU'):\n",
    "    freqs = jnp.arange(N//2)\n",
    "    \n",
    "    if fourier_type == \"FRU\": # Fourier Recurrent Unit (FRU) - non-vectorized\n",
    "        d = jnp.stack([jnp.zeros(N//2), freqs], axis=-1).reshape(-1)[1:]\n",
    "        A = jnp.pi*(-jnp.diag(d, 1) + jnp.diag(d, -1))\n",
    "        \n",
    "        B = jnp.zeros(A.shape[1])\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "        \n",
    "        A = A - B[:, None] * B[None, :]\n",
    "        B = B[:, None]\n",
    "\n",
    "    elif fourier_type == \"FouT\": # truncated Fourier (FouT) - non-vectorized\n",
    "        freqs *= 2\n",
    "        d = jnp.stack([jnp.zeros(N//2), freqs], axis=-1).reshape(-1)[1:]\n",
    "        A = jnp.pi*(-jnp.diag(d, 1) + jnp.diag(d, -1))\n",
    "        \n",
    "        B = jnp.zeros(A.shape[1])\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        # Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case\n",
    "        A = A - B[:, None] * B[None, :] * 2\n",
    "        B = B[:, None] * 2\n",
    "        \n",
    "    elif fourier_type == \"fourier_decay\":\n",
    "        d = jnp.stack([jnp.zeros(N//2), freqs], axis=-1).reshape(-1)[1:]\n",
    "        A = jnp.pi*(-jnp.diag(d, 1) + jnp.diag(d, -1))\n",
    "        \n",
    "        B = jnp.zeros(A.shape[1])\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        # Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case\n",
    "        A = A - 0.5 * B[:, None] * B[None, :]\n",
    "        B = 0.5 * B[:, None]\n",
    "        \n",
    "            \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      "[[-1.         0.        -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999 -3.1415927]\n",
      " [ 0.         0.         3.1415927  0.       ]]\n",
      "v:\n",
      "[[-1.        -0.        -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -2.        -3.1415927]\n",
      " [ 0.         0.         3.1415927  0.       ]]\n",
      "A Comparison:\n",
      " True\n",
      "B Comparison:\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "nv_Fourier_A, nv_Fourier_B = build_Fourier(N=N, fourier_type='FRU')\n",
    "Fourier_A, Fourier_B = build_Fourier_V(N=N, fourier_type='FRU')\n",
    "print(f\"nv:\\n{nv_Fourier_A}\")\n",
    "print(f\"v:\\n{Fourier_A}\")\n",
    "# print(f\"nv:\\n{nv_Fourier_B}\")\n",
    "# print(f\"v:\\n{Fourier_B}\")\n",
    "print(f\"A Comparison:\\n {jnp.allclose(nv_Fourier_A, Fourier_A)}\")\n",
    "print(f\"B Comparison:\\n {jnp.allclose(nv_Fourier_B, Fourier_B)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_HiPPO(N, v='nv', measure=\"legs\", lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1):\n",
    "    \"\"\"\n",
    "        Instantiates the HiPPO matrix of a given order using a particular measure. \n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            v (str): choose between this repo's implementation or hazy research's implementation.\n",
    "            measure (str): \n",
    "                choose between \n",
    "                    - HiPPO w/ Translated Legendre (LegT) - legt\n",
    "                    - HiPPO w/ Translated Laguerre (LagT) - lagt\n",
    "                    - HiPPO w/ Scaled Legendre (LegS) - legs\n",
    "                    - HiPPO w/ Fourier basis - fourier\n",
    "                        - FRU: Fourier Recurrent Unit \n",
    "                        - FouT: Translated Fourier \n",
    "            lambda_n (int): The amount of tilt applied to the HiPPO-LegS basis, determines between LegS and LMU. \n",
    "            fourier_type (str): chooses between the following:\n",
    "                - FRU: Fourier Recurrent Unit - fru\n",
    "                - FouT: Translated Fourier - fout\n",
    "                - FourD: Fourier Decay - fourd\n",
    "            alpha (float): The order of the Laguerre basis.\n",
    "            beta (float): The scale of the Laguerre basis.\n",
    "            \n",
    "        Returns:\n",
    "            A (jnp.ndarray): The HiPPO matrix multiplied by -1.\n",
    "            B (jnp.ndarray): The other corresponding state space matrix. \n",
    "            \n",
    "    \"\"\"\n",
    "    A = None\n",
    "    B = None\n",
    "    if measure == \"legt\":\n",
    "        if v == 'nv':\n",
    "            A, B = build_LegT(N=N, lambda_n=lambda_n)\n",
    "        else:\n",
    "            A, B = build_LegT_V(N=N, lambda_n=lambda_n) \n",
    "        \n",
    "    elif measure == \"lagt\":\n",
    "        if v == 'nv':\n",
    "            A, B = build_LagT(alpha=alpha, beta=beta, N=N)\n",
    "        else:\n",
    "            A, B = build_LagT_V(alpha=alpha, beta=beta, N=N)\n",
    "        \n",
    "    elif measure == \"legs\":\n",
    "        if v == 'nv':\n",
    "            A, B = build_LegS(N=N)\n",
    "        else:\n",
    "            A, B = build_LegS_V(N=N)\n",
    "        \n",
    "    elif measure == \"fourier\":\n",
    "        if v == 'nv':\n",
    "            A, B = build_Fourier(N=N, fourier_type=fourier_type)\n",
    "        else:\n",
    "            A, B = build_Fourier_V(N=N, fourier_type=fourier_type)\n",
    "        \n",
    "    elif measure == \"random\":\n",
    "        A = jnp.random.randn(N, N) / N\n",
    "        B = jnp.random.randn(N, 1)\n",
    "        \n",
    "    elif measure == \"diagonal\":\n",
    "        A = -jnp.diag(jnp.exp(jnp.random.randn(N)))\n",
    "        B = jnp.random.randn(N, 1)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid HiPPO type\")\n",
    "    \n",
    "    A_copy = A.copy()\n",
    "    B_copy = B.copy()\n",
    "    \n",
    "    return -jnp.array(A_copy), B_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      " [[ 1.        -0.        -0.        -0.        -0.       ]\n",
      " [ 1.7320508  1.9999999 -0.        -0.        -0.       ]\n",
      " [ 2.236068   3.8729835  3.        -0.        -0.       ]\n",
      " [ 2.6457512  4.582576   5.9160795  4.        -0.       ]\n",
      " [ 3.         5.196152   6.708204   7.9372544  5.       ]]\n",
      "v:\n",
      " [[ 1.        -0.        -0.        -0.        -0.       ]\n",
      " [ 1.7320508  2.        -0.        -0.        -0.       ]\n",
      " [ 2.236068   3.8729832  3.        -0.        -0.       ]\n",
      " [ 2.6457512  4.5825753  5.9160795  4.        -0.       ]\n",
      " [ 3.         5.196152   6.7082043  7.937254   5.       ]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_A, nv_B = make_HiPPO(N=N, v='nv', measure=\"legs\", lambda_n=1, fourier_type=\"FRU\", alpha=0, beta=1)\n",
    "v_A, v_B = make_HiPPO(N=N, v='v', measure=\"legs\", lambda_n=1, fourier_type=\"FRU\", alpha=0, beta=1)\n",
    "print(f\"nv:\\n\", nv_A)\n",
    "print(f\"v:\\n\", v_A)\n",
    "# print(f\"nv:\\n\", nv_B)\n",
    "# print(f\"v:\\n\", v_B)\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_A, v_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_B, v_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def discretize(A, B, C, step, alpha=0.5):\n",
    "#     '''\n",
    "#     - forward Euler corresponds to α = 0,\n",
    "#     - backward Euler corresponds to α = 1,\n",
    "#     - bilinear corresponds to α = 0.5,\n",
    "#     - Zero-order Hold corresponds to α > 1\n",
    "#     '''\n",
    "#     I = jnp.eye(A.shape[0])\n",
    "#     GBT = jnp.linalg.inv(I - ((step * alpha) * A))\n",
    "#     GBT_A = GBT @ (I + ((step * (1-alpha)) * A))\n",
    "#     GBT_B = (step * GBT) @ B\n",
    "    \n",
    "#     if alpha > 1: # Zero-order Hold\n",
    "#         GBT_A = jax.scipy.linalg.expm(step * A)\n",
    "#         GBT_B = (jnp.linalg.inv(A) @ (jax.scipy.linalg.expm(step * A) - I)) @ B \n",
    "    \n",
    "#     return GBT_A, GBT_B, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def discretize_b(A, B, A_stacked, B_stacked, N, max_length=1024, discretization='bilinear'):\n",
    "#     for t in range(1, max_length + 1):\n",
    "#         At = A / t\n",
    "#         Bt = B / t\n",
    "#         if discretization == 'forward':\n",
    "#             A_stacked[t - 1] = jnp.eye(N) + At\n",
    "#             B_stacked[t - 1] = Bt\n",
    "#         elif discretization == 'backward':\n",
    "#             A_stacked[t - 1] = la.solve_triangular(jnp.eye(N) - At, jnp.eye(N), lower=True)\n",
    "#             B_stacked[t - 1] = la.solve_triangular(jnp.eye(N) - At, Bt, lower=True)\n",
    "#         elif discretization == 'bilinear':\n",
    "            \n",
    "#             A_stacked[t - 1] = la.solve_triangular(jnp.eye(N) - At / 2, jnp.eye(N) + At / 2, lower=True)\n",
    "#             B_stacked[t - 1] = la.solve_triangular(jnp.eye(N) - At / 2, Bt, lower=True)\n",
    "#         else: # ZOH\n",
    "#             A_stacked[t - 1] = la.expm(A * (math.log(t + 1) - math.log(t)))\n",
    "#             B_stacked[t - 1] = la.solve_triangular(A, A_stacked[t - 1] @ B - B, lower=True)\n",
    "            \n",
    "#     return A_stacked, B_stacked\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collect_SSM_vars(A, B, C, u, alpha=0.5):\n",
    "#     L = u.shape[0]\n",
    "#     N = A.shape[0]\n",
    "#     Ab, Bb, Cb = discretize(A, B, C, step=1.0 / L, alpha=alpha)\n",
    "    \n",
    "#     return Ab, Bb, Cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scan_SSM(Ab, Bb, Cb, u, x0):\n",
    "#     '''\n",
    "#     This is for returning the discretized hidden state often needed for an RNN. \n",
    "#     '''\n",
    "#     def step(x_k_1, u_k):\n",
    "#         x_k = Ab @ x_k_1 + Bb @ u_k\n",
    "#         y_k = Cb @ x_k\n",
    "#         return x_k, y_k\n",
    "\n",
    "#     return jax.lax.scan(step, x0, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Old utilities for parallel scan implementation of Linear RNNs. \"\"\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "### Utilities\n",
    "\n",
    "\n",
    "def shift_up(a, s=None, drop=True, dim=0):\n",
    "    assert dim == 0\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(a[0, ...])\n",
    "    s = s.unsqueeze(dim)\n",
    "    if drop:\n",
    "        a = a[:-1, ...]\n",
    "    return torch.cat((s, a), dim=dim)\n",
    "\n",
    "def interleave(a, b, uneven=False, dim=0):\n",
    "    \"\"\" Interleave two tensors of same shape \"\"\"\n",
    "    # assert(a.shape == b.shape)\n",
    "    assert dim == 0 # TODO temporary to make handling uneven case easier\n",
    "    if dim < 0:\n",
    "        dim = N + dim\n",
    "    if uneven:\n",
    "        a_ = a[-1:, ...]\n",
    "        a = a[:-1, ...]\n",
    "    c = torch.stack((a, b), dim+1)\n",
    "    out_shape = list(a.shape)\n",
    "    out_shape[dim] *= 2\n",
    "    c = c.view(out_shape)\n",
    "    if uneven:\n",
    "        c = torch.cat((c, a_), dim=dim)\n",
    "    return c\n",
    "\n",
    "def batch_mult(A, u, has_batch=None):\n",
    "    \"\"\" Matrix mult A @ u with special case to save memory if u has additional batch dim\n",
    "\n",
    "    The batch dimension is assumed to be the second dimension\n",
    "    A : (L, ..., N, N)\n",
    "    u : (L, [B], ..., N)\n",
    "    has_batch: True, False, or None. If None, determined automatically\n",
    "\n",
    "    Output:\n",
    "    x : (L, [B], ..., N)\n",
    "      A @ u broadcasted appropriately\n",
    "    \"\"\"\n",
    "\n",
    "    if has_batch is None:\n",
    "        has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    if has_batch:\n",
    "        u = u.permute([0] + list(range(2, len(u.shape))) + [1])\n",
    "    else:\n",
    "        u = u.unsqueeze(-1)\n",
    "    v = (A @ u)\n",
    "    if has_batch:\n",
    "        v = v.permute([0] + [len(u.shape)-1] + list(range(1, len(u.shape)-1)))\n",
    "    else:\n",
    "        v = v[..., 0]\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "### Main unrolling functions\n",
    "\n",
    "def unroll(A, u):\n",
    "    \"\"\"\n",
    "    A : (..., N, N) # TODO I think this can't take batch dimension?\n",
    "    u : (L, ..., N)\n",
    "    output : x (..., N) # TODO a lot of these shapes are wrong\n",
    "    x[i, ...] = A^{i} @ u[0, ...] + ... + A @ u[i-1, ...] + u[i, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    m = u.new_zeros(u.shape[1:])\n",
    "    outputs = []\n",
    "    for u_ in torch.unbind(u, dim=0):\n",
    "        m = F.linear(m, A) + u_\n",
    "        outputs.append(m)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive(A, u):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of unroll. \"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_(A, u):\n",
    "        if u.shape[0] == 1:\n",
    "            return u\n",
    "\n",
    "        u_evens = u[0::2, ...]\n",
    "        u_odds = u[1::2, ...]\n",
    "\n",
    "        # u2 = F.linear(u_evens, A) + u_odds\n",
    "        u2 = (A @ u_evens.unsqueeze(-1)).squeeze(-1) + u_odds\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_odds = parallel_unroll_recursive_(A2, u2)\n",
    "        # x_evens = F.linear(shift_up(x_odds), A) + u_evens\n",
    "        x_evens = (A @ shift_up(x_odds).unsqueeze(-1)).squeeze(-1) + u_evens\n",
    "\n",
    "        x = interleave(x_evens, x_odds, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n)/math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N-u.shape[0],) + u.shape[1:] )), dim=0)\n",
    "\n",
    "    return parallel_unroll_recursive_(A, u)[:n, ...]\n",
    "\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive_br(A, u):\n",
    "    \"\"\" Same as parallel_unroll_recursive but uses bit reversal for locality. \"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_br_(A, u):\n",
    "        n = u.shape[0]\n",
    "        if n == 1:\n",
    "            return u\n",
    "\n",
    "        m = n//2\n",
    "        u_0 = u[:m, ...]\n",
    "        u_1 = u[m:, ...]\n",
    "\n",
    "        u2 = F.linear(u_0, A) + u_1\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_1 = parallel_unroll_recursive_br_(A2, u2)\n",
    "        x_0 = F.linear(shift_up(x_1), A) + u_0\n",
    "\n",
    "        # x = torch.cat((x_0, x_1), dim=0) # is there a way to do this with cat?\n",
    "        x = interleave(x_0, x_1, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n)/math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N-u.shape[0],) + u.shape[1:] )), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    x = parallel_unroll_recursive_br_(A, u)\n",
    "    return x[:n, ...]\n",
    "\n",
    "def parallel_unroll_iterative(A, u):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of unroll, implemented iteratively \"\"\"\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n)/math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N-u.shape[0],) + u.shape[1:] )), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    # Main recursive loop, flattened\n",
    "    us = [] # stores the u_0 terms in the recursive version\n",
    "    N_ = N\n",
    "    As = [] # stores the A matrices\n",
    "    for l in range(m):\n",
    "        N_ = N_ // 2\n",
    "        As.append(A)\n",
    "        u_0 = u[:N_, ...]\n",
    "        us.append(u_0)\n",
    "        u = F.linear(u_0, A) + u[N_:, ...]\n",
    "        A = A @ A\n",
    "    x_0 = []\n",
    "    x = u # x_1\n",
    "    for l in range(m-1, -1, -1):\n",
    "        x_0 = F.linear(shift_up(x), As[l]) + us[l]\n",
    "        x = interleave(x_0, x, dim=0)\n",
    "\n",
    "    return x[:n, ...]\n",
    "\n",
    "\n",
    "def variable_unroll_sequential(A, u, s=None, variable=True):\n",
    "    \"\"\" Unroll with variable (in time/length) transitions A.\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] @ s + A[i..1] @ u[0] + ... + A[i] @ u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        # s = F.linear(s, A_) + u_\n",
    "        # print(\"shapes\", A_.shape, s.shape, has_batch)\n",
    "        s = batch_mult(A_.unsqueeze(0), s.unsqueeze(0), has_batch)[0]\n",
    "        # breakpoint()\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def variable_unroll(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of variable_unroll. \"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        return variable_unroll_sequential(A, u, s, variable)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1  = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1  = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = A_1 @ A_0_\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll(A_10, u_10, s, variable, recurse_limit)\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "\n",
    "    x = interleave(x_0, x_1, uneven, dim=0) # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "def variable_unroll_general_sequential(A, u, s, op, variable=True):\n",
    "    \"\"\" Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        s = op(A_, s)\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "def variable_unroll_matrix_sequential(A, u, s=None, variable=True):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    return variable_unroll_general_sequential(A, u, s, op, variable=True)\n",
    "\n",
    "def variable_unroll_toeplitz_sequential(A, u, s=None, variable=True, pad=False):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        # print(\"shapes\", A.shape, u.shape)\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        # print(\"shapes\", A.shape, u.shape)\n",
    "        ret = variable_unroll_general_sequential(A, u, s, triangular_toeplitz_multiply_padded, variable=True)\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    return variable_unroll_general_sequential(A, u, s, triangular_toeplitz_multiply, variable=True)\n",
    "\n",
    "\n",
    "\n",
    "### General parallel scan functions with generic binary composition operators\n",
    "\n",
    "def variable_unroll_general(A, u, s, op, compose_op=None, sequential_op=None, variable=True, recurse_limit=16):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of variable_unroll.\n",
    "\n",
    "    compose is an optional function that defines how to compose A without multiplying by a leaf u\n",
    "    \"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        if sequential_op is None:\n",
    "            sequential_op = op\n",
    "        return variable_unroll_general_sequential(A, u, s, sequential_op, variable)\n",
    "\n",
    "    if compose_op is None:\n",
    "        compose_op = op\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1 = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1 = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = op(A_1, u_0_) # batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = compose_op(A_1, A_0_)\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll_general(A_10, u_10, s, op, compose_op, sequential_op, variable=variable, recurse_limit=recurse_limit)\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = op(A_0, x_0) # batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "\n",
    "    x = interleave(x_0, x_1, uneven, dim=0) # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "def variable_unroll_matrix(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "    op = lambda x, y: batch_mult(x, y, has_batch)\n",
    "    sequential_op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    matmul = lambda x, y: x @ y\n",
    "    return variable_unroll_general(A, u, s, op, compose_op=matmul, sequential_op=sequential_op, variable=variable, recurse_limit=recurse_limit)\n",
    "\n",
    "def variable_unroll_toeplitz(A, u, s=None, variable=True, recurse_limit=8, pad=False):\n",
    "    \"\"\" Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (L, [B], ..., N) same shape as u\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "    # Add the batch dimension to A if necessary\n",
    "    A_batch_dims = len(A.shape) - int(variable)\n",
    "    u_batch_dims = len(u.shape)-1\n",
    "    if u_batch_dims > A_batch_dims:\n",
    "        # assert u_batch_dims == A_batch_dims + 1\n",
    "        if variable:\n",
    "            while len(A.shape) < len(u.shape):\n",
    "                A = A.unsqueeze(1)\n",
    "        # else:\n",
    "        #     A = A.unsqueeze(0)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        # print(\"shapes\", A.shape, u.shape)\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        # print(\"shapes\", A.shape, u.shape)\n",
    "        op = triangular_toeplitz_multiply_padded\n",
    "        ret = variable_unroll_general(A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit)\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    op = triangular_toeplitz_multiply\n",
    "    ret = variable_unroll_general(A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit)\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "### Testing\n",
    "\n",
    "def test_correctness():\n",
    "    print(\"Testing Correctness\\n====================\")\n",
    "\n",
    "    # Test sequential unroll\n",
    "    L = 3\n",
    "    A = torch.Tensor([[1, 1], [1, 0]])\n",
    "    u = torch.ones((L, 2))\n",
    "    x = unroll(A, u)\n",
    "    assert torch.isclose(x, torch.Tensor([[1., 1.], [3., 2.], [6., 4.]])).all()\n",
    "\n",
    "    # Test utilities\n",
    "    assert torch.isclose(shift_up(x), torch.Tensor([[0., 0.], [1., 1.], [3., 2.]])).all()\n",
    "    assert torch.isclose(interleave(x, x), torch.Tensor([[1., 1.], [1., 1.], [3., 2.], [3., 2.], [6., 4.], [6., 4.]])).all()\n",
    "\n",
    "    # Test parallel unroll\n",
    "    x = parallel_unroll_recursive(A, u)\n",
    "    assert torch.isclose(x, torch.Tensor([[1., 1.], [3., 2.], [6., 4.]])).all()\n",
    "\n",
    "    # Powers\n",
    "    L = 12\n",
    "    A = torch.Tensor([[1, 0, 0], [2, 1, 0], [3, 3, 1]])\n",
    "    u = torch.ones((L, 3))\n",
    "    x = parallel_unroll_recursive(A, u)\n",
    "    print(\"recursive\", x)\n",
    "    x = parallel_unroll_recursive_br(A, u)\n",
    "    print(\"recursive_br\", x)\n",
    "    x = parallel_unroll_iterative(A, u)\n",
    "    print(\"iterative_br\", x)\n",
    "\n",
    "\n",
    "    A = A.repeat((L, 1, 1))\n",
    "    s = torch.zeros(3)\n",
    "    print(\"A shape\", A.shape)\n",
    "    x = variable_unroll_sequential(A, u, s)\n",
    "    print(\"variable_unroll\", x)\n",
    "    x = variable_unroll(A, u, s)\n",
    "    print(\"parallel_variable_unroll\", x)\n",
    "\n",
    "\n",
    "def generate_data(L, N, B=None, cuda=True):\n",
    "    A = torch.eye(N) + torch.normal(0, 1, size=(N, N)) / (N**.5) / L\n",
    "    u = torch.normal(0, 1, size=(L, B, N))\n",
    "\n",
    "\n",
    "    # device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    device = torch.device('cuda:0') if cuda else torch.device('cpu')\n",
    "    A = A.to(device)\n",
    "    u = u.to(device)\n",
    "    return A, u\n",
    "\n",
    "def test_stability():\n",
    "    print(\"Testing Stability\\n====================\")\n",
    "    L = 256\n",
    "    N = L // 2\n",
    "    B = 100\n",
    "    A, u = generate_data(L, N, B)\n",
    "\n",
    "    x = unroll(A, u)\n",
    "    x1 = parallel_unroll_recursive(A, u)\n",
    "    x2 = parallel_unroll_recursive_br(A, u)\n",
    "    x3 = parallel_unroll_iterative(A, u)\n",
    "    print(\"norm error\", torch.norm(x-x1))\n",
    "    print(\"norm error\", torch.norm(x-x2))\n",
    "    print(\"norm error\", torch.norm(x-x3))\n",
    "    # print(x-x1)\n",
    "    # print(x-x2)\n",
    "    # print(x-x3)\n",
    "    print(\"max error\", torch.max(torch.abs(x-x1)))\n",
    "    print(\"max error\", torch.max(torch.abs(x-x2)))\n",
    "    print(\"max error\", torch.max(torch.abs(x-x3)))\n",
    "\n",
    "    A = A.repeat((L, 1, 1))\n",
    "    x = variable_unroll_sequential(A, u)\n",
    "    x_ = variable_unroll(A, u)\n",
    "    # x_ = variable_unroll_matrix_sequential(A, u)\n",
    "    x_ = variable_unroll_matrix(A, u)\n",
    "    print(x-x_)\n",
    "    abserr = torch.abs(x-x_)\n",
    "    relerr = abserr/(torch.abs(x)+1e-8)\n",
    "    print(\"norm abs error\", torch.norm(abserr))\n",
    "    print(\"max abs error\", torch.max(abserr))\n",
    "    print(\"norm rel error\", torch.norm(relerr))\n",
    "    print(\"max rel error\", torch.max(relerr))\n",
    "\n",
    "def test_toeplitz():\n",
    "    from model.toeplitz import construct_toeplitz\n",
    "    def summarize(name, x, x_, showdiff=False):\n",
    "        print(name, \"stats\")\n",
    "        if showdiff:\n",
    "            print(x-x_)\n",
    "        abserr = torch.abs(x-x_)\n",
    "        relerr = abserr/(torch.abs(x)+1e-8)\n",
    "        print(\"  norm abs error\", torch.norm(abserr))\n",
    "        print(\"  max abs error\", torch.max(abserr))\n",
    "        print(\"  norm rel error\", torch.norm(relerr))\n",
    "        print(\"  max rel error\", torch.max(relerr))\n",
    "\n",
    "    print(\"Testing Toeplitz\\n====================\")\n",
    "    L = 512\n",
    "    N = L // 2\n",
    "    B = 100\n",
    "    A, u = generate_data(L, N, B)\n",
    "\n",
    "    A = A[..., 0]\n",
    "    A = construct_toeplitz(A)\n",
    "\n",
    "    # print(\"SHAPES\", A.shape, u.shape)\n",
    "\n",
    "    # Static A\n",
    "    x = unroll(A, u)\n",
    "    x_ = variable_unroll(A, u, variable=False)\n",
    "    summarize(\"nonvariable matrix original\", x, x_, showdiff=False)\n",
    "    x_ = variable_unroll_matrix(A, u, variable=False)\n",
    "    summarize(\"nonvariable matrix general\", x, x_, showdiff=False)\n",
    "    x_ = variable_unroll_toeplitz(A[..., 0], u, variable=False)\n",
    "    summarize(\"nonvariable toeplitz\", x, x_, showdiff=False)\n",
    "\n",
    "    # Sequential\n",
    "    A = A.repeat((L, 1, 1))\n",
    "    for _ in range(1):\n",
    "        x_ = variable_unroll_sequential(A, u)\n",
    "        summarize(\"variable unroll sequential\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_matrix_sequential(A, u)\n",
    "        summarize(\"variable matrix sequential\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_toeplitz_sequential(A[..., 0], u, pad=True)\n",
    "        summarize(\"variable toeplitz sequential\", x, x_, showdiff=False)\n",
    "\n",
    "    # Parallel\n",
    "    for _ in range(1):\n",
    "        x_ = variable_unroll(A, u)\n",
    "        summarize(\"variable matrix original\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_matrix(A, u)\n",
    "        summarize(\"variable matrix general\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_toeplitz(A[..., 0], u, pad=True, recurse_limit=8)\n",
    "        summarize(\"variable toeplitz\", x, x_, showdiff=False)\n",
    "\n",
    "def test_speed(variable=False, it=1):\n",
    "    print(\"Testing Speed\\n====================\")\n",
    "    N = 256\n",
    "    L = 1024\n",
    "    B = 100\n",
    "    A, u = generate_data(L, N, B)\n",
    "    As = A.repeat((L, 1, 1))\n",
    "\n",
    "    u.requires_grad=True\n",
    "    As.requires_grad=True\n",
    "    for _ in range(it):\n",
    "        x = unroll(A, u)\n",
    "        x = torch.sum(x)\n",
    "        x.backward()\n",
    "\n",
    "        x = parallel_unroll_recursive(A, u)\n",
    "        x = torch.sum(x)\n",
    "        x.backward()\n",
    "\n",
    "        # parallel_unroll_recursive_br(A, u)\n",
    "        # parallel_unroll_iterative(A, u)\n",
    "\n",
    "    for _ in range(it):\n",
    "        if variable:\n",
    "            x = variable_unroll_sequential(As, u, variable=True, recurse_limit=16)\n",
    "            x = torch.sum(x)\n",
    "            x.backward()\n",
    "            x = variable_unroll(As, u, variable=True, recurse_limit=16)\n",
    "            x = torch.sum(x)\n",
    "            x.backward()\n",
    "        else:\n",
    "            variable_unroll_sequential(A, u, variable=False, recurse_limit=16)\n",
    "            variable_unroll(A, u, variable=False, recurse_limit=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LegT(nn.Module):\n",
    "    def __init__(self, N, dt=1.0, discretization='bilinear'):\n",
    "        \"\"\"\n",
    "        N: the order of the HiPPO projection\n",
    "        dt: discretization step size - should be roughly inverse to the length of the sequence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        #A, B = transition('lmu', N)\n",
    "        A, B = make_HiPPO(N=self.N, v='v', measure='legt', lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1)\n",
    "        C = np.ones((1, N))\n",
    "        D = np.zeros((1,))\n",
    "        # dt, discretization options\n",
    "        A, B, _, _, _ = signal.cont2discrete((A, B, C, D), dt=dt, method=discretization)\n",
    "\n",
    "        B = B.squeeze(-1)\n",
    "\n",
    "        self.register_buffer('A', torch.Tensor(A)) # (N, N)\n",
    "        self.register_buffer('B', torch.Tensor(B)) # (N,)\n",
    "\n",
    "        # vals = np.linspace(0.0, 1.0, 1./dt)\n",
    "        vals = np.arange(0.0, 1.0, dt)\n",
    "        self.eval_matrix = torch.Tensor(ss.eval_legendre(np.arange(N)[:, None], 1 - 2 * vals).T)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = inputs * self.B # (length, ..., N)\n",
    "\n",
    "        c = torch.zeros(u.shape[1:])\n",
    "        cs = []\n",
    "        for f in inputs:\n",
    "            c = F.linear(c, self.A) + self.B * f\n",
    "            cs.append(c)\n",
    "        return torch.stack(cs, dim=0)\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        return (self.eval_matrix @ c.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LegS(nn.Module):\n",
    "    \"\"\" Vanilla HiPPO-LegS model (scale invariant instead of time invariant) \"\"\"\n",
    "    def __init__(self, N, max_length=1024, measure='legs', discretization='bilinear'):\n",
    "        \"\"\"\n",
    "        max_length: maximum sequence length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        A, B = make_HiPPO(N=self.N, v='v', measure=measure, lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1)\n",
    "        #A, B = transition(measure, N)\n",
    "        B = B.squeeze(-1)\n",
    "        A_stacked = np.empty((max_length, N, N), dtype=A.dtype)\n",
    "        B_stacked = np.empty((max_length, N), dtype=B.dtype)\n",
    "        for t in range(1, max_length + 1):\n",
    "            At = A / t\n",
    "            Bt = B / t\n",
    "            if discretization == 'forward':\n",
    "                A_stacked[t - 1] = np.eye(N) + At\n",
    "                B_stacked[t - 1] = Bt\n",
    "            elif discretization == 'backward':\n",
    "                # print(f\"BEFORE - Stacked A matrix:\\n{A_stacked[t - 1]}\")\n",
    "                # print(np.eye(N) - At)\n",
    "                A_stacked[t - 1] = la.solve_triangular(np.eye(N) - At, np.eye(N), lower=True)\n",
    "                print(f\"AFTER - Stacked A matrix:\\n{A_stacked[t - 1]}\")\n",
    "                B_stacked[t - 1] = la.solve_triangular(np.eye(N) - At, Bt, lower=True)\n",
    "            elif discretization == 'bilinear':\n",
    "                # print(f\"1st Term:\\n{np.eye(N) - At / 2}\")\n",
    "                # print(f\"1st Term Singular value\\n{np.linalg.svd(np.eye(N) - At / 2)[1]}\")\n",
    "                # print(f\"2nd Term:\\n{np.eye(N) + At / 2}\")\n",
    "                # print(f\"2nd Term Singular value\\n{np.linalg.svd(np.eye(N) + At / 2)[1]}\")\n",
    "                A_stacked[t - 1] = np.linalg.lstsq(np.eye(N) - At / 2,  np.eye(N) + At / 2, rcond=None)[0] # TODO: Referencing this: https://stackoverflow.com/questions/64527098/numpy-linalg-linalgerror-singular-matrix-error-when-trying-to-solve \n",
    "                #A_stacked[t - 1] = la.solve_triangular(np.eye(N) - At / 2, np.eye(N) + At / 2, lower=True)\n",
    "                # print(f\"AFTER - Stacked A matrix:\\n{A_stacked[t - 1]}\")\n",
    "                B_stacked[t - 1] = np.linalg.lstsq(np.eye(N) - At / 2,  Bt, rcond=None)[0]\n",
    "                #B_stacked[t - 1] = la.solve_triangular(np.eye(N) - At / 2, Bt, lower=True)\n",
    "            else: # ZOH\n",
    "                A_stacked[t - 1] = la.expm(A * (math.log(t + 1) - math.log(t)))\n",
    "                B_stacked[t - 1] = la.solve_triangular(A, A_stacked[t - 1] @ B - B, lower=True)\n",
    "        self.A_stacked = torch.Tensor(A_stacked.copy()) # (max_length, N, N)\n",
    "        self.B_stacked = torch.Tensor(B_stacked.copy()) # (max_length, N)\n",
    "        # print(f\"A_stacked:\\n{self.A_stacked.shape}\")\n",
    "        # print(f\"B_stacked:\\n{self.B_stacked.shape}\")\n",
    "\n",
    "        vals = np.linspace(0.0, 1.0, max_length)\n",
    "        self.eval_matrix = torch.from_numpy(np.asarray(((B[:, None] * ss.eval_legendre(np.arange(N)[:, None], 2 * vals - 1)).T)))\n",
    "        print(f\"eval_matrix:\\n{self.eval_matrix}\")\n",
    "        print(f\"eval_matrix:\\n{self.eval_matrix.shape}\")\n",
    "\n",
    "    def forward(self, inputs, fast=False):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "        result = None\n",
    "        \n",
    "        L = inputs.shape[0]\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = torch.transpose(inputs, 0, -2)\n",
    "        u = u * self.B_stacked[:L]\n",
    "        u = torch.transpose(u, 0, -2) # (length, ..., N)\n",
    "\n",
    "        if fast:\n",
    "            result = variable_unroll_matrix(self.A_stacked[:L], u)\n",
    "            \n",
    "        else:\n",
    "            result = variable_unroll_matrix_sequential(self.A_stacked[:L], u)\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        a = self.eval_matrix @ c.unsqueeze(-1)\n",
    "        return a.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO(jnn.Module):\n",
    "    '''\n",
    "    class that constructs HiPPO model using the defined measure. \n",
    "    \n",
    "    Args:\n",
    "        N (int): order of the HiPPO projection, aka the number of coefficients to describe the matrix\n",
    "        max_length (int): maximum sequence length to be input\n",
    "        measure (str): the measure used to define which way to instantiate the HiPPO matrix\n",
    "        step (float): step size used for descretization\n",
    "        GBT_alpha (float): represents which descretization transformation to use based off the alpha value\n",
    "        seq_L (int): length of the sequence to be used for training\n",
    "    '''\n",
    "    N: int \n",
    "    max_length: int \n",
    "    measure: str \n",
    "    step: float \n",
    "    GBT_alpha: float \n",
    "    seq_L: int \n",
    "    \n",
    "    def setup(self):\n",
    "        is_initialized = self.has_variable('A', 'B', 'C', 'D', 'eval_matrix')\n",
    "        \n",
    "        A, B = make_HiPPO(N=self.N, v='v', measure=\"legs\", lambda_n=1, fourier_type=\"FRU\", alpha=0, beta=1)\n",
    "        print(f\"A:\\n{A}\")\n",
    "        print(f\"B:\\n{B}\")\n",
    "        self.A = self.param(\"A\", A, (self.N, self.N))\n",
    "        self.B = self.param(\"B\", B, (self.N, 1))\n",
    "        self.C = self.param(\"C\", lecun_normal(), (1, self.N))\n",
    "        self.D = self.param(\"D\", jnn.initializers.ones, (1,))\n",
    "        \n",
    "        # self.A = A\n",
    "        # self.B = B.squeeze(-1)\n",
    "        # self.C = jnp.ones((1, self.N))\n",
    "        # self.D = jnp.zeros((1,))\n",
    "        \n",
    "        if self.measure == \"legt\":\n",
    "            L = self.seq_L\n",
    "            vals = jnp.arange(0.0, 1.0, L)\n",
    "            self.eval_matrix = jnp.ndarray(ss.eval_legendre(jnp.arange(self.N)[:, None], 1 - 2 * vals).T)\n",
    "            \n",
    "        elif self.measure == \"legs\":\n",
    "            L = self.max_length\n",
    "            vals = jnp.linspace(0.0, 1.0, L)\n",
    "            self.eval_matrix = jnp.ndarray((B[:, None] * ss.eval_legendre(jnp.arange(self.N)[:, None], 2 * vals - 1)).T)\n",
    "        \n",
    "    def __call__(self, u, kernel=True):\n",
    "        if not kernel:\n",
    "            Ab, Bb, Cb, Db = self.collect_SSM_vars(self.A, self.B, self.C, u, alpha=self.GBT_alpha)\n",
    "            c_k = self.scan_SSM(Ab, Bb, Cb, Db, u[:, jnp.newaxis], jnp.zeros((self.N,)))[1]\n",
    "        else:\n",
    "            Ab, Bb, Cb, Db = self.discretize(self.A, self.B, self.C, self.D, step=self.step, alpha=self.GBT_alpha)\n",
    "            c_k = self.causal_convolution(u, self.K_conv(Ab, Bb, Cb, Db, L=self.max_length))\n",
    "            \n",
    "        return c_k\n",
    "    \n",
    "    def reconstruct(self, c):\n",
    "        '''\n",
    "        Uses coeffecients to reconstruct the signal\n",
    "        \n",
    "        Args: \n",
    "            c (jnp.ndarray): coefficients of the HiPPO projection\n",
    "            \n",
    "        Returns:\n",
    "            reconstructed signal\n",
    "        '''\n",
    "        a = self.eval_matrix @ jnp.expand_dims(c, -1)\n",
    "        return a.squeeze(-1)\n",
    "    \n",
    "    def discretize(self, A, B, C, D, step, alpha=0.5):\n",
    "        '''\n",
    "        function used for descretizing the HiPPO matrix\n",
    "        \n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            step (float): step size used for discretization\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "                - forward Euler corresponds to α = 0,\n",
    "                - backward Euler corresponds to α = 1,\n",
    "                - bilinear corresponds to α = 0.5,\n",
    "                - Zero-order Hold corresponds to α > 1\n",
    "        '''\n",
    "        I = jnp.eye(A.shape[0])\n",
    "        GBT = jnp.linalg.inv(I - ((step * alpha) * A))\n",
    "        GBT_A = GBT @ (I + ((step * (1-alpha)) * A))\n",
    "        GBT_B = (step * GBT) @ B\n",
    "        \n",
    "        if alpha > 1: # Zero-order Hold\n",
    "            GBT_A = jax.scipy.linalg.expm(step * A)\n",
    "            GBT_B = (jnp.linalg.inv(A) @ (jax.scipy.linalg.expm(step * A) - I)) @ B \n",
    "        \n",
    "        return GBT_A, GBT_B, C, D\n",
    "    \n",
    "    def collect_SSM_vars(self, A, B, C, D, u, alpha=0.5):\n",
    "        '''\n",
    "        turns the continous HiPPO matrix components into a discrete ones\n",
    "        \n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            u (jnp.ndarray): input signal\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "            \n",
    "        Returns:\n",
    "            Ab (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Bb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Cb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Db (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            \n",
    "            What is her name again\n",
    "            Mari\n",
    "        '''\n",
    "        L = u.shape[0]\n",
    "        assert L == self.seq_L\n",
    "        N = A.shape[0]\n",
    "        Ab, Bb, Cb, Db = self.discretize(A, B, C, D, step=1.0 / L, alpha=alpha)\n",
    "    \n",
    "        return Ab, Bb, Cb, Db\n",
    "    \n",
    "    def scan_SSM(self, Ab, Bb, Cb, Db, u, x0):\n",
    "        '''\n",
    "        This is for returning the discretized hidden state often needed for an RNN. \n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            u (jnp.ndarray): the input sequence\n",
    "            x0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        '''\n",
    "        def step(x_k_1, u_k):\n",
    "            '''\n",
    "            Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "            Args:\n",
    "                x_k_1: previous hidden state\n",
    "                u_k: output from function f at, descritized, time step, k.\n",
    "            \n",
    "            Returns: \n",
    "                x_k: current hidden state\n",
    "                y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "            '''\n",
    "            x_k = (Ab @ x_k_1) + (Bb @ u_k)\n",
    "            y_k = (Cb @ x_k) + (Db @ u_k)\n",
    "            return x_k, y_k\n",
    "\n",
    "        return jax.lax.scan(step, x0, u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # N = 256\n",
    "    # L = 128\n",
    "    \n",
    "    N = 16\n",
    "    L = 8\n",
    "    \n",
    "    x = torch.randn(L, 1)\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    loss = nn.MSELoss()\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegT model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print('\\nTesting HiPPO LegT model')\n",
    "    hippo_legt = HiPPO_LegT(N, dt=1./L)\n",
    "    \n",
    "    y = hippo_legt(x)\n",
    "    \n",
    "    print(f\"h-y shape for LegT:\\n{y.shape}\")\n",
    "    z = hippo_legt.reconstruct(y)\n",
    "    print(f\"h-z shape for LegT:\\n{z.shape}\")\n",
    "\n",
    "    # mse = torch.mean((z[-1,0,:L].flip(-1) - x.squeeze(-1))**2)\n",
    "    # mse = torch.mean((z[-1,0,:L] - x.s}\")\n",
    "    mse = loss(z[-1,0,:L], x.squeeze(-1))\n",
    "    # mse = torch.mean((z[-1,0,:L] - x.squeeze(-1))**2)\n",
    "    print(f\"h-MSE shape:\\n{mse}\")\n",
    "    print(f\"end of test for HiPPO LegT model\")\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegS model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print('\\nTesting HiPPO LegS model')\n",
    "    # print(y.shape)\n",
    "    hippo_legs = HiPPO_LegS(N, max_length=L) #The Gu's\n",
    "    \n",
    "    y = hippo_legs(x)\n",
    "    \n",
    "    print(f\"h-y shape for LegS:\\n{y.shape}\")\n",
    "    \n",
    "    z = hippo_legs(x, fast=True)\n",
    "    \n",
    "    print(f\"h-reconstruction shape for LegS:\\n{hippo_legs.reconstruct(z).shape}\")\n",
    "    \n",
    "    # print(y-z)\n",
    "    print(f\"end of test for HiPPO LegT model\")\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test Generic HiPPO model --------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print('\\nTesting BRYANS HiPPO LegS model')\n",
    "    hippo_LegS = HiPPO(N=N,\n",
    "                       max_length=L, \n",
    "                       measure='legs', \n",
    "                       step=1.0/L, \n",
    "                       GBT_alpha=0.5, \n",
    "                       seq_L=L) # Bryan's\n",
    "    \n",
    "    y_legs = hippo_LegS(x)\n",
    "    \n",
    "    print(f\"U-y shape for LegS:\\n{y_legs.shape}\")\n",
    "    \n",
    "    z_legs = hippo_LegS(x, fast=True)\n",
    "    \n",
    "    print(f\"U-reconstruction shape for LegS:\\n{hippo_LegS.reconstruct(z_legs).shape}\")\n",
    "    print(f\"end of test for HiPPO LegT model\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing HiPPO LegT model\n",
      "h-y shape for LegT:\n",
      "torch.Size([8, 1, 16])\n",
      "h-z shape for LegT:\n",
      "torch.Size([8, 1, 8])\n",
      "h-MSE shape:\n",
      "inf\n",
      "end of test for HiPPO LegT model\n",
      "\n",
      "Testing HiPPO LegS model\n",
      "eval_matrix:\n",
      "tensor([[ 1.0000, -1.7321,  2.2361, -2.6458,  3.0000, -3.3166,  3.6056, -3.8730,\n",
      "          4.1231, -4.3589,  4.5826, -4.7958,  5.0000, -5.1962,  5.3852, -5.5678],\n",
      "        [ 1.0000, -1.2372,  0.5932,  0.4242, -1.1983,  1.2778, -0.6189, -0.3970,\n",
      "          1.1854, -1.2936,  0.6599,  0.3521, -1.1626,  1.3075, -0.7039, -0.3025],\n",
      "        [ 1.0000, -0.7423, -0.5020,  1.1802, -0.4985, -0.7584,  1.1471, -0.2216,\n",
      "         -0.9582,  1.0417,  0.0666, -1.0990,  0.8746,  0.3501, -1.1745,  0.6561],\n",
      "        [ 1.0000, -0.2474, -1.0496,  0.5477,  0.9009, -0.8053, -0.6730,  0.9979,\n",
      "          0.3888, -1.1092, -0.0724,  1.1301, -0.2502, -1.0588,  0.5525,  0.9011],\n",
      "        [ 1.0000,  0.2474, -1.0496, -0.5477,  0.9009,  0.8053, -0.6730, -0.9979,\n",
      "          0.3888,  1.1092, -0.0724, -1.1301, -0.2502,  1.0588,  0.5525, -0.9011],\n",
      "        [ 1.0000,  0.7423, -0.5020, -1.1802, -0.4985,  0.7584,  1.1471,  0.2216,\n",
      "         -0.9582, -1.0417,  0.0666,  1.0990,  0.8746, -0.3501, -1.1745, -0.6561],\n",
      "        [ 1.0000,  1.2372,  0.5932, -0.4242, -1.1983, -1.2778, -0.6189,  0.3970,\n",
      "          1.1854,  1.2936,  0.6599, -0.3521, -1.1626, -1.3075, -0.7039,  0.3025],\n",
      "        [ 1.0000,  1.7321,  2.2361,  2.6458,  3.0000,  3.3166,  3.6056,  3.8730,\n",
      "          4.1231,  4.3589,  4.5826,  4.7958,  5.0000,  5.1962,  5.3852,  5.5678]])\n",
      "eval_matrix:\n",
      "torch.Size([8, 16])\n",
      "h-y shape for LegS:\n",
      "torch.Size([8, 1, 16])\n",
      "h-reconstruction shape for LegS:\n",
      "torch.Size([8, 1, 8])\n",
      "end of test for HiPPO LegT model\n",
      "\n",
      "Testing BRYANS HiPPO LegS model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "\"HiPPO\" object has no attribute \"A\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/beegass/Documents/Coding/S4/HiPPO.ipynb Cell 36'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000034?line=0'>1</a>\u001b[0m test()\n",
      "\u001b[1;32m/Users/beegass/Documents/Coding/S4/HiPPO.ipynb Cell 35'\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000033?line=52'>53</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTesting BRYANS HiPPO LegS model\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000033?line=53'>54</a>\u001b[0m hippo_LegS \u001b[39m=\u001b[39m HiPPO(N\u001b[39m=\u001b[39mN,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000033?line=54'>55</a>\u001b[0m                    max_length\u001b[39m=\u001b[39mL, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000033?line=55'>56</a>\u001b[0m                    measure\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlegs\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000033?line=56'>57</a>\u001b[0m                    step\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39mL, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000033?line=57'>58</a>\u001b[0m                    GBT_alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000033?line=58'>59</a>\u001b[0m                    seq_L\u001b[39m=\u001b[39mL) \u001b[39m# Bryan's\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000033?line=60'>61</a>\u001b[0m y_legs \u001b[39m=\u001b[39m hippo_LegS(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000033?line=62'>63</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mU-y shape for LegS:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00my_legs\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000033?line=64'>65</a>\u001b[0m z_legs \u001b[39m=\u001b[39m hippo_LegS(x, fast\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.virtualenvs/jax-pytorch-s4/lib/python3.10/site-packages/flax/linen/transforms.py:1246\u001b[0m, in \u001b[0;36mnamed_call.<locals>.wrapped_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(prewrapped_fn)\n\u001b[1;32m   1243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1244\u001b[0m   \u001b[39mif\u001b[39;00m ((\u001b[39mnot\u001b[39;00m force \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m linen_module\u001b[39m.\u001b[39m_use_named_call)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m       \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39min_setup):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1246\u001b[0m     \u001b[39mreturn\u001b[39;00m prewrapped_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1247\u001b[0m   fn_name \u001b[39m=\u001b[39m class_fn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m   1248\u001b[0m   method_suffix \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mfn_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m fn_name \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__call__\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/jax-pytorch-s4/lib/python3.10/site-packages/flax/linen/module.py:352\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    351\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 352\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    353\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/jax-pytorch-s4/lib/python3.10/site-packages/flax/linen/module.py:651\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m _context\u001b[39m.\u001b[39mmodule_stack\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m)\n\u001b[1;32m    650\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 651\u001b[0m   y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    652\u001b[0m   \u001b[39mif\u001b[39;00m _context\u001b[39m.\u001b[39mcapture_stack:\n\u001b[1;32m    653\u001b[0m     filter_fn \u001b[39m=\u001b[39m _context\u001b[39m.\u001b[39mcapture_stack[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32m/Users/beegass/Documents/Coding/S4/HiPPO.ipynb Cell 34'\u001b[0m in \u001b[0;36mHiPPO.__call__\u001b[0;34m(self, u, kernel)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000032?line=48'>49</a>\u001b[0m     c_k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_SSM(Ab, Bb, Cb, Db, u[:, jnp\u001b[39m.\u001b[39mnewaxis], jnp\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN,)))[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000032?line=49'>50</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000032?line=50'>51</a>\u001b[0m     Ab, Bb, Cb, Db \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscretize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mA, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mB, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD, step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep, alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGBT_alpha)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000032?line=51'>52</a>\u001b[0m     c_k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcausal_convolution(u, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mK_conv(Ab, Bb, Cb, Db, L\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/S4/HiPPO.ipynb#ch0000032?line=53'>54</a>\u001b[0m \u001b[39mreturn\u001b[39;00m c_k\n",
      "File \u001b[0;32m~/.virtualenvs/jax-pytorch-s4/lib/python3.10/site-packages/flax/linen/module.py:717\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    715\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[name]\n\u001b[1;32m    716\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    718\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: \"HiPPO\" object has no attribute \"A\""
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aa_1, Bb_1, C_1 = a_func(N, max_length=1024, measure='legs', discretization='bilinear')\n",
    "# Aa_2, Bb_2, C_2 = another_func(N, max_length=1024, measure='legs', discretization='bilinear')\n",
    "# print(f\"A-1:\\n{Aa_1}\")\n",
    "# print(f\"B-1:\\n{Bb_1}\")\n",
    "# print(f\"C-1:\\n{C_1}\")\n",
    "# print(f\"A-2:\\n{Aa_2}\")\n",
    "# print(f\"B-2:\\n{Bb_2}\")\n",
    "# print(f\"C-2:\\n{C_2}\")\n",
    "# print(f\"A Comparison:\\n \", jnp.allclose(Aa_1, Aa_2))\n",
    "# print(f\"B Comparison:\\n \", jnp.allclose(Bb_1, Bb_2))\n",
    "# print(f\"C Comparison:\\n \", jnp.allclose(C_1, C_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jax-pytorch-s4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef2b914a37cedc76fc9872a46094ffe13a6a8170158ba89febe80f19b5718ff4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
