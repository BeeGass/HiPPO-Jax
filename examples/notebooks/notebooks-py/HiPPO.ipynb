{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiPPO Matrices\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GpuDevice(id=0, process_index=0)]\n",
      "The Device: gpu\n"
     ]
    }
   ],
   "source": [
    "## import packages\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import linen as jnn\n",
    "\n",
    "from jax.nn.initializers import lecun_normal, uniform\n",
    "from jax.numpy.linalg import eig, inv, matrix_power\n",
    "from jax.scipy.signal import convolve\n",
    "\n",
    "# import modules \n",
    "from src.models.hippo.gu_transition import GuTransMatrix\n",
    "\n",
    "import requests\n",
    "\n",
    "from scipy import linalg as la\n",
    "from scipy import signal\n",
    "from scipy import special as ss\n",
    "\n",
    "import math\n",
    "\n",
    "print(jax.devices())\n",
    "print(f\"The Device: {jax.lib.xla_bridge.get_backend().platform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS enabled: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(f\"MPS enabled: {torch.backends.mps.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 5\n",
    "rng, key2, key3, key4, key5 = jax.random.split(key, num=num_copies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_SSM(rng, N):\n",
    "    a_r, b_r, c_r = jax.random.split(rng, 3)\n",
    "    A = jax.random.uniform(a_r, (N, N))\n",
    "    B = jax.random.uniform(b_r, (N, 1))\n",
    "    C = jax.random.uniform(c_r, (1, N))\n",
    "\n",
    "    return A, B, C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate The HiPPO Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransMatrix:\n",
    "    def __init__(\n",
    "        self, N, measure=\"legs\", lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiates the HiPPO matrix of a given order using a particular measure.\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            v (str): choose between this repo's implementation or hazy research's implementation.\n",
    "            measure (str):\n",
    "                choose between\n",
    "                    - HiPPO w/ Translated Legendre (LegT) - legt\n",
    "                    - HiPPO w/ Translated Laguerre (LagT) - lagt\n",
    "                    - HiPPO w/ Scaled Legendre (LegS) - legs\n",
    "                    - HiPPO w/ Fourier basis - fourier\n",
    "                        - FRU: Fourier Recurrent Unit\n",
    "                        - FouT: Translated Fourier\n",
    "            lambda_n (int): The amount of tilt applied to the HiPPO-LegS basis, determines between LegS and LMU.\n",
    "            fourier_type (str): chooses between the following:\n",
    "                - FRU: Fourier Recurrent Unit - fru\n",
    "                - FouT: Translated Fourier - fout\n",
    "                - FourD: Fourier Decay - fourd\n",
    "            alpha (float): The order of the Laguerre basis.\n",
    "            beta (float): The scale of the Laguerre basis.\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The HiPPO matrix multiplied by -1.\n",
    "            B (jnp.ndarray): The other corresponding state space matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        A = None\n",
    "        B = None\n",
    "        if measure == \"legt\":\n",
    "            A, B = self.build_LegT(N=N, lambda_n=lambda_n)\n",
    "\n",
    "        elif measure == \"lagt\":\n",
    "            A, B = self.build_LagT(alpha=alpha, beta=beta, N=N)\n",
    "\n",
    "        elif measure == \"legs\":\n",
    "            A, B = self.build_LegS(N=N)\n",
    "\n",
    "        elif measure == \"fourier\":\n",
    "            A, B = self.build_Fourier(N=N, fourier_type=fourier_type)\n",
    "\n",
    "        elif measure == \"random\":\n",
    "            A = jnp.random.randn(N, N) / N\n",
    "            B = jnp.random.randn(N, 1)\n",
    "\n",
    "        elif measure == \"diagonal\":\n",
    "            A = -jnp.diag(jnp.exp(jnp.random.randn(N)))\n",
    "            B = jnp.random.randn(N, 1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid HiPPO type\")\n",
    "\n",
    "        self.A_matrix = A.copy()\n",
    "        self.B_matrix = B.copy()\n",
    "\n",
    "    # Translated Legendre (LegT) - vectorized\n",
    "    @staticmethod\n",
    "    def build_LegT(N, lambda_n=1):\n",
    "        \"\"\"\n",
    "        The, vectorized implementation of the, measure derived from the translated Legendre basis.\n",
    "\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            legt_type (str): Choice between the two different tilts of basis.\n",
    "                - legt: translated Legendre - 'legt'\n",
    "                - lmu: Legendre Memory Unit - 'lmu'\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        q = jnp.arange(N, dtype=jnp.float32)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "        case = jnp.power(-1.0, (n - k))\n",
    "        A = None\n",
    "        B = None\n",
    "\n",
    "        if lambda_n == 1:\n",
    "            A_base = -jnp.sqrt(2 * n + 1) * jnp.sqrt(2 * k + 1)\n",
    "            pre_D = jnp.sqrt(jnp.diag(2 * q + 1))\n",
    "            B = D = jnp.diag(pre_D)[:, None]\n",
    "            A = jnp.where(\n",
    "                k <= n, A_base, A_base * case\n",
    "            )  # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "        elif lambda_n == 2:  # (jnp.sqrt(2*n+1) * jnp.power(-1, n)):\n",
    "            A_base = -(2 * n + 1)\n",
    "            B = jnp.diag((2 * q + 1) * jnp.power(-1, n))[:, None]\n",
    "            A = jnp.where(\n",
    "                k <= n, A_base * case, A_base\n",
    "            )  # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "        return A, B\n",
    "\n",
    "    # Translated Laguerre (LagT) - non-vectorized\n",
    "    @staticmethod\n",
    "    def build_LagT(alpha, beta, N):\n",
    "        \"\"\"\n",
    "        The, vectorized implementation of the, measure derived from the translated Laguerre basis.\n",
    "\n",
    "        Args:\n",
    "            alpha (float): The order of the Laguerre basis.\n",
    "            beta (float): The scale of the Laguerre basis.\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        L = jnp.exp(\n",
    "            0.5\n",
    "            * (ss.gammaln(jnp.arange(N) + alpha + 1) - ss.gammaln(jnp.arange(N) + 1))\n",
    "        )\n",
    "        inv_L = 1.0 / L[:, None]\n",
    "        pre_A = (jnp.eye(N) * ((1 + beta) / 2)) + jnp.tril(jnp.ones((N, N)), -1)\n",
    "        pre_B = ss.binom(alpha + jnp.arange(N), jnp.arange(N))[:, None]\n",
    "\n",
    "        A = -inv_L * pre_A * L[None, :]\n",
    "        B = (\n",
    "            jnp.exp(-0.5 * ss.gammaln(1 - alpha))\n",
    "            * jnp.power(beta, (1 - alpha) / 2)\n",
    "            * inv_L\n",
    "            * pre_B\n",
    "        )\n",
    "\n",
    "        return A, B\n",
    "\n",
    "    # Scaled Legendre (LegS) vectorized\n",
    "    @staticmethod\n",
    "    def build_LegS(N):\n",
    "        \"\"\"\n",
    "        The, vectorized implementation of the, measure derived from the Scaled Legendre basis.\n",
    "\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        q = jnp.arange(N, dtype=jnp.float32)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "        pre_D = jnp.sqrt(jnp.diag(2 * q + 1))\n",
    "        B = D = jnp.diag(pre_D)[:, None]\n",
    "\n",
    "        A_base = (-jnp.sqrt(2 * n + 1)) * jnp.sqrt(2 * k + 1)\n",
    "        case_2 = (n + 1) / (2 * n + 1)\n",
    "\n",
    "        A = jnp.where(n > k, A_base, 0.0)  # if n > k, then A_base is used, otherwise 0\n",
    "        A = jnp.where(\n",
    "            n == k, (A_base * case_2), A\n",
    "        )  # if n == k, then A_base is used, otherwise A\n",
    "\n",
    "        return A, B\n",
    "\n",
    "    # Fourier Basis OPs and functions - vectorized\n",
    "    @staticmethod\n",
    "    def build_Fourier(N, fourier_type=\"fru\"):\n",
    "        \"\"\"\n",
    "        Vectorized measure implementations derived from fourier basis.\n",
    "\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            fourier_type (str): The type of Fourier measure.\n",
    "                - FRU: Fourier Recurrent Unit - fru\n",
    "                - FouT: truncated Fourier - fout\n",
    "                - fouD: decayed Fourier - foud\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        A = jnp.diag(\n",
    "            jnp.stack([jnp.zeros(N // 2), jnp.zeros(N // 2)], axis=-1).reshape(-1)[1:],\n",
    "            1,\n",
    "        )\n",
    "        B = jnp.zeros(A.shape[1], dtype=jnp.float32)\n",
    "\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        q = jnp.arange(A.shape[1], dtype=jnp.float32)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "\n",
    "        n_odd = n % 2 == 0\n",
    "        k_odd = k % 2 == 0\n",
    "\n",
    "        case_1 = (n == k) & (n == 0)\n",
    "        case_2_3 = ((k == 0) & (n_odd)) | ((n == 0) & (k_odd))\n",
    "        case_4 = (n_odd) & (k_odd)\n",
    "        case_5 = (n - k == 1) & (k_odd)\n",
    "        case_6 = (k - n == 1) & (n_odd)\n",
    "\n",
    "        if fourier_type == \"fru\":  # Fourier Recurrent Unit (FRU) - vectorized\n",
    "            A = jnp.where(\n",
    "                case_1,\n",
    "                -1.0,\n",
    "                jnp.where(\n",
    "                    case_2_3,\n",
    "                    -jnp.sqrt(2),\n",
    "                    jnp.where(\n",
    "                        case_4,\n",
    "                        -2,\n",
    "                        jnp.where(\n",
    "                            case_5,\n",
    "                            jnp.pi * (n // 2),\n",
    "                            jnp.where(case_6, -jnp.pi * (k // 2), 0.0),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        elif fourier_type == \"fout\":  # truncated Fourier (FouT) - vectorized\n",
    "            A = jnp.where(\n",
    "                case_1,\n",
    "                -1.0,\n",
    "                jnp.where(\n",
    "                    case_2_3,\n",
    "                    -jnp.sqrt(2),\n",
    "                    jnp.where(\n",
    "                        case_4,\n",
    "                        -2,\n",
    "                        jnp.where(\n",
    "                            case_5,\n",
    "                            jnp.pi * (n // 2),\n",
    "                            jnp.where(case_6, -jnp.pi * (k // 2), 0.0),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            A = 2 * A\n",
    "            B = 2 * B\n",
    "\n",
    "        elif fourier_type == \"fourd\":\n",
    "            A = jnp.where(\n",
    "                case_1,\n",
    "                -1.0,\n",
    "                jnp.where(\n",
    "                    case_2_3,\n",
    "                    -jnp.sqrt(2),\n",
    "                    jnp.where(\n",
    "                        case_4,\n",
    "                        -2,\n",
    "                        jnp.where(\n",
    "                            case_5,\n",
    "                            2 * jnp.pi * (n // 2),\n",
    "                            jnp.where(case_6, 2 * -jnp.pi * (k // 2), 0.0),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            A = 0.5 * A\n",
    "            B = 0.5 * B\n",
    "\n",
    "        B = B[:, None]\n",
    "\n",
    "        return A, B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Legendre (LegT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LegT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LegT():\n",
    "    legt_matrices = TransMatrix(N=8, measure=\"legt\", lambda_n=1.0)\n",
    "    A, B = legt_matrices.A_matrix, legt_matrices.B_matrix\n",
    "    gu_legt_matrices = GuTransMatrix(N=8, measure=\"legt\", lambda_n=1.0)\n",
    "    gu_A, gu_B = gu_legt_matrices.A_matrix, gu_legt_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -1.          1.7320508  -2.2360678   2.6457512  -3.          3.3166246\n",
      "   -3.6055512   3.8729832]\n",
      " [ -1.7320508  -3.          3.872983   -4.5825753   5.196152   -5.744562\n",
      "    6.244998   -6.708204 ]\n",
      " [ -2.2360678  -3.872983   -4.999999    5.916079   -6.7082033   7.4161973\n",
      "   -8.062257    8.660253 ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -6.9999995   7.937254   -8.774963\n",
      "    9.5393915 -10.24695  ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -9.          9.949874\n",
      "  -10.816654   11.61895  ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874  -10.999999\n",
      "   11.958261  -12.845232 ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261\n",
      "  -13.         13.964239 ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232\n",
      "  -13.964239  -14.999999 ]]\n",
      "Gu's A:\n",
      " [[ -1.          1.7320508  -2.2360678   2.6457512  -3.          3.3166246\n",
      "   -3.6055512   3.8729832]\n",
      " [ -1.7320508  -3.          3.872983   -4.5825753   5.196152   -5.744562\n",
      "    6.244998   -6.708204 ]\n",
      " [ -2.2360678  -3.872983   -4.999999    5.916079   -6.7082033   7.4161973\n",
      "   -8.062257    8.660253 ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -6.9999995   7.937254   -8.774963\n",
      "    9.5393915 -10.24695  ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -9.          9.949874\n",
      "  -10.816654   11.61895  ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874  -10.999999\n",
      "   11.958261  -12.845232 ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261\n",
      "  -13.         13.964239 ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232\n",
      "  -13.964239  -14.999999 ]]\n",
      "B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n",
      "Gu's B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n"
     ]
    }
   ],
   "source": [
    "test_LegT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LMU():\n",
    "    lmu_matrices = TransMatrix(\n",
    "        N=8, measure=\"legt\", lambda_n=2.0\n",
    "    )  # change lambda so resulting matrix is in the form of LMU\n",
    "    A, B = lmu_matrices.A_matrix, lmu_matrices.B_matrix\n",
    "    gu_lmu_matrices = GuTransMatrix(\n",
    "        N=8, measure=\"legt\", lambda_n=2.0\n",
    "    )  # change lambda so resulting matrix is in the form of LMU\n",
    "    gu_A, gu_B = gu_lmu_matrices.A_matrix, gu_lmu_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.]\n",
      " [ -5.   5.  -5.  -5.  -5.  -5.  -5.  -5.]\n",
      " [  7.  -7.   7.  -7.  -7.  -7.  -7.  -7.]\n",
      " [ -9.   9.  -9.   9.  -9.  -9.  -9.  -9.]\n",
      " [ 11. -11.  11. -11.  11. -11. -11. -11.]\n",
      " [-13.  13. -13.  13. -13.  13. -13. -13.]\n",
      " [ 15. -15.  15. -15.  15. -15.  15. -15.]]\n",
      "Gu's A:\n",
      " [[ -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.]\n",
      " [ -5.   5.  -5.  -5.  -5.  -5.  -5.  -5.]\n",
      " [  7.  -7.   7.  -7.  -7.  -7.  -7.  -7.]\n",
      " [ -9.   9.  -9.   9.  -9.  -9.  -9.  -9.]\n",
      " [ 11. -11.  11. -11.  11. -11. -11. -11.]\n",
      " [-13.  13. -13.  13. -13.  13. -13. -13.]\n",
      " [ 15. -15.  15. -15.  15. -15.  15. -15.]]\n",
      "B:\n",
      " [[  1.]\n",
      " [ -3.]\n",
      " [  5.]\n",
      " [ -7.]\n",
      " [  9.]\n",
      " [-11.]\n",
      " [ 13.]\n",
      " [-15.]]\n",
      "Gu's B:\n",
      " [[  1.]\n",
      " [ -3.]\n",
      " [  5.]\n",
      " [ -7.]\n",
      " [  9.]\n",
      " [-11.]\n",
      " [ 13.]\n",
      " [-15.]]\n"
     ]
    }
   ],
   "source": [
    "test_LMU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Laguerre (LagT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LagT():\n",
    "    lagt_matrices = TransMatrix(\n",
    "        N=8,\n",
    "        measure=\"lagt\",\n",
    "        alpha=0.0,  # change resulting tilt through alpha and beta\n",
    "        beta=1.0,\n",
    "    )  # change resulting tilt through alpha and beta\n",
    "    A, B = lagt_matrices.A_matrix, lagt_matrices.B_matrix\n",
    "    gu_lagt_matrices = GuTransMatrix(\n",
    "        N=8,\n",
    "        measure=\"lagt\",\n",
    "        alpha=0.0,  # change resulting tilt through alpha and beta\n",
    "        beta=1.0,\n",
    "    )  # change resulting tilt through alpha and beta\n",
    "    gu_A, gu_B = gu_lagt_matrices.A_matrix, gu_lagt_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-1.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -0.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -0.        ]\n",
      " [-0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976\n",
      "  -0.99999976 -1.        ]]\n",
      "Gu's A:\n",
      " [[-1.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -0.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -0.        ]\n",
      " [-0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976\n",
      "  -0.99999976 -1.        ]]\n",
      "B:\n",
      " [[1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Gu's B:\n",
      " [[1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n"
     ]
    }
   ],
   "source": [
    "test_LagT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Legendre (LegS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LegS():\n",
    "    legs_matrices = TransMatrix(N=8, measure=\"legs\")\n",
    "    A, B = legs_matrices.A_matrix, legs_matrices.B_matrix\n",
    "    gu_legs_matrices = GuTransMatrix(N=8, measure=\"legs\")\n",
    "    gu_A, gu_B = gu_legs_matrices.A_matrix, gu_legs_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -1.          0.          0.          0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -1.7320508  -2.          0.          0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.2360678  -3.872983   -2.9999995   0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -4.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -5.          0.\n",
      "    0.          0.       ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874   -5.9999995\n",
      "    0.          0.       ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261\n",
      "   -7.0000005   0.       ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232\n",
      "  -13.964239   -8.       ]]\n",
      "Gu's A:\n",
      " [[ -1.          0.          0.          0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -1.7320508  -1.9999999   0.          0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.2360678  -3.872983   -3.          0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.6457512  -4.582576   -5.91608    -4.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -3.         -5.196152   -6.7082047  -7.9372544  -5.          0.\n",
      "    0.          0.       ]\n",
      " [ -3.3166246  -5.744562   -7.4161987  -8.774965   -9.949874   -6.\n",
      "    0.          0.       ]\n",
      " [ -3.6055512  -6.244998   -8.062259   -9.539392  -10.816654  -11.958261\n",
      "   -7.          0.       ]\n",
      " [ -3.8729832  -6.708204   -8.6602545 -10.246951  -11.61895   -12.845232\n",
      "  -13.964239   -7.9999995]]\n",
      "B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n",
      "Gu's B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n"
     ]
    }
   ],
   "source": [
    "test_LegS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Recurrent Unit (FRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FRU():\n",
    "    fru_matrices = TransMatrix(N=8, measure=\"fourier\", fourier_type=\"fru\")\n",
    "    A, B = fru_matrices.A_matrix, fru_matrices.B_matrix\n",
    "    gu_fru_matrices = GuTransMatrix(N=8, measure=\"fourier\", fourier_type=\"fru\")\n",
    "    gu_A, gu_B = gu_fru_matrices.A_matrix, gu_fru_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-1.        -0.        -1.4142135  0.        -1.4142135  0.\n",
      "  -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   0.         0.       ]\n",
      " [-1.4142135  0.        -2.        -3.1415927 -2.         0.\n",
      "  -2.         0.       ]\n",
      " [ 0.         0.         3.1415927  0.         0.         0.\n",
      "   0.         0.       ]\n",
      " [-1.4142135  0.        -2.         0.        -2.        -6.2831855\n",
      "  -2.         0.       ]\n",
      " [ 0.         0.         0.         0.         6.2831855  0.\n",
      "   0.         0.       ]\n",
      " [-1.4142135  0.        -2.         0.        -2.         0.\n",
      "  -2.        -9.424778 ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   9.424778   0.       ]]\n",
      "Gu's A:\n",
      " [[-1.         0.        -1.4142135  0.        -1.4142135  0.\n",
      "  -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999 -3.1415927 -1.9999999  0.\n",
      "  -1.9999999  0.       ]\n",
      " [ 0.         0.         3.1415927  0.         0.         0.\n",
      "   0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999  0.        -1.9999999 -6.2831855\n",
      "  -1.9999999  0.       ]\n",
      " [ 0.         0.         0.         0.         6.2831855  0.\n",
      "   0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999  0.        -1.9999999  0.\n",
      "  -1.9999999 -9.424778 ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   9.424778   0.       ]]\n",
      "B:\n",
      " [[1.       ]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]]\n",
      "Gu's B:\n",
      " [[1.       ]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]]\n"
     ]
    }
   ],
   "source": [
    "test_FRU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated Fourier (FouT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FouT():\n",
    "    fout_matrices = TransMatrix(N=8, measure=\"fourier\", fourier_type=\"fout\")\n",
    "    A, B = fout_matrices.A_matrix, fout_matrices.B_matrix\n",
    "    gu_fout_matrices = GuTransMatrix(N=8, measure=\"fourier\", fourier_type=\"fout\")\n",
    "    gu_A, gu_B = gu_fout_matrices.A_matrix, gu_fout_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -2.         -0.         -2.828427    0.         -2.828427    0.\n",
      "   -2.828427    0.       ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.828427    0.         -4.         -6.2831855  -4.          0.\n",
      "   -4.          0.       ]\n",
      " [  0.          0.          6.2831855   0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.828427    0.         -4.          0.         -4.        -12.566371\n",
      "   -4.          0.       ]\n",
      " [  0.          0.          0.          0.         12.566371    0.\n",
      "    0.          0.       ]\n",
      " [ -2.828427    0.         -4.          0.         -4.          0.\n",
      "   -4.        -18.849556 ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "   18.849556    0.       ]]\n",
      "Gu's A:\n",
      " [[ -2.          0.         -2.828427    0.         -2.828427    0.\n",
      "   -2.828427    0.       ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.828427    0.         -3.9999998  -6.2831855  -3.9999998   0.\n",
      "   -3.9999998   0.       ]\n",
      " [  0.          0.          6.2831855   0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.828427    0.         -3.9999998   0.         -3.9999998 -12.566371\n",
      "   -3.9999998   0.       ]\n",
      " [  0.          0.          0.          0.         12.566371    0.\n",
      "    0.          0.       ]\n",
      " [ -2.828427    0.         -3.9999998   0.         -3.9999998   0.\n",
      "   -3.9999998 -18.849556 ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "   18.849556    0.       ]]\n",
      "B:\n",
      " [[2.      ]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]]\n",
      "Gu's B:\n",
      " [[2.      ]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]]\n"
     ]
    }
   ],
   "source": [
    "test_FouT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier With Decay (FourD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FourD():\n",
    "    fourd_matrices = TransMatrix(N=8, measure=\"fourier\", fourier_type=\"fourd\")\n",
    "    A, B = fourd_matrices.A_matrix, fourd_matrices.B_matrix\n",
    "    gu_fourd_matrices = GuTransMatrix(N=8, measure=\"fourier\", fourier_type=\"fourd\")\n",
    "    gu_A, gu_B = gu_fourd_matrices.A_matrix, gu_fourd_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-0.5        -0.         -0.70710677  0.         -0.70710677  0.\n",
      "  -0.70710677  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.70710677  0.         -1.         -3.1415927  -1.          0.\n",
      "  -1.          0.        ]\n",
      " [ 0.          0.          3.1415927   0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.70710677  0.         -1.          0.         -1.         -6.2831855\n",
      "  -1.          0.        ]\n",
      " [ 0.          0.          0.          0.          6.2831855   0.\n",
      "   0.          0.        ]\n",
      " [-0.70710677  0.         -1.          0.         -1.          0.\n",
      "  -1.         -9.424778  ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   9.424778    0.        ]]\n",
      "Gu's A:\n",
      " [[-0.5         0.         -0.70710677  0.         -0.70710677  0.\n",
      "  -0.70710677  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.70710677  0.         -0.99999994 -3.1415927  -0.99999994  0.\n",
      "  -0.99999994  0.        ]\n",
      " [ 0.          0.          3.1415927   0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.70710677  0.         -0.99999994  0.         -0.99999994 -6.2831855\n",
      "  -0.99999994  0.        ]\n",
      " [ 0.          0.          0.          0.          6.2831855   0.\n",
      "   0.          0.        ]\n",
      " [-0.70710677  0.         -0.99999994  0.         -0.99999994  0.\n",
      "  -0.99999994 -9.424778  ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   9.424778    0.        ]]\n",
      "B:\n",
      " [[0.5       ]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]]\n",
      "Gu's B:\n",
      " [[0.5       ]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "test_FourD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities For Gu HiPPO Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_up(a, s=None, drop=True, dim=0):\n",
    "    assert dim == 0\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(a[0, ...])\n",
    "    s = s.unsqueeze(dim)\n",
    "    if drop:\n",
    "        a = a[:-1, ...]\n",
    "    return torch.cat((s, a), dim=dim)\n",
    "\n",
    "def interleave(a, b, uneven=False, dim=0):\n",
    "    \"\"\" Interleave two tensors of same shape \"\"\"\n",
    "    # assert(a.shape == b.shape)\n",
    "    assert dim == 0 # TODO temporary to make handling uneven case easier\n",
    "    if dim < 0:\n",
    "        dim = N + dim\n",
    "    if uneven:\n",
    "        a_ = a[-1:, ...]\n",
    "        a = a[:-1, ...]\n",
    "    c = torch.stack((a, b), dim+1)\n",
    "    out_shape = list(a.shape)\n",
    "    out_shape[dim] *= 2\n",
    "    c = c.view(out_shape)\n",
    "    if uneven:\n",
    "        c = torch.cat((c, a_), dim=dim)\n",
    "    return c\n",
    "\n",
    "def batch_mult(A, u, has_batch=None):\n",
    "    \"\"\" Matrix mult A @ u with special case to save memory if u has additional batch dim\n",
    "\n",
    "    The batch dimension is assumed to be the second dimension\n",
    "    A : (L, ..., N, N)\n",
    "    u : (L, [B], ..., N)\n",
    "    has_batch: True, False, or None. If None, determined automatically\n",
    "\n",
    "    Output:\n",
    "    x : (L, [B], ..., N)\n",
    "      A @ u broadcasted appropriately\n",
    "    \"\"\"\n",
    "\n",
    "    if has_batch is None:\n",
    "        has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    if has_batch:\n",
    "        u = u.permute([0] + list(range(2, len(u.shape))) + [1])\n",
    "    else:\n",
    "        u = u.unsqueeze(-1)\n",
    "    v = (A @ u)\n",
    "    if has_batch:\n",
    "        v = v.permute([0] + [len(u.shape)-1] + list(range(1, len(u.shape)-1)))\n",
    "    else:\n",
    "        v = v[..., 0]\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "### Main unrolling functions\n",
    "\n",
    "def unroll(A, u):\n",
    "    \"\"\"\n",
    "    A : (..., N, N) # TODO I think this can't take batch dimension?\n",
    "    u : (L, ..., N)\n",
    "    output : x (..., N) # TODO a lot of these shapes are wrong\n",
    "    x[i, ...] = A^{i} @ u[0, ...] + ... + A @ u[i-1, ...] + u[i, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    m = u.new_zeros(u.shape[1:])\n",
    "    outputs = []\n",
    "    for u_ in torch.unbind(u, dim=0):\n",
    "        m = F.linear(m, A) + u_\n",
    "        outputs.append(m)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive(A, u):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of unroll. \"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_(A, u):\n",
    "        if u.shape[0] == 1:\n",
    "            return u\n",
    "\n",
    "        u_evens = u[0::2, ...]\n",
    "        u_odds = u[1::2, ...]\n",
    "\n",
    "        # u2 = F.linear(u_evens, A) + u_odds\n",
    "        u2 = (A @ u_evens.unsqueeze(-1)).squeeze(-1) + u_odds\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_odds = parallel_unroll_recursive_(A2, u2)\n",
    "        # x_evens = F.linear(shift_up(x_odds), A) + u_evens\n",
    "        x_evens = (A @ shift_up(x_odds).unsqueeze(-1)).squeeze(-1) + u_evens\n",
    "\n",
    "        x = interleave(x_evens, x_odds, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n)/math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N-u.shape[0],) + u.shape[1:] )), dim=0)\n",
    "\n",
    "    return parallel_unroll_recursive_(A, u)[:n, ...]\n",
    "\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive_br(A, u):\n",
    "    \"\"\" Same as parallel_unroll_recursive but uses bit reversal for locality. \"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_br_(A, u):\n",
    "        n = u.shape[0]\n",
    "        if n == 1:\n",
    "            return u\n",
    "\n",
    "        m = n//2\n",
    "        u_0 = u[:m, ...]\n",
    "        u_1 = u[m:, ...]\n",
    "\n",
    "        u2 = F.linear(u_0, A) + u_1\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_1 = parallel_unroll_recursive_br_(A2, u2)\n",
    "        x_0 = F.linear(shift_up(x_1), A) + u_0\n",
    "\n",
    "        # x = torch.cat((x_0, x_1), dim=0) # is there a way to do this with cat?\n",
    "        x = interleave(x_0, x_1, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n)/math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N-u.shape[0],) + u.shape[1:] )), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    x = parallel_unroll_recursive_br_(A, u)\n",
    "    return x[:n, ...]\n",
    "\n",
    "def parallel_unroll_iterative(A, u):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of unroll, implemented iteratively \"\"\"\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n)/math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N-u.shape[0],) + u.shape[1:] )), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    # Main recursive loop, flattened\n",
    "    us = [] # stores the u_0 terms in the recursive version\n",
    "    N_ = N\n",
    "    As = [] # stores the A matrices\n",
    "    for l in range(m):\n",
    "        N_ = N_ // 2\n",
    "        As.append(A)\n",
    "        u_0 = u[:N_, ...]\n",
    "        us.append(u_0)\n",
    "        u = F.linear(u_0, A) + u[N_:, ...]\n",
    "        A = A @ A\n",
    "    x_0 = []\n",
    "    x = u # x_1\n",
    "    for l in range(m-1, -1, -1):\n",
    "        x_0 = F.linear(shift_up(x), As[l]) + us[l]\n",
    "        x = interleave(x_0, x, dim=0)\n",
    "\n",
    "    return x[:n, ...]\n",
    "\n",
    "\n",
    "def variable_unroll_sequential(A, u, s=None, variable=True):\n",
    "    \"\"\" Unroll with variable (in time/length) transitions A.\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] @ s + A[i..1] @ u[0] + ... + A[i] @ u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        # s = F.linear(s, A_) + u_\n",
    "        s = batch_mult(A_.unsqueeze(0), s.unsqueeze(0), has_batch)[0]\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def variable_unroll(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of variable_unroll. \"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        return variable_unroll_sequential(A, u, s, variable)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1  = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1  = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = A_1 @ A_0_\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll(A_10, u_10, s, variable, recurse_limit)\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "\n",
    "    x = interleave(x_0, x_1, uneven, dim=0) # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "def variable_unroll_general_sequential(A, u, s, op, variable=True):\n",
    "    \"\"\" Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        s = op(A_, s)\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "def variable_unroll_matrix_sequential(A, u, s=None, variable=True):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    return variable_unroll_general_sequential(A, u, s, op, variable=True)\n",
    "\n",
    "def variable_unroll_toeplitz_sequential(A, u, s=None, variable=True, pad=False):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        ret = variable_unroll_general_sequential(A, u, s, triangular_toeplitz_multiply_padded, variable=True)\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    return variable_unroll_general_sequential(A, u, s, triangular_toeplitz_multiply, variable=True)\n",
    "\n",
    "\n",
    "\n",
    "### General parallel scan functions with generic binary composition operators\n",
    "\n",
    "def variable_unroll_general(A, u, s, op, compose_op=None, sequential_op=None, variable=True, recurse_limit=16):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of variable_unroll.\n",
    "\n",
    "    compose is an optional function that defines how to compose A without multiplying by a leaf u\n",
    "    \"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        if sequential_op is None:\n",
    "            sequential_op = op\n",
    "        return variable_unroll_general_sequential(A, u, s, sequential_op, variable)\n",
    "\n",
    "    if compose_op is None:\n",
    "        compose_op = op\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1 = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1 = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = op(A_1, u_0_) # batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = compose_op(A_1, A_0_)\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll_general(A_10, u_10, s, op, compose_op, sequential_op, variable=variable, recurse_limit=recurse_limit)\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = op(A_0, x_0) # batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "\n",
    "    x = interleave(x_0, x_1, uneven, dim=0) # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "def variable_unroll_matrix(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "    op = lambda x, y: batch_mult(x, y, has_batch)\n",
    "    sequential_op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    matmul = lambda x, y: x @ y\n",
    "    return variable_unroll_general(A, u, s, op, compose_op=matmul, sequential_op=sequential_op, variable=variable, recurse_limit=recurse_limit)\n",
    "\n",
    "def variable_unroll_toeplitz(A, u, s=None, variable=True, recurse_limit=8, pad=False):\n",
    "    \"\"\" Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (L, [B], ..., N) same shape as u\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "    # Add the batch dimension to A if necessary\n",
    "    A_batch_dims = len(A.shape) - int(variable)\n",
    "    u_batch_dims = len(u.shape)-1\n",
    "    if u_batch_dims > A_batch_dims:\n",
    "        # assert u_batch_dims == A_batch_dims + 1\n",
    "        if variable:\n",
    "            while len(A.shape) < len(u.shape):\n",
    "                A = A.unsqueeze(1)\n",
    "        # else:\n",
    "        #     A = A.unsqueeze(0)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        op = triangular_toeplitz_multiply_padded\n",
    "        ret = variable_unroll_general(A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit)\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    op = triangular_toeplitz_multiply\n",
    "    ret = variable_unroll_general(A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gu's HiPPO LegT Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LegT(nn.Module):\n",
    "    def __init__(self, N, dt=1.0, discretization=\"bilinear\", lambda_n=1.0):\n",
    "        \"\"\"\n",
    "        N: the order of the HiPPO projection\n",
    "        dt: discretization step size - should be roughly inverse to the length of the sequence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        # A, B = transition('lmu', N)\n",
    "        legt_matrices = GuTransMatrix(N=N, measure=\"legt\", lambda_n=lambda_n)\n",
    "        A = legt_matrices.A_matrix\n",
    "        B = legt_matrices.B_matrix\n",
    "        C = np.ones((1, N))\n",
    "        D = np.zeros((1,))\n",
    "        # dt, discretization options\n",
    "        A, B, _, _, _ = signal.cont2discrete((A, B, C, D), dt=dt, method=discretization)\n",
    "\n",
    "        B = B.squeeze(-1)\n",
    "\n",
    "        self.register_buffer(\"A\", torch.Tensor(A))  # (N, N)\n",
    "        self.register_buffer(\"B\", torch.Tensor(B))  # (N,)\n",
    "\n",
    "        # vals = np.linspace(0.0, 1.0, 1./dt)\n",
    "        vals = np.arange(0.0, 1.0, dt)\n",
    "        self.eval_matrix = torch.Tensor(\n",
    "            ss.eval_legendre(np.arange(N)[:, None], 1 - 2 * vals).T\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = inputs * self.B  # (length, ..., N)\n",
    "\n",
    "        c = torch.zeros(u.shape[1:])\n",
    "        cs = []\n",
    "        for f in inputs:\n",
    "            c = F.linear(c, self.A) + self.B * f\n",
    "            # print(f\"f:\\n{f}\")\n",
    "            cs.append(c)\n",
    "        return torch.stack(cs, dim=0)\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        return (self.eval_matrix @ c.unsqueeze(-1)).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gu's Scale invariant HiPPO LegS Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LegS(nn.Module):\n",
    "    \"\"\"Vanilla HiPPO-LegS model (scale invariant instead of time invariant)\"\"\"\n",
    "\n",
    "    def __init__(self, N, max_length=1024, measure=\"legs\", discretization=\"bilinear\"):\n",
    "        \"\"\"\n",
    "        max_length: maximum sequence length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        legs_matrices = GuTransMatrix(N=self.N, measure=measure)\n",
    "        A = legs_matrices.A_matrix\n",
    "        B = legs_matrices.B_matrix\n",
    "        # A, B = transition(measure, N)\n",
    "        B = B.squeeze(-1)\n",
    "        A_stacked = np.empty((max_length, N, N), dtype=A.dtype)\n",
    "        B_stacked = np.empty((max_length, N), dtype=B.dtype)\n",
    "        for t in range(1, max_length + 1):\n",
    "            At = A / t\n",
    "            Bt = B / t\n",
    "            if discretization == \"forward\":\n",
    "                A_stacked[t - 1] = np.eye(N) + At\n",
    "                B_stacked[t - 1] = Bt\n",
    "            elif discretization == \"backward\":\n",
    "                A_stacked[t - 1] = la.solve_triangular(\n",
    "                    np.eye(N) - At, np.eye(N), lower=True\n",
    "                )\n",
    "                B_stacked[t - 1] = la.solve_triangular(np.eye(N) - At, Bt, lower=True)\n",
    "            elif discretization == \"bilinear\":\n",
    "                alpha = 0.5\n",
    "                A_stacked[t - 1] = np.linalg.lstsq(\n",
    "                    np.eye(N) - (At * alpha), np.eye(N) + (At * alpha), rcond=None\n",
    "                )[\n",
    "                    0\n",
    "                ]  # TODO: Referencing this: https://stackoverflow.com/questions/64527098/numpy-linalg-linalgerror-singular-matrix-error-when-trying-to-solve\n",
    "                B_stacked[t - 1] = np.linalg.lstsq(\n",
    "                    np.eye(N) - (At * alpha), Bt, rcond=None\n",
    "                )[0]\n",
    "            else:  # ZOH\n",
    "                A_stacked[t - 1] = la.expm(A * (math.log(t + 1) - math.log(t)))\n",
    "                B_stacked[t - 1] = la.solve_triangular(\n",
    "                    A, A_stacked[t - 1] @ B - B, lower=True\n",
    "                )\n",
    "        self.A_stacked = torch.Tensor(A_stacked.copy())  # (max_length, N, N)\n",
    "        self.B_stacked = torch.Tensor(B_stacked.copy())  # (max_length, N)\n",
    "        vals = np.linspace(0.0, 1.0, max_length)\n",
    "        self.eval_matrix = torch.from_numpy(\n",
    "            np.asarray(\n",
    "                ((B[:, None] * ss.eval_legendre(np.arange(N)[:, None], 2 * vals - 1)).T)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, fast=False):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "        result = None\n",
    "\n",
    "        L = inputs.shape[0]\n",
    "\n",
    "        u = inputs.unsqueeze(-1)\n",
    "        u = torch.transpose(u, 0, -2)\n",
    "        u = u * self.B_stacked[:L]  # c_k = A @ c_{k-1} + B @ f_k\n",
    "        print(f\"u - Gu: {u}\")\n",
    "        my_b = torch.Tensor(\n",
    "            [\n",
    "                [6.6666657e-01],\n",
    "                [5.7735050e-01],\n",
    "                [1.4907140e-01],\n",
    "                [-2.3096800e-07],\n",
    "                [-2.7939677e-09],\n",
    "                [2.9616058e-07],\n",
    "                [-2.2817403e-08],\n",
    "                [-8.1490725e-08],\n",
    "            ]\n",
    "        )\n",
    "        u = torch.transpose(u, 0, -2)  # (length, ..., N)\n",
    "\n",
    "        # print(f\"A_stacked: {self.A_stacked[:L]}\")\n",
    "        # print(f\"B_stacked: {self.B_stacked[:L]}\")\n",
    "\n",
    "        if fast:\n",
    "            result = variable_unroll_matrix(self.A_stacked[:L], u)\n",
    "\n",
    "        else:\n",
    "            result = variable_unroll_matrix_sequential(self.A_stacked[:L], u)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        a = self.eval_matrix @ c.unsqueeze(-1)\n",
    "        return a.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Of General HiPPO Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO(jnn.Module):\n",
    "    \"\"\"\n",
    "    class that constructs HiPPO model using the defined measure.\n",
    "\n",
    "    Args:\n",
    "        N (int): order of the HiPPO projection, aka the number of coefficients to describe the matrix\n",
    "        max_length (int): maximum sequence length to be input\n",
    "        measure (str): the measure used to define which way to instantiate the HiPPO matrix\n",
    "        step (float): step size used for descretization\n",
    "        GBT_alpha (float): represents which descretization transformation to use based off the alpha value\n",
    "        seq_L (int): length of the sequence to be used for training\n",
    "        v (str): choice of vectorized or non-vectorized function instantiation\n",
    "            - 'v': vectorized\n",
    "            - 'nv': non-vectorized\n",
    "        lambda_n (float): value associated with the tilt of legt\n",
    "            - 1: tilt on legt\n",
    "            - \\sqrt(2n+1)(-1)^{N}: tilt associated with the legendre memory unit (LMU)\n",
    "        fourier_type (str): choice of fourier measures\n",
    "            - fru: fourier recurrent unit measure (FRU) - 'fru'\n",
    "            - fout: truncated Fourier (FouT) - 'fout'\n",
    "            - fourd: decaying fourier transform - 'fourd'\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "    \"\"\"\n",
    "\n",
    "    N: int\n",
    "    max_length: int\n",
    "    step: float\n",
    "    GBT_alpha: float\n",
    "    seq_L: int\n",
    "    A: jnp.ndarray\n",
    "    B: jnp.ndarray\n",
    "    measure: str\n",
    "\n",
    "    def setup(self):\n",
    "        A = self.A\n",
    "        B = self.B\n",
    "        self.C = jnp.ones((self.N,))\n",
    "        self.D = jnp.zeros((1,))\n",
    "\n",
    "        if self.measure == \"legt\":\n",
    "            L = self.seq_L\n",
    "            vals = jnp.arange(0.0, 1.0, L)\n",
    "            # n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 1 - 2 * vals\n",
    "            self.eval_matrix = jax.scipy.special.lpmn_values(\n",
    "                m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "            ).T  # ss.eval_legendre(n, x).T\n",
    "\n",
    "        elif self.measure == \"lmu\":\n",
    "            raise NotImplementedError(\"LMU measure not implemented yet\")\n",
    "\n",
    "        elif self.measure == \"legs\":\n",
    "            L = self.max_length\n",
    "            vals = jnp.linspace(0.0, 1.0, L)\n",
    "            # n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 2 * vals - 1\n",
    "            self.eval_matrix = (\n",
    "                B[:, None]\n",
    "                * jax.scipy.special.lpmn_values(\n",
    "                    m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "                )\n",
    "            ).T  # ss.eval_legendre(n, x)).T\n",
    "\n",
    "        elif self.measure == \"lagt\":\n",
    "            raise NotImplementedError(\"Translated Laguerre measure not implemented yet\")\n",
    "\n",
    "        elif self.measure == \"fru\":\n",
    "            raise NotImplementedError(\n",
    "                \"Fourier Recurrent Unit measure not implemented yet\"\n",
    "            )\n",
    "\n",
    "        elif self.measure == \"fout\":\n",
    "            raise NotImplementedError(\"Translated Fourier measure not implemented yet\")\n",
    "\n",
    "        elif self.measure == \"fourd\":\n",
    "            raise NotImplementedError(\"Decaying Fourier measure not implemented yet\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"invalid measure\")\n",
    "\n",
    "    def __call__(self, f, init_state=None, t_step=0, kernel=False):\n",
    "        # print(f\"u shape:\\n{f.shape}\")\n",
    "        # print(f\"u:\\n{f}\")\n",
    "        if not kernel:\n",
    "            if init_state is None:\n",
    "                init_state = jnp.zeros((self.N, 1))\n",
    "\n",
    "            # Ab, Bb, Cb, Db = self.collect_SSM_vars(\n",
    "            #     self.A, self.B, self.C, self.D, f, t_step=t_step, alpha=self.GBT_alpha\n",
    "            # )\n",
    "            c_k, y_k = self.loop_SSM(\n",
    "                A=self.A,\n",
    "                B=self.B,\n",
    "                C=self.C,\n",
    "                D=self.D,\n",
    "                c_0=init_state,\n",
    "                f=f,\n",
    "                t_step=t_step,\n",
    "                alpha=self.GBT_alpha,\n",
    "            )\n",
    "            # c_k, y_k = self.scan_SSM(Ab=Ab, Bb=Bb, Cb=Cb, Db=Db, c_0=init_state, f=f)\n",
    "\n",
    "        else:\n",
    "            Ab, Bb, Cb, Db = self.discretize(\n",
    "                self.A, self.B, self.C, self.D, step=self.step, alpha=self.GBT_alpha\n",
    "            )\n",
    "            c_k, y_k = self.causal_convolution(\n",
    "                f, self.K_conv(Ab, Bb, Cb, Db, L=self.max_length)\n",
    "            )\n",
    "\n",
    "        return c_k\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        \"\"\"\n",
    "        Uses coeffecients to reconstruct the signal\n",
    "\n",
    "        Args:\n",
    "            c (jnp.ndarray): coefficients of the HiPPO projection\n",
    "\n",
    "        Returns:\n",
    "            reconstructed signal\n",
    "        \"\"\"\n",
    "        return (self.eval_matrix @ jnp.expand_dims(c, -1)).squeeze(-1)\n",
    "\n",
    "    def discretize(self, A, B, C, D, step, alpha=0.5):\n",
    "        \"\"\"\n",
    "        function used for discretizing the HiPPO matrix\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            step (float): step size used for discretization\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "                - forward Euler corresponds to  = 0,\n",
    "                - backward Euler corresponds to  = 1,\n",
    "                - bilinear corresponds to  = 0.5,\n",
    "                - Zero-order Hold corresponds to  > 1\n",
    "        \"\"\"\n",
    "        I = jnp.eye(A.shape[0])\n",
    "        step_size = 1 / step\n",
    "        part1 = I - (A * step_size * alpha)\n",
    "        part2 = I + (A * step_size * (1 - alpha))\n",
    "\n",
    "        GBT_A = jnp.linalg.lstsq(part1, part2, rcond=None)[0]\n",
    "\n",
    "        base_GBT_B = jnp.linalg.lstsq(part1, B, rcond=None)[0]\n",
    "        GBT_B = step_size * base_GBT_B\n",
    "\n",
    "        if alpha > 1:  # Zero-order Hold\n",
    "            GBT_A = jax.scipy.linalg.expm(step_size * A)\n",
    "            GBT_B = (jnp.linalg.inv(A) @ (jax.scipy.linalg.expm(step_size * A) - I)) @ B\n",
    "\n",
    "        return GBT_A, GBT_B, C, D\n",
    "\n",
    "    def collect_SSM_vars(self, A, B, C, D, f, t_step=0, alpha=0.5):\n",
    "        \"\"\"\n",
    "        turns the continuos HiPPO matrix components into discrete ones\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            f (jnp.ndarray): input signal\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "\n",
    "        Returns:\n",
    "            Ab (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Bb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Cb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Db (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "        \"\"\"\n",
    "        N = A.shape[0]\n",
    "\n",
    "        if t_step == 0:\n",
    "            L = f.shape[0]  # seq_L, 1\n",
    "            assert (\n",
    "                L == self.seq_L\n",
    "            ), f\"sequence length must match, currently {L} != {self.seq_L}\"\n",
    "            assert N == self.N, f\"Order number must match, currently {N} != {self.N}\"\n",
    "        else:\n",
    "            L = t_step\n",
    "            assert t_step >= 1, f\"time step must be greater than 0, currently {t_step}\"\n",
    "            assert N == self.N, f\"Order number must match, currently {N} != {self.N}\"\n",
    "\n",
    "        Ab, Bb, Cb, Db = self.discretize(A, B, C, D, step=L, alpha=alpha)\n",
    "\n",
    "        return Ab, Bb, Cb, Db\n",
    "\n",
    "    def scan_SSM(self, Ad, Bd, Cd, Dd, c_0, f):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            f (jnp.ndarray): the input sequence\n",
    "            c_0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "\n",
    "        def step(c_k_1, f_k):\n",
    "            \"\"\"\n",
    "            Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "            Args:\n",
    "                c_k_1: previous hidden state\n",
    "                f_k: output from function f at, descritized, time step, k.\n",
    "                t:\n",
    "\n",
    "            Returns:\n",
    "                c_k: current hidden state\n",
    "                y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "            \"\"\"\n",
    "            part1 = Ad @ c_k_1\n",
    "            part2 = jnp.expand_dims((Bd @ f_k), -1)\n",
    "\n",
    "            c_k = part1 + part2\n",
    "            y_k = Cd @ c_k  # + (Db.T @ f_k)\n",
    "\n",
    "            return c_k, y_k\n",
    "\n",
    "        return jax.lax.scan(step, c_0, f)\n",
    "\n",
    "    def loop_SSM(self, A, B, C, D, c_0, f, t_step=0, alpha=0.5):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            f (jnp.ndarray): the input sequence\n",
    "            c_0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "        c_k_list = []\n",
    "        y_k_list = []\n",
    "        c_k = c_0.copy()\n",
    "        for i in range(1, (t_step + 1)):\n",
    "            f_idx = i - 1\n",
    "            Ad_i, Bd_i, Cd_i, Dd_i = self.collect_SSM_vars(\n",
    "                A=A, B=B, C=C, D=D, f=f, t_step=i, alpha=alpha\n",
    "            )\n",
    "            c_k, y_k = self.loop_step(\n",
    "                Ad=Ad_i, Bd=Bd_i, Cd=Cd_i, Dd=Dd_i, c_k_i=c_k, f_k=f[0][f_idx]\n",
    "            )\n",
    "            c_k_list.append(c_k.copy())\n",
    "            y_k_list.append(y_k.copy())\n",
    "\n",
    "        return c_k_list, y_k_list\n",
    "\n",
    "    def loop_step(self, Ad, Bd, Cd, Dd, c_k_i, f_k):\n",
    "        \"\"\"\n",
    "        Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "        Args:\n",
    "            c_k_i: previous hidden state\n",
    "            f_k: output from function f at, descritized, time step, k.\n",
    "\n",
    "        Returns:\n",
    "            c_k: current hidden state\n",
    "            y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "        \"\"\"\n",
    "\n",
    "        part1 = Ad @ c_k_i\n",
    "        part2 = f_k * Bd\n",
    "        c_k = part1 + part2\n",
    "        y_k = Cd @ c_k  # + (Db.T @ f_k)\n",
    "\n",
    "        return c_k, y_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # N = 256\n",
    "    # L = 128\n",
    "\n",
    "    N = 16\n",
    "    L = 8\n",
    "\n",
    "    np.random.seed(1701)\n",
    "    x = np.array(\n",
    "        [\n",
    "            [0.3527],\n",
    "            [0.6617],\n",
    "            [0.2434],\n",
    "            [0.6674],\n",
    "            [1.2293],\n",
    "            [0.0964],\n",
    "            [-2.2756],\n",
    "            [0.5618],\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    # x = np.array(\n",
    "    #     [\n",
    "    #         [1.00000],\n",
    "    #         [1.00000],\n",
    "    #         [1.00000],\n",
    "    #         [1.00000],\n",
    "    #         [1.00000],\n",
    "    #         [1.00000],\n",
    "    #         [1.00000],\n",
    "    #         [1.00000],\n",
    "    #     ],\n",
    "    #     dtype=np.float32,\n",
    "    # )\n",
    "\n",
    "    # x = torch.randn(L, 1)\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "    print(f\"THIS IS X:\\n{x}\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegT model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"\\nTesting HiPPO LegT model\")\n",
    "    hippo_legt = HiPPO_LegT(N, dt=1.0 / L)\n",
    "\n",
    "    c_k = hippo_legt(x)\n",
    "\n",
    "    # print(f\"Gu's Coeffiecients for LegT:\\n{c_k}\")\n",
    "    # print(f\"Gu's Coeffiecient shapes for LegT:\\n{c_k.shape}\")\n",
    "\n",
    "    # z = hippo_legt.reconstruct(c_k)\n",
    "    # print(f\"Gu's Reconstruction for LegT:\\n{z}\")\n",
    "    # print(f\"Gu's Reconstruction shape for LegT:\\n{z.shape}\")\n",
    "\n",
    "    # mse = loss(z[-1, 0, :L], x.squeeze(-1))\n",
    "    # print(f\"h-MSE shape:\\n{mse}\")\n",
    "    # print(f\"end of test for HiPPO LegT model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegS model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"\\nTesting HiPPO LegS model\")\n",
    "    hippo_legs = HiPPO_LegS(N, max_length=L)  # The Gu's\n",
    "\n",
    "    c_k = hippo_legs(x, fast=True)\n",
    "\n",
    "    print(f\"Gu's Coeffiecients  for LegS:\\n{c_k}\")\n",
    "    print(f\"Gu's Coeffiecient shapes for LegS:\\n{c_k.shape}\")\n",
    "\n",
    "    # z = hippo_legs.reconstruct(c_k)\n",
    "\n",
    "    # print(f\"Gu's Reconstruction for LegS:\\n{z}\")\n",
    "    # print(f\"Gu's Reconstruction shape for LegS:\\n{z.shape}\")\n",
    "\n",
    "    # print(y-z)\n",
    "    print(f\"end of test for HiPPO LegT model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test Generic HiPPO model --------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    the_measure = \"legs\"\n",
    "    print(f\"\\nTesting BRYANS HiPPO-{the_measure} model\")\n",
    "    legs_matrices = TransMatrix(N=N, measure=the_measure)\n",
    "    A = legs_matrices.A_matrix\n",
    "    B = legs_matrices.B_matrix\n",
    "    hippo_LegS_B = HiPPO(\n",
    "        N=N,\n",
    "        max_length=L,\n",
    "        step=1.0 / L,\n",
    "        GBT_alpha=0.5,\n",
    "        seq_L=L,\n",
    "        A=A,\n",
    "        B=B,\n",
    "        measure=the_measure,\n",
    "    )  # Bryan's\n",
    "\n",
    "    x = jnp.asarray(x, dtype=jnp.float32)  # convert torch array to jax array\n",
    "    print(f\"input:\\n{x}\")\n",
    "    print(f\"input type:\\n{type(x)}\")\n",
    "\n",
    "    print(f\"Bryan's Coeffiecients for HiPPO-{the_measure}\")\n",
    "\n",
    "    LTI_bool = True\n",
    "    # c_k = None\n",
    "    c_k = []\n",
    "    i = 0\n",
    "    params = hippo_LegS_B.init(key2, f=x, init_state=c_k, t_step=i)\n",
    "    if LTI_bool:\n",
    "        for i in range(1, (x.shape[0] + 1)):\n",
    "            # c_k = hippo_LegS_B.apply(params, f=x, init_state=c_k, t_step=i)\n",
    "            # c_k = hippo_LegS_B.apply(params, f=x, init_state=c_k, t_step=(x.shape[0]))\n",
    "            c_k = hippo_LegS_B.apply(params, f=x, t_step=(x.shape[0]))\n",
    "            gu_c_k = jnp.array(\n",
    "                        [\n",
    "                            [\n",
    "                                [\n",
    "                                    2.3513e-01,\n",
    "                                    2.0363e-01,\n",
    "                                    5.2577e-02,\n",
    "                                    -6.8057e-09,\n",
    "                                    1.1977e-08,\n",
    "                                    -1.0683e-09,\n",
    "                                    -8.7712e-09,\n",
    "                                    4.9337e-09,\n",
    "                                ]\n",
    "                            ],\n",
    "                            [\n",
    "                                [\n",
    "                                    4.0576e-01,\n",
    "                                    2.6490e-01,\n",
    "                                    -3.3701e-02,\n",
    "                                    -5.6627e-02,\n",
    "                                    -7.1343e-03,\n",
    "                                    -5.3342e-09,\n",
    "                                    -1.3616e-08,\n",
    "                                    7.8134e-09,\n",
    "                                ]\n",
    "                            ],\n",
    "                            [\n",
    "                                [\n",
    "                                    3.5937e-01,\n",
    "                                    7.2189e-02,\n",
    "                                    -2.2545e-01,\n",
    "                                    -8.6126e-02,\n",
    "                                    2.5252e-02,\n",
    "                                    1.1226e-02,\n",
    "                                    9.3872e-04,\n",
    "                                    -2.7088e-09,\n",
    "                                ]\n",
    "                            ],\n",
    "                            [\n",
    "                                [\n",
    "                                    4.2782e-01,\n",
    "                                    1.3816e-01,\n",
    "                                    -6.5221e-02,\n",
    "                                    1.5500e-01,\n",
    "                                    1.5606e-01,\n",
    "                                    2.6968e-02,\n",
    "                                    -4.6502e-03,\n",
    "                                    -1.5067e-03,\n",
    "                                ]\n",
    "                            ],\n",
    "                            [\n",
    "                                [\n",
    "                                    5.7355e-01,\n",
    "                                    3.0244e-01,\n",
    "                                    8.4267e-02,\n",
    "                                    1.8955e-01,\n",
    "                                    7.2271e-07,\n",
    "                                    -1.4422e-01,\n",
    "                                    -7.2802e-02,\n",
    "                                    -1.3106e-02,\n",
    "                                ]\n",
    "                            ],\n",
    "                            [\n",
    "                                [\n",
    "                                    5.0014e-01,\n",
    "                                    1.0705e-01,\n",
    "                                    -1.8648e-01,\n",
    "                                    -1.3038e-01,\n",
    "                                    -2.6791e-01,\n",
    "                                    -1.7971e-01,\n",
    "                                    4.9144e-02,\n",
    "                                    8.3597e-02,\n",
    "                                ]\n",
    "                            ],\n",
    "                            [\n",
    "                                [\n",
    "                                    1.3004e-01,\n",
    "                                    -4.8061e-01,\n",
    "                                    -7.1708e-01,\n",
    "                                    -4.4194e-01,\n",
    "                                    -2.8475e-01,\n",
    "                                    3.7278e-02,\n",
    "                                    2.1051e-01,\n",
    "                                    5.7035e-02,\n",
    "                                ]\n",
    "                            ],\n",
    "                            [\n",
    "                                [\n",
    "                                    1.8084e-01,\n",
    "                                    -2.9561e-01,\n",
    "                                    -2.3676e-01,\n",
    "                                    3.0236e-01,\n",
    "                                    5.1647e-01,\n",
    "                                    6.1457e-01,\n",
    "                                    3.6490e-01,\n",
    "                                    -2.4947e-02,\n",
    "                                ]\n",
    "                            ],\n",
    "                        ]\n",
    "                    )\n",
    "            idx = i - 1\n",
    "            g_c_k = gu_c_k[0][idx]\n",
    "            gu = jnp.expand_dims(g_c_k, -1)\n",
    "            #assert jnp.allclose(c_k[idx][:3], gu[:3], rtol=1e-04, atol=1e-06), f\"\\nc_k\\n{c_k[idx]}\\n\\n!=\\n\\ngu_c_k[0][{idx}]\\n{gu}\\n\"\n",
    "            print(f\"c_k output:\\n{c_k[idx]}\\n\\n\")\n",
    "    else:\n",
    "        for _ in range(x.shape[0]):\n",
    "            c_k = hippo_LegS_B.apply(params, f=x, init_state=c_k)\n",
    "            #print(c_k)\n",
    "            # print(f\"Bryan's Coeffiecient shape for HiPPO-{the_measure}:\\n{c_k.shape}\")\n",
    "\n",
    "    # y_legs = hippo_LegS_B.apply(\n",
    "    #     {\"params\": params}, c_k, method=hippo_LegS_B.reconstruct\n",
    "    # )\n",
    "\n",
    "    # print(f\"Bryan's Reconstruction for HiPPO-{the_measure}:\\n{y_legs}\")\n",
    "    # print(f\"Bryan's Reconstruction shape for HiPPO-{the_measure}:\\n{y_legs.shape}\")\n",
    "\n",
    "    print(f\"end of test for HiPPO-{the_measure} model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS X:\n",
      "tensor([[ 0.3527],\n",
      "        [ 0.6617],\n",
      "        [ 0.2434],\n",
      "        [ 0.6674],\n",
      "        [ 1.2293],\n",
      "        [ 0.0964],\n",
      "        [-2.2756],\n",
      "        [ 0.5618]])\n",
      "\n",
      "Testing HiPPO LegT model\n",
      "\n",
      "Testing HiPPO LegS model\n",
      "u - Gu: tensor([[[ 2.3513e-01,  2.0363e-01,  5.2577e-02, -6.8057e-09,  1.1977e-08,\n",
      "          -1.0683e-09, -8.7712e-09,  4.9337e-09,  2.4193e-09, -1.1095e-08,\n",
      "           8.6118e-09,  7.2668e-09, -8.7974e-09, -3.7027e-09,  4.4328e-09,\n",
      "           6.3843e-09, -1.6965e-08,  2.8071e-08, -2.8261e-08,  2.2708e-08,\n",
      "          -2.2831e-08,  1.6208e-08, -3.7894e-09, -7.3962e-09,  1.3591e-08,\n",
      "          -1.4951e-08,  1.2072e-08, -5.8378e-09,  8.5046e-09, -1.8455e-08,\n",
      "           2.3140e-08, -2.1376e-08],\n",
      "         [ 2.6468e-01,  3.0563e-01,  1.6910e-01,  5.0020e-02,  6.3019e-03,\n",
      "           1.8810e-09, -1.1216e-08,  6.1723e-09,  3.2040e-09, -1.2649e-08,\n",
      "           6.4224e-09,  1.0074e-08, -1.0795e-08, -3.2728e-09,  8.2969e-09,\n",
      "           4.8348e-09, -1.8868e-08,  3.1102e-08, -2.6925e-08,  1.4913e-08,\n",
      "          -1.3797e-08,  5.1350e-09,  5.3345e-09, -1.4744e-08,  2.3378e-08,\n",
      "          -2.1417e-08,  7.5115e-09,  8.0422e-09, -3.5252e-09, -1.3676e-08,\n",
      "           2.1313e-08, -2.1508e-08],\n",
      "         [ 6.9543e-02,  9.0339e-02,  6.4793e-02,  3.0666e-02,  9.4831e-03,\n",
      "           1.7473e-03,  1.4611e-04, -6.3698e-11,  6.4311e-09, -9.8203e-09,\n",
      "           9.9448e-09, -3.4636e-09,  4.3021e-09, -5.8968e-09, -2.6182e-09,\n",
      "           1.0350e-08, -1.1367e-08,  1.2458e-08, -1.2555e-08,  4.4572e-09,\n",
      "          -2.4310e-09,  9.8213e-09, -8.5037e-09,  4.4690e-09,  2.5328e-09,\n",
      "          -9.5093e-09,  6.8128e-09,  8.6205e-10,  3.9443e-10, -1.0717e-08,\n",
      "           1.0903e-08, -5.7360e-09],\n",
      "         [ 1.4831e-01,  2.0551e-01,  1.6883e-01,  9.9882e-02,  4.3560e-02,\n",
      "           1.3759e-02,  2.9916e-03,  4.0169e-04,  2.5158e-05, -7.0011e-09,\n",
      "           5.6133e-10,  6.3480e-09, -6.4952e-09, -5.9490e-10,  7.0811e-09,\n",
      "           4.4716e-10, -9.0578e-09,  1.3772e-08, -9.3259e-09,  5.5488e-10,\n",
      "           6.7141e-11, -2.8301e-09,  2.3960e-09, -4.6827e-09,  1.2419e-08,\n",
      "          -8.5096e-09, -6.3378e-09,  1.6649e-08, -1.0442e-08, -4.0256e-09,\n",
      "           1.0139e-08, -1.0894e-08],\n",
      "         [ 2.2351e-01,  3.2261e-01,  2.8834e-01,  1.9495e-01,  1.0316e-01,\n",
      "           4.2767e-02,  1.3674e-02,  3.2641e-03,  5.4871e-04,  5.7984e-05,\n",
      "           2.8791e-06,  1.8154e-08, -1.4813e-08,  1.2435e-08, -9.4309e-09,\n",
      "           5.2348e-08, -3.6901e-08,  3.3792e-08, -2.5849e-08, -1.3070e-08,\n",
      "           6.4364e-09,  5.2576e-09,  6.9982e-09, -3.7743e-08,  2.5664e-08,\n",
      "           1.1348e-08, -1.5463e-08,  9.3176e-09, -4.4852e-09, -2.1588e-08,\n",
      "           3.2104e-08, -1.3769e-08],\n",
      "         [ 1.4831e-02,  2.2018e-02,  2.0845e-02,  1.5415e-02,  9.2537e-03,\n",
      "           4.5468e-03,  1.8211e-03,  5.8684e-04,  1.4875e-04,  2.8590e-05,\n",
      "           3.9227e-06,  3.4146e-07,  1.5653e-08, -1.2607e-09, -1.1139e-09,\n",
      "           2.5951e-09, -2.0462e-09,  1.8049e-09, -2.2088e-09, -2.2700e-10,\n",
      "           5.8035e-10,  2.7412e-09, -2.4677e-09,  1.2042e-09,  1.1729e-09,\n",
      "          -2.9236e-09,  1.3896e-09,  7.0132e-10, -1.7378e-10, -2.9274e-09,\n",
      "           2.5935e-09, -5.2037e-10],\n",
      "         [-3.0341e-01, -4.5984e-01, -4.5396e-01, -3.5809e-01, -2.3507e-01,\n",
      "          -1.2994e-01, -6.0541e-02, -2.3648e-02, -7.6620e-03, -2.0250e-03,\n",
      "          -4.2578e-04, -6.8574e-05, -7.8921e-06, -5.8695e-07, -4.8417e-08,\n",
      "          -3.8051e-08,  7.5489e-08, -2.3758e-08, -4.7165e-08,  5.7073e-08,\n",
      "          -2.3115e-08,  2.4643e-08, -4.1219e-08,  1.7205e-08,  3.1836e-08,\n",
      "          -1.0608e-08,  3.7837e-08, -9.0032e-08,  2.4834e-08,  1.9605e-08,\n",
      "          -1.8115e-08,  1.2420e-08],\n",
      "         [ 6.6094e-02,  1.0176e-01,  1.0371e-01,  8.5900e-02,  6.0296e-02,\n",
      "           3.6360e-02,  1.8904e-02,  8.4611e-03,  3.2427e-03,  1.0548e-03,\n",
      "           2.8750e-04,  6.4478e-05,  1.1587e-05,  1.6057e-06,  1.6460e-07,\n",
      "           1.0456e-08, -3.2712e-09,  3.1495e-09, -9.4276e-10, -2.0077e-09,\n",
      "           2.1088e-09, -1.1722e-09, -2.3391e-09,  9.2737e-10,  4.4205e-09,\n",
      "          -1.4367e-09, -6.0752e-09,  8.9661e-09, -4.8414e-09, -2.4725e-09,\n",
      "           4.3928e-09, -4.2849e-09]]])\n",
      "Gu's Coeffiecients  for LegS:\n",
      "tensor([[[ 2.3513e-01,  2.0363e-01,  5.2577e-02, -6.8057e-09,  1.1977e-08,\n",
      "          -1.0683e-09, -8.7712e-09,  4.9337e-09,  2.4193e-09, -1.1095e-08,\n",
      "           8.6118e-09,  7.2668e-09, -8.7974e-09, -3.7027e-09,  4.4328e-09,\n",
      "           6.3843e-09, -1.6965e-08,  2.8071e-08, -2.8261e-08,  2.2708e-08,\n",
      "          -2.2831e-08,  1.6208e-08, -3.7894e-09, -7.3962e-09,  1.3591e-08,\n",
      "          -1.4951e-08,  1.2072e-08, -5.8378e-09,  8.5046e-09, -1.8455e-08,\n",
      "           2.3140e-08, -2.1376e-08]],\n",
      "\n",
      "        [[ 4.0576e-01,  2.6490e-01, -3.3701e-02, -5.6627e-02, -7.1343e-03,\n",
      "          -5.3342e-09, -1.3616e-08,  7.8134e-09,  3.6308e-09, -1.8966e-08,\n",
      "           1.8565e-08,  1.0396e-08, -1.4172e-08, -7.4030e-09,  3.9058e-09,\n",
      "           1.3680e-08, -2.9535e-08,  4.9003e-08, -5.4303e-08,  5.1243e-08,\n",
      "          -5.2873e-08,  4.2812e-08, -1.7407e-08, -5.4968e-09,  1.4309e-08,\n",
      "          -2.0607e-08,  2.7711e-08, -2.6618e-08,  2.9505e-08, -3.9882e-08,\n",
      "           4.5292e-08, -3.9780e-08]],\n",
      "\n",
      "        [[ 3.5937e-01,  7.2189e-02, -2.2545e-01, -8.6126e-02,  2.5252e-02,\n",
      "           1.1226e-02,  9.3872e-04, -2.7088e-09,  8.4389e-09, -2.8654e-08,\n",
      "           2.2930e-08,  1.3402e-08, -1.6172e-08, -4.5762e-09,  3.3330e-09,\n",
      "           1.4151e-08, -2.8522e-08,  3.4202e-08, -4.0743e-08,  5.4185e-08,\n",
      "          -6.3019e-08,  5.0664e-08, -2.0033e-08, -4.1892e-09, -2.2121e-10,\n",
      "          -1.4917e-09,  1.5929e-08, -2.6575e-08,  3.1847e-08, -3.2186e-08,\n",
      "           2.8775e-08, -2.4398e-08]],\n",
      "\n",
      "        [[ 4.2782e-01,  1.3816e-01, -6.5221e-02,  1.5500e-01,  1.5606e-01,\n",
      "           2.6968e-02, -4.6502e-03, -1.5067e-03, -9.4333e-05, -2.3030e-08,\n",
      "           2.7047e-08, -4.1893e-10, -1.8696e-08,  2.6491e-09,  1.5033e-09,\n",
      "           2.0232e-09, -1.4307e-08,  3.5185e-08, -4.4070e-08,  4.1647e-08,\n",
      "          -4.7400e-08,  6.1205e-08, -5.6016e-08,  3.4510e-08, -8.8922e-09,\n",
      "           5.9724e-09, -2.0993e-09, -1.4700e-08,  2.2738e-08, -3.4784e-08,\n",
      "           4.5789e-08, -2.9622e-08]],\n",
      "\n",
      "        [[ 5.7355e-01,  3.0244e-01,  8.4267e-02,  1.8955e-01,  7.2271e-07,\n",
      "          -1.4422e-01, -7.2802e-02, -1.3106e-02, -7.7266e-04,  8.0246e-06,\n",
      "           4.0852e-07,  1.2543e-09, -1.2378e-08,  8.3537e-09,  7.5473e-09,\n",
      "           3.9631e-08, -3.6466e-08,  5.1941e-08, -6.2802e-08,  3.1208e-08,\n",
      "          -3.0361e-08,  3.0092e-08, -2.5455e-08,  1.9724e-08, -3.2040e-08,\n",
      "           4.3065e-08, -4.7235e-08,  2.0183e-08,  2.0182e-08, -5.3455e-08,\n",
      "           7.0529e-08, -6.1590e-08]],\n",
      "\n",
      "        [[ 5.0014e-01,  1.0705e-01, -1.8648e-01, -1.3038e-01, -2.6791e-01,\n",
      "          -1.7971e-01,  4.9144e-02,  8.3597e-02,  3.3702e-02,  7.2215e-03,\n",
      "           9.8257e-04,  8.5302e-05,  3.5477e-06, -3.8096e-11,  1.2474e-09,\n",
      "          -1.5939e-08, -3.6680e-08,  4.7298e-08, -3.8677e-08,  6.0494e-08,\n",
      "          -6.2315e-08,  3.4082e-08, -5.7439e-10, -2.5901e-10, -1.3935e-08,\n",
      "           2.4592e-08, -2.4157e-08,  2.4779e-08,  9.6801e-09, -3.9766e-08,\n",
      "           4.7229e-08, -5.2801e-08]],\n",
      "\n",
      "        [[ 1.3004e-01, -4.8061e-01, -7.1708e-01, -4.4194e-01, -2.8475e-01,\n",
      "           3.7278e-02,  2.1051e-01,  5.7035e-02, -5.5451e-02, -4.6113e-02,\n",
      "          -1.6562e-02, -3.6093e-03, -5.0025e-04, -4.0571e-05, -1.4995e-06,\n",
      "          -5.2171e-08,  1.2294e-07, -1.2290e-08, -8.8122e-08,  1.0268e-07,\n",
      "          -7.7514e-08,  1.0177e-07, -1.1686e-07,  4.8456e-08,  2.0371e-08,\n",
      "          -1.5587e-08,  7.6660e-08, -1.4321e-07,  5.6414e-08, -1.0833e-08,\n",
      "           2.0165e-08, -2.2032e-08]],\n",
      "\n",
      "        [[ 1.8084e-01, -2.9561e-01, -2.3676e-01,  3.0236e-01,  5.1647e-01,\n",
      "           6.1457e-01,  3.6490e-01, -2.4947e-02, -8.0949e-02,  1.7140e-02,\n",
      "           4.6492e-02,  2.6006e-02,  8.1263e-03,  1.6085e-03,  2.0059e-04,\n",
      "           1.4404e-05,  5.0984e-07, -1.5815e-07,  3.7571e-08,  8.7072e-08,\n",
      "          -7.4346e-08,  1.0180e-07, -1.6602e-07,  1.7060e-07, -1.2008e-07,\n",
      "           2.3809e-08,  3.9904e-08, -1.0140e-07,  1.3585e-07, -4.7373e-08,\n",
      "          -6.6805e-09,  1.6106e-08]]])\n",
      "Gu's Coeffiecient shapes for LegS:\n",
      "torch.Size([8, 1, 32])\n",
      "end of test for HiPPO LegT model\n",
      "\n",
      "Testing BRYANS HiPPO-legs model\n",
      "input:\n",
      "[[ 0.3527]\n",
      " [ 0.6617]\n",
      " [ 0.2434]\n",
      " [ 0.6674]\n",
      " [ 1.2293]\n",
      " [ 0.0964]\n",
      " [-2.2756]\n",
      " [ 0.5618]]\n",
      "input type:\n",
      "<class 'jaxlib.xla_extension.DeviceArray'>\n",
      "Bryan's Coeffiecients for HiPPO-legs\n",
      "c_k output:\n",
      "[[ 2.35132501e-01]\n",
      " [ 2.03631595e-01]\n",
      " [ 5.25767542e-02]\n",
      " [ 5.61039542e-07]\n",
      " [ 9.19736891e-08]\n",
      " [ 1.28763162e-07]\n",
      " [-3.40302648e-07]\n",
      " [ 2.33218998e-07]\n",
      " [-6.30676738e-08]\n",
      " [-1.52413548e-07]\n",
      " [ 3.67894764e-08]\n",
      " [ 1.47157905e-07]\n",
      " [-2.36503777e-08]\n",
      " [-3.36360927e-07]\n",
      " [ 4.04684243e-07]\n",
      " [-2.83804525e-07]\n",
      " [ 2.20736865e-07]\n",
      " [-6.30676738e-08]\n",
      " [-2.10225579e-08]\n",
      " [ 7.88345886e-08]\n",
      " [-8.67180532e-08]\n",
      " [ 8.34332781e-08]\n",
      " [-1.28763162e-07]\n",
      " [ 2.16795129e-07]\n",
      " [-2.91687996e-07]\n",
      " [ 2.49642881e-07]\n",
      " [-1.91173882e-07]\n",
      " [ 1.08397565e-07]\n",
      " [-9.98571466e-08]\n",
      " [ 1.27120785e-07]\n",
      " [-1.55205598e-07]\n",
      " [ 7.65352510e-08]]\n",
      "\n",
      "\n",
      "c_k output:\n",
      "[[ 2.82160282e-01]\n",
      " [ 1.22179233e-01]\n",
      " [-1.12666115e-01]\n",
      " [-7.99841508e-02]\n",
      " [-1.00776199e-02]\n",
      " [-5.66451632e-08]\n",
      " [-4.21418562e-08]\n",
      " [ 2.52777966e-07]\n",
      " [-2.66341289e-07]\n",
      " [ 1.69074241e-07]\n",
      " [ 6.06786230e-08]\n",
      " [-9.90797844e-08]\n",
      " [-9.13666725e-08]\n",
      " [ 2.79172667e-07]\n",
      " [-1.01618667e-07]\n",
      " [-1.40976482e-07]\n",
      " [ 2.37773548e-07]\n",
      " [-3.68103429e-07]\n",
      " [ 2.89102303e-07]\n",
      " [-1.96925839e-07]\n",
      " [ 1.24260168e-07]\n",
      " [-1.06224903e-07]\n",
      " [ 1.10905667e-07]\n",
      " [-1.15968000e-07]\n",
      " [ 9.59680833e-08]\n",
      " [ 2.30678125e-08]\n",
      " [-1.20365115e-07]\n",
      " [ 1.68494566e-07]\n",
      " [-1.35625683e-07]\n",
      " [ 9.10247664e-08]\n",
      " [-1.22831167e-07]\n",
      " [ 2.22648794e-07]]\n",
      "\n",
      "\n",
      "c_k output:\n",
      "[[ 3.02314401e-01]\n",
      " [ 8.72706547e-02]\n",
      " [-9.76443738e-02]\n",
      " [ 4.44362089e-02]\n",
      " [ 8.70294943e-02]\n",
      " [ 2.53198948e-02]\n",
      " [ 2.11739191e-03]\n",
      " [ 5.77824046e-08]\n",
      " [-1.25663931e-07]\n",
      " [ 1.62381468e-07]\n",
      " [-7.63005730e-08]\n",
      " [-1.47418703e-07]\n",
      " [ 2.48284266e-07]\n",
      " [-1.74823676e-08]\n",
      " [-1.74039570e-07]\n",
      " [ 3.92052186e-08]\n",
      " [ 1.50198360e-07]\n",
      " [-2.08257404e-07]\n",
      " [ 4.57562493e-07]\n",
      " [-5.19160267e-07]\n",
      " [ 3.88664660e-07]\n",
      " [-2.53919040e-07]\n",
      " [ 1.82752345e-07]\n",
      " [-2.33717302e-07]\n",
      " [ 1.69175138e-07]\n",
      " [-1.79680654e-07]\n",
      " [ 2.16021903e-07]\n",
      " [-1.44652589e-07]\n",
      " [-4.54902604e-08]\n",
      " [ 1.05130454e-07]\n",
      " [-2.00065244e-08]\n",
      " [-6.20852774e-08]]\n",
      "\n",
      "\n",
      "c_k output:\n",
      "[[ 3.1351113e-01]\n",
      " [ 6.7877285e-02]\n",
      " [-8.0801159e-02]\n",
      " [ 6.3288093e-02]\n",
      " [ 5.0502885e-03]\n",
      " [-7.7257410e-02]\n",
      " [-4.0511940e-02]\n",
      " [-7.4296938e-03]\n",
      " [-4.6530124e-04]\n",
      " [ 1.1032638e-07]\n",
      " [-2.1634170e-07]\n",
      " [ 1.8995715e-07]\n",
      " [ 3.2297791e-09]\n",
      " [-2.0404349e-07]\n",
      " [ 9.0187953e-08]\n",
      " [ 1.5633995e-07]\n",
      " [-1.4471286e-07]\n",
      " [ 2.0240520e-08]\n",
      " [-2.6788385e-08]\n",
      " [-1.6498703e-07]\n",
      " [ 3.6074368e-07]\n",
      " [-4.2342592e-07]\n",
      " [ 3.2973739e-07]\n",
      " [-1.9244519e-07]\n",
      " [ 2.3630096e-07]\n",
      " [-2.1963439e-07]\n",
      " [ 1.7037573e-07]\n",
      " [-2.2923894e-07]\n",
      " [ 2.6731311e-07]\n",
      " [-1.5174652e-07]\n",
      " [-9.7202886e-09]\n",
      " [ 4.7971213e-08]]\n",
      "\n",
      "\n",
      "c_k output:\n",
      "[[ 3.2063642e-01]\n",
      " [ 5.5536103e-02]\n",
      " [-6.8020232e-02]\n",
      " [ 6.3080877e-02]\n",
      " [-2.7131042e-02]\n",
      " [-3.5447530e-02]\n",
      " [ 5.5623513e-02]\n",
      " [ 5.1133685e-02]\n",
      " [ 1.5647512e-02]\n",
      " [ 2.0970560e-03]\n",
      " [ 1.0485568e-04]\n",
      " [ 1.6371699e-07]\n",
      " [-1.7942314e-07]\n",
      " [ 7.4890487e-08]\n",
      " [ 1.6098636e-07]\n",
      " [-4.5106823e-08]\n",
      " [-2.7354457e-07]\n",
      " [ 3.6357918e-07]\n",
      " [-1.6338396e-07]\n",
      " [ 2.1069087e-07]\n",
      " [-1.7711426e-07]\n",
      " [-8.8612289e-08]\n",
      " [ 2.8454110e-07]\n",
      " [-3.0473689e-07]\n",
      " [ 1.6330407e-07]\n",
      " [-2.2340602e-07]\n",
      " [ 3.0745258e-07]\n",
      " [-2.9087445e-07]\n",
      " [ 2.8488910e-07]\n",
      " [-2.6016397e-07]\n",
      " [ 2.8074862e-07]\n",
      " [-1.7985897e-07]]\n",
      "\n",
      "\n",
      "c_k output:\n",
      "[[ 3.2556930e-01]\n",
      " [ 4.6992119e-02]\n",
      " [-5.8460593e-02]\n",
      " [ 5.8730397e-02]\n",
      " [-3.9027900e-02]\n",
      " [-4.3305214e-03]\n",
      " [ 4.7382474e-02]\n",
      " [-2.8278477e-02]\n",
      " [-5.4184068e-02]\n",
      " [-2.5262864e-02]\n",
      " [-5.4727150e-03]\n",
      " [-5.7782955e-04]\n",
      " [-2.4278273e-05]\n",
      " [ 1.8016340e-07]\n",
      " [-1.0477798e-07]\n",
      " [-1.3432211e-07]\n",
      " [ 1.0602794e-07]\n",
      " [ 1.9946637e-07]\n",
      " [-3.5742906e-07]\n",
      " [ 1.7099316e-07]\n",
      " [-1.7620933e-07]\n",
      " [ 2.0112954e-07]\n",
      " [-4.6430706e-08]\n",
      " [-1.6161235e-07]\n",
      " [ 2.9167853e-07]\n",
      " [-1.7794844e-07]\n",
      " [ 1.6061834e-07]\n",
      " [-2.2354645e-07]\n",
      " [ 2.3108218e-07]\n",
      " [-2.3298036e-07]\n",
      " [ 2.1639795e-07]\n",
      " [-2.9351858e-07]]\n",
      "\n",
      "\n",
      "c_k output:\n",
      "[[ 3.2918662e-01]\n",
      " [ 4.0726557e-02]\n",
      " [-5.1150367e-02]\n",
      " [ 5.3765815e-02]\n",
      " [-4.2775478e-02]\n",
      " [ 1.3454878e-02]\n",
      " [ 2.6322272e-02]\n",
      " [-4.3700628e-02]\n",
      " [ 1.5488090e-03]\n",
      " [ 4.8707947e-02]\n",
      " [ 3.4019329e-02]\n",
      " [ 1.0719752e-02]\n",
      " [ 1.7985350e-03]\n",
      " [ 1.5699342e-04]\n",
      " [ 5.4319121e-06]\n",
      " [ 1.2245007e-08]\n",
      " [ 2.2142940e-07]\n",
      " [-8.5655984e-08]\n",
      " [-2.5893840e-07]\n",
      " [ 4.3057059e-07]\n",
      " [-2.6105550e-07]\n",
      " [ 2.1975563e-07]\n",
      " [-2.5383636e-07]\n",
      " [ 1.8829110e-07]\n",
      " [-3.4556685e-08]\n",
      " [-1.8590123e-07]\n",
      " [ 1.5012739e-07]\n",
      " [-9.4105928e-08]\n",
      " [ 1.3416334e-07]\n",
      " [-1.1903447e-07]\n",
      " [ 1.6632670e-07]\n",
      " [-1.6565861e-07]]\n",
      "\n",
      "\n",
      "c_k output:\n",
      "[[ 3.3195287e-01]\n",
      " [ 3.5935208e-02]\n",
      " [-4.5415573e-02]\n",
      " [ 4.9113698e-02]\n",
      " [-4.3090116e-02]\n",
      " [ 2.3122519e-02]\n",
      " [ 8.9668967e-03]\n",
      " [-3.6471982e-02]\n",
      " [ 2.9256858e-02]\n",
      " [ 1.9245761e-02]\n",
      " [-3.5868701e-02]\n",
      " [-3.9571851e-02]\n",
      " [-1.7332541e-02]\n",
      " [-4.1356706e-03]\n",
      " [-5.6679006e-04]\n",
      " [-4.1885967e-05]\n",
      " [-1.3329640e-06]\n",
      " [-2.1494191e-07]\n",
      " [ 1.1477607e-07]\n",
      " [ 2.1837840e-07]\n",
      " [-4.3472130e-07]\n",
      " [ 2.9209701e-07]\n",
      " [-2.1250510e-07]\n",
      " [ 2.5229761e-07]\n",
      " [-2.2969299e-07]\n",
      " [ 1.2242144e-07]\n",
      " [ 1.1991739e-07]\n",
      " [-1.3936344e-07]\n",
      " [ 8.2968782e-08]\n",
      " [-1.1718603e-07]\n",
      " [ 7.3367858e-08]\n",
      " [-1.2799617e-07]]\n",
      "\n",
      "\n",
      "end of test for HiPPO-legs model\n"
     ]
    }
   ],
   "source": [
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('s4mer-pkg-jZnBSgjq-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a81e05d1d7f7eae781698b7c1b81c0d771335201ebad1d81045cb177cef974b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
