{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiPPO Matrices\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GpuDevice(id=0, process_index=0)]\n",
      "The Device: gpu\n"
     ]
    }
   ],
   "source": [
    "## import packages\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import linen as jnn\n",
    "\n",
    "from jax.nn.initializers import lecun_normal, uniform\n",
    "from jax.numpy.linalg import eig, inv, matrix_power\n",
    "from jax.scipy.signal import convolve\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from scipy import linalg as la\n",
    "from scipy import signal\n",
    "from scipy import special as ss\n",
    "\n",
    "import math\n",
    "\n",
    "print(jax.devices())\n",
    "print(f\"The Device: {jax.lib.xla_bridge.get_backend().platform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS enabled: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(f\"MPS enabled: {torch.backends.mps.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 5\n",
    "rng, key2, key3, key4, key5 = jax.random.split(key, num=num_copies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_SSM(rng, N):\n",
    "    a_r, b_r, c_r = jax.random.split(rng, 3)\n",
    "    A = jax.random.uniform(a_r, (N, N))\n",
    "    B = jax.random.uniform(b_r, (N, 1))\n",
    "    C = jax.random.uniform(c_r, (1, N))\n",
    "\n",
    "    return A, B, C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate The HiPPO Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Legendre (LegT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Legendre (LegT) - vectorized\n",
    "def build_LegT_V(N, lambda_n=1):\n",
    "    \"\"\"\n",
    "    The, vectorized implementation of the, measure derived from the translated Legendre basis.\n",
    "\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "        legt_type (str): Choice between the two different tilts of basis.\n",
    "            - legt: translated Legendre - 'legt'\n",
    "            - lmu: Legendre Memory Unit - 'lmu'\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    q = jnp.arange(N, dtype=jnp.float64)\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    case = jnp.power(-1.0, (n - k))\n",
    "    A = None\n",
    "    B = None\n",
    "\n",
    "    if lambda_n == 1:\n",
    "        A_base = -jnp.sqrt(2 * n + 1) * jnp.sqrt(2 * k + 1)\n",
    "        pre_D = jnp.sqrt(jnp.diag(2 * q + 1))\n",
    "        B = D = jnp.diag(pre_D)[:, None]\n",
    "        A = jnp.where(\n",
    "            k <= n, A_base, A_base * case\n",
    "        )  # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "    elif lambda_n == 2:  # (jnp.sqrt(2*n+1) * jnp.power(-1, n)):\n",
    "        A_base = -(2 * n + 1)\n",
    "        B = jnp.diag((2 * q + 1) * jnp.power(-1, n))[:, None]\n",
    "        A = jnp.where(\n",
    "            k <= n, A_base * case, A_base\n",
    "        )  # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Legendre (LegT) - non-vectorized\n",
    "def build_LegT(N, legt_type=\"legt\"):\n",
    "    \"\"\"\n",
    "    The, non-vectorized implementation of the, measure derived from the translated Legendre basis\n",
    "\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "        legt_type (str): Choice between the two different tilts of basis.\n",
    "            - legt: translated Legendre - 'legt'\n",
    "            - lmu: Legendre Memory Unit - 'lmu'\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    Q = jnp.arange(N, dtype=jnp.float64)\n",
    "    pre_R = 2 * Q + 1\n",
    "    k, n = jnp.meshgrid(Q, Q)\n",
    "\n",
    "    if legt_type == \"legt\":\n",
    "        R = jnp.sqrt(pre_R)\n",
    "        A = R[:, None] * jnp.where(n < k, (-1.0) ** (n - k), 1) * R[None, :]\n",
    "        B = R[:, None]\n",
    "        A = -A\n",
    "\n",
    "        # Halve again for timescale correctness\n",
    "        # A, B = A/2, B/2\n",
    "        # A *= 0.5\n",
    "        # B *= 0.5\n",
    "\n",
    "    elif legt_type == \"lmu\":\n",
    "        R = pre_R[:, None]\n",
    "        A = jnp.where(n < k, -1, (-1.0) ** (n - k + 1)) * R\n",
    "        B = (-1.0) ** Q[:, None] * R\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65736/1287737312.py:17: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in arange is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  Q = jnp.arange(N, dtype=jnp.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A nv:\n",
      " [[ -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.]\n",
      " [ -5.   5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.]\n",
      " [  7.  -7.   7.  -7.  -7.  -7.  -7.  -7.  -7.  -7.]\n",
      " [ -9.   9.  -9.   9.  -9.  -9.  -9.  -9.  -9.  -9.]\n",
      " [ 11. -11.  11. -11.  11. -11. -11. -11. -11. -11.]\n",
      " [-13.  13. -13.  13. -13.  13. -13. -13. -13. -13.]\n",
      " [ 15. -15.  15. -15.  15. -15.  15. -15. -15. -15.]\n",
      " [-17.  17. -17.  17. -17.  17. -17.  17. -17. -17.]\n",
      " [ 19. -19.  19. -19.  19. -19.  19. -19.  19. -19.]]\n",
      "A v:\n",
      " [[ -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.]\n",
      " [ -5.   5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.  -5.]\n",
      " [  7.  -7.   7.  -7.  -7.  -7.  -7.  -7.  -7.  -7.]\n",
      " [ -9.   9.  -9.   9.  -9.  -9.  -9.  -9.  -9.  -9.]\n",
      " [ 11. -11.  11. -11.  11. -11. -11. -11. -11. -11.]\n",
      " [-13.  13. -13.  13. -13.  13. -13. -13. -13. -13.]\n",
      " [ 15. -15.  15. -15.  15. -15.  15. -15. -15. -15.]\n",
      " [-17.  17. -17.  17. -17.  17. -17.  17. -17. -17.]\n",
      " [ 19. -19.  19. -19.  19. -19.  19. -19.  19. -19.]]\n",
      "B nv:\n",
      " [[  1.]\n",
      " [ -3.]\n",
      " [  5.]\n",
      " [ -7.]\n",
      " [  9.]\n",
      " [-11.]\n",
      " [ 13.]\n",
      " [-15.]\n",
      " [ 17.]\n",
      " [-19.]]\n",
      "B v:\n",
      " [[  1.]\n",
      " [ -3.]\n",
      " [  5.]\n",
      " [ -7.]\n",
      " [  9.]\n",
      " [-11.]\n",
      " [ 13.]\n",
      " [-15.]\n",
      " [ 17.]\n",
      " [-19.]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_LegT_A, nv_LegT_B = build_LegT(N=N, legt_type=\"lmu\")\n",
    "LegT_A, LegT_B = build_LegT_V(N=N, lambda_n=2)\n",
    "print(f\"A nv:\\n\", nv_LegT_A)\n",
    "print(f\"A v:\\n\", LegT_A)\n",
    "print(f\"B nv:\\n\", nv_LegT_B)\n",
    "print(f\"B v:\\n\", LegT_B)\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_LegT_A, LegT_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_LegT_B, LegT_B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Laguerre (LagT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Laguerre (LagT) - non-vectorized\n",
    "def build_LagT_V(alpha, beta, N):\n",
    "    \"\"\"\n",
    "    The, vectorized implementation of the, measure derived from the translated Laguerre basis.\n",
    "\n",
    "    Args:\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    L = jnp.exp(\n",
    "        0.5 * (ss.gammaln(jnp.arange(N) + alpha + 1) - ss.gammaln(jnp.arange(N) + 1))\n",
    "    )\n",
    "    inv_L = 1.0 / L[:, None]\n",
    "    pre_A = (jnp.eye(N) * ((1 + beta) / 2)) + jnp.tril(jnp.ones((N, N)), -1)\n",
    "    pre_B = ss.binom(alpha + jnp.arange(N), jnp.arange(N))[:, None]\n",
    "\n",
    "    A = -inv_L * pre_A * L[None, :]\n",
    "    B = (\n",
    "        jnp.exp(-0.5 * ss.gammaln(1 - alpha))\n",
    "        * jnp.power(beta, (1 - alpha) / 2)\n",
    "        * inv_L\n",
    "        * pre_B\n",
    "    )\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Laguerre (LagT) - non-vectorized\n",
    "def build_LagT(alpha, beta, N):\n",
    "    \"\"\"\n",
    "    The, non-vectorized implementation of the, measure derived from the translated Laguerre basis.\n",
    "\n",
    "    Args:\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    A = -jnp.eye(N) * (1 + beta) / 2 - jnp.tril(jnp.ones((N, N)), -1)\n",
    "    B = ss.binom(alpha + jnp.arange(N), jnp.arange(N))[:, None]\n",
    "\n",
    "    L = jnp.exp(\n",
    "        0.5 * (ss.gammaln(jnp.arange(N) + alpha + 1) - ss.gammaln(jnp.arange(N) + 1))\n",
    "    )\n",
    "    A = (1.0 / L[:, None]) * A * L[None, :]\n",
    "    B = (\n",
    "        (1.0 / L[:, None])\n",
    "        * B\n",
    "        * jnp.exp(-0.5 * ss.gammaln(1 - alpha))\n",
    "        * beta ** ((1 - alpha) / 2)\n",
    "    )\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A nv:\n",
      "[[-1. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      " [-1. -1. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      " [-1. -1. -1. -0. -0. -0. -0. -0. -0. -0.]\n",
      " [-1. -1. -1. -1. -0. -0. -0. -0. -0. -0.]\n",
      " [-1. -1. -1. -1. -1. -0. -0. -0. -0. -0.]\n",
      " [-1. -1. -1. -1. -1. -1. -0. -0. -0. -0.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -0. -0. -0.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -0. -0.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -0.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]]\n",
      "A v:\n",
      "[[-1. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      " [-1. -1. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      " [-1. -1. -1. -0. -0. -0. -0. -0. -0. -0.]\n",
      " [-1. -1. -1. -1. -0. -0. -0. -0. -0. -0.]\n",
      " [-1. -1. -1. -1. -1. -0. -0. -0. -0. -0.]\n",
      " [-1. -1. -1. -1. -1. -1. -0. -0. -0. -0.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -0. -0. -0.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -0. -0.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -0.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]]\n",
      "B nv:\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "B v:\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_LagT_A, nv_LagT_B = build_LagT(alpha=0, beta=1, N=N)\n",
    "LagT_A, LagT_B = build_LagT_V(alpha=0, beta=1, N=N)\n",
    "print(f\"A nv:\\n{nv_LagT_A}\")\n",
    "print(f\"A v:\\n{LagT_A}\")\n",
    "print(f\"B nv:\\n{nv_LagT_B}\")\n",
    "print(f\"B v:\\n{LagT_B}\")\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_LagT_A, LagT_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_LagT_B, LagT_B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Legendre (LegS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Legendre (LegS) vectorized\n",
    "def build_LegS_V(N):\n",
    "    \"\"\"\n",
    "    The, vectorized implementation of the, measure derived from the Scaled Legendre basis.\n",
    "\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    q = jnp.arange(N, dtype=jnp.float64)\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    pre_D = jnp.sqrt(jnp.diag(2 * q + 1))\n",
    "    B = D = jnp.diag(pre_D)[:, None]\n",
    "\n",
    "    A_base = (-jnp.sqrt(2 * n + 1)) * jnp.sqrt(2 * k + 1)\n",
    "    case_2 = (n + 1) / (2 * n + 1)\n",
    "\n",
    "    A = jnp.where(n > k, A_base, 0.0)  # if n > k, then A_base is used, otherwise 0\n",
    "    A = jnp.where(\n",
    "        n == k, (A_base * case_2), A\n",
    "    )  # if n == k, then A_base is used, otherwise A\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Legendre (LegS), non-vectorized\n",
    "def build_LegS(N):\n",
    "    \"\"\"\n",
    "    The, non-vectorized implementation of the, measure derived from the Scaled Legendre basis.\n",
    "\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    q = jnp.arange(\n",
    "        N, dtype=jnp.float64\n",
    "    )  # q represents the values 1, 2, ..., N each column has\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    r = 2 * q + 1\n",
    "    M = -(jnp.where(n >= k, r, 0) - jnp.diag(q))  # represents the state matrix M\n",
    "    D = jnp.sqrt(\n",
    "        jnp.diag(2 * q + 1)\n",
    "    )  # represents the diagonal matrix D $D := \\text{diag}[(2n+1)^{\\frac{1}{2}}]^{N-1}_{n=0}$\n",
    "    A = D @ M @ jnp.linalg.inv(D)\n",
    "    B = jnp.diag(D)[:, None]\n",
    "    B = (\n",
    "        B.copy()\n",
    "    )  # Otherwise \"UserWarning: given NumPY array is not writeable...\" after torch.as_tensor(B)\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65736/3992732761.py:14: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in arange is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  q = jnp.arange(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A nv:\n",
      "[[ -1.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -1.7320508  -1.9999999   0.          0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -2.2360678  -3.872983   -3.          0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -2.6457512  -4.582576   -5.91608    -4.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -3.         -5.196152   -6.7082047  -7.9372544  -5.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -3.3166246  -5.744562   -7.4161987  -8.774965   -9.949874   -6.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -3.6055512  -6.244998   -8.062259   -9.539392  -10.816654  -11.958261\n",
      "   -7.          0.          0.          0.       ]\n",
      " [ -3.8729832  -6.708204   -8.6602545 -10.246951  -11.61895   -12.845232\n",
      "  -13.964239   -7.9999995   0.          0.       ]\n",
      " [ -4.1231055  -7.141428   -9.219545  -10.908712  -12.369316  -13.674794\n",
      "  -14.866069  -15.9687195  -9.          0.       ]\n",
      " [ -4.3588986  -7.5498343  -9.746795  -11.532562  -13.076696  -14.456831\n",
      "  -15.716233  -16.88194   -17.9722    -10.       ]]\n",
      "A v:\n",
      "[[ -1.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -1.7320508  -2.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -2.2360678  -3.872983   -2.9999995   0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -4.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -5.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874   -5.9999995\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261\n",
      "   -7.0000005   0.          0.          0.       ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232\n",
      "  -13.964239   -8.          0.          0.       ]\n",
      " [ -4.1231055  -7.141428   -9.219543  -10.908711  -12.369316  -13.674793\n",
      "  -14.866068  -15.968719   -9.          0.       ]\n",
      " [ -4.3588986  -7.549834   -9.746793  -11.532561  -13.076696  -14.456831\n",
      "  -15.716232  -16.88194   -17.972198   -9.999999 ]]\n",
      "B nv:\n",
      "[[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]\n",
      " [4.1231055]\n",
      " [4.3588986]]\n",
      "B v:\n",
      "[[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]\n",
      " [4.1231055]\n",
      " [4.3588986]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_LegS_A, nv_LegS_B = build_LegS(N=N)\n",
    "LegS_A, LegS_B = build_LegS_V(N=N)\n",
    "print(f\"A nv:\\n{nv_LegS_A}\")\n",
    "print(f\"A v:\\n{LegS_A}\")\n",
    "print(f\"B nv:\\n{nv_LegS_B}\")\n",
    "print(f\"B v:\\n{LegS_B}\")\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_LegS_A, LegS_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_LegS_B, LegS_B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fourier Basis OPs and functions - vectorized\n",
    "def build_Fourier_V(N, fourier_type=\"fru\"):\n",
    "    \"\"\"\n",
    "    Vectorized measure implementations derived from fourier basis.\n",
    "\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "        fourier_type (str): The type of Fourier measure.\n",
    "            - FRU: Fourier Recurrent Unit - fru\n",
    "            - FouT: truncated Fourier - fout\n",
    "            - fouD: decayed Fourier - foud\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    A = jnp.diag(\n",
    "        jnp.stack([jnp.zeros(N // 2), jnp.zeros(N // 2)], axis=-1).reshape(-1)[1:], 1\n",
    "    )\n",
    "    B = jnp.zeros(A.shape[1], dtype=jnp.float64)\n",
    "\n",
    "    B = B.at[0::2].set(jnp.sqrt(2))\n",
    "    B = B.at[0].set(1)\n",
    "    \n",
    "    q = jnp.arange(A.shape[1], dtype=jnp.float64)\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "\n",
    "    n_odd = n % 2 == 0\n",
    "    k_odd = k % 2 == 0\n",
    "\n",
    "    case_1 = (n == k) & (n == 0)\n",
    "    case_2_3 = ((k == 0) & (n_odd)) | ((n == 0) & (k_odd))\n",
    "    case_4 = (n_odd) & (k_odd)\n",
    "    case_5 = (n - k == 1) & (k_odd)\n",
    "    case_6 = (k - n == 1) & (n_odd)\n",
    "\n",
    "    if fourier_type == \"fru\":  # Fourier Recurrent Unit (FRU) - vectorized\n",
    "        A = jnp.where(\n",
    "            case_1,\n",
    "            -1.0,\n",
    "            jnp.where(\n",
    "                case_2_3,\n",
    "                -jnp.sqrt(2),\n",
    "                jnp.where(\n",
    "                    case_4,\n",
    "                    -2,\n",
    "                    jnp.where(\n",
    "                        case_5,\n",
    "                        jnp.pi * (n // 2),\n",
    "                        jnp.where(case_6, -jnp.pi * (k // 2), 0.0),\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    elif fourier_type == \"fout\":  # truncated Fourier (FouT) - vectorized\n",
    "        A = jnp.where(\n",
    "            case_1,\n",
    "            -1.0,\n",
    "            jnp.where(\n",
    "                case_2_3,\n",
    "                -jnp.sqrt(2),\n",
    "                jnp.where(\n",
    "                    case_4,\n",
    "                    -2,\n",
    "                    jnp.where(\n",
    "                        case_5,\n",
    "                        jnp.pi * (n // 2),\n",
    "                        jnp.where(case_6, -jnp.pi * (k // 2), 0.0),\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        A = 2 * A\n",
    "        B = 2 * B\n",
    "\n",
    "    elif fourier_type == \"foud\":\n",
    "        A = jnp.where(\n",
    "            case_1,\n",
    "            -1.0,\n",
    "            jnp.where(\n",
    "                case_2_3,\n",
    "                -jnp.sqrt(2),\n",
    "                jnp.where(\n",
    "                    case_4,\n",
    "                    -2,\n",
    "                    jnp.where(\n",
    "                        case_5,\n",
    "                        2 * jnp.pi * (n // 2),\n",
    "                        jnp.where(case_6, 2 * -jnp.pi * (k // 2), 0.0),\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        A = 0.5 * A\n",
    "        B = 0.5 * B\n",
    "\n",
    "    B = B[:, None]\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Fourier(N, fourier_type=\"fru\"):\n",
    "    \"\"\"\n",
    "    Non-vectorized measure implementations derived from fourier basis.\n",
    "\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "        fourier_type (str): The type of Fourier measure.\n",
    "            - FRU: Fourier Recurrent Unit - fru\n",
    "            - FouT: truncated Fourier - fout\n",
    "            - fouD: decayed Fourier - foud\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    freqs = jnp.arange(N // 2)\n",
    "\n",
    "    if fourier_type == \"fru\":  # Fourier Recurrent Unit (FRU) - non-vectorized\n",
    "        d = jnp.stack([jnp.zeros(N // 2), freqs], axis=-1).reshape(-1)[1:]\n",
    "        A = jnp.pi * (-jnp.diag(d, 1) + jnp.diag(d, -1))\n",
    "\n",
    "        B = jnp.zeros(A.shape[1])\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        A = A - B[:, None] * B[None, :]\n",
    "        B = B[:, None]\n",
    "\n",
    "    elif fourier_type == \"fout\":  # truncated Fourier (FouT) - non-vectorized\n",
    "        freqs *= 2\n",
    "        d = jnp.stack([jnp.zeros(N // 2), freqs], axis=-1).reshape(-1)[1:]\n",
    "        A = jnp.pi * (-jnp.diag(d, 1) + jnp.diag(d, -1))\n",
    "\n",
    "        B = jnp.zeros(A.shape[1])\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        # Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case\n",
    "        A = A - B[:, None] * B[None, :] * 2\n",
    "        B = B[:, None] * 2\n",
    "\n",
    "    elif fourier_type == \"foud\":\n",
    "        d = jnp.stack([jnp.zeros(N // 2), freqs], axis=-1).reshape(-1)[1:]\n",
    "        A = jnp.pi * (-jnp.diag(d, 1) + jnp.diag(d, -1))\n",
    "\n",
    "        B = jnp.zeros(A.shape[1])\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        # Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case\n",
    "        A = A - 0.5 * B[:, None] * B[None, :]\n",
    "        B = 0.5 * B[:, None]\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65736/4044268312.py:21: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  B = jnp.zeros(A.shape[1], dtype=jnp.float64)\n",
      "/tmp/ipykernel_65736/4044268312.py:26: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in arange is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  q = jnp.arange(A.shape[1], dtype=jnp.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv for A:\n",
      "[[ -1.          0.         -1.4142135   0.         -1.4142135   0.\n",
      "   -1.4142135   0.         -1.4142135   0.       ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -1.4142135   0.         -1.9999999  -3.1415927  -1.9999999   0.\n",
      "   -1.9999999   0.         -1.9999999   0.       ]\n",
      " [  0.          0.          3.1415927   0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -1.4142135   0.         -1.9999999   0.         -1.9999999  -6.2831855\n",
      "   -1.9999999   0.         -1.9999999   0.       ]\n",
      " [  0.          0.          0.          0.          6.2831855   0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -1.4142135   0.         -1.9999999   0.         -1.9999999   0.\n",
      "   -1.9999999  -9.424778   -1.9999999   0.       ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "    9.424778    0.          0.          0.       ]\n",
      " [ -1.4142135   0.         -1.9999999   0.         -1.9999999   0.\n",
      "   -1.9999999   0.         -1.9999999 -12.566371 ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.         12.566371    0.       ]]\n",
      "v for A:\n",
      "[[ -1.         -0.         -1.4142135   0.         -1.4142135   0.\n",
      "   -1.4142135   0.         -1.4142135   0.       ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -1.4142135   0.         -2.         -3.1415927  -2.          0.\n",
      "   -2.          0.         -2.          0.       ]\n",
      " [  0.          0.          3.1415927   0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -1.4142135   0.         -2.          0.         -2.         -6.2831855\n",
      "   -2.          0.         -2.          0.       ]\n",
      " [  0.          0.          0.          0.          6.2831855   0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -1.4142135   0.         -2.          0.         -2.          0.\n",
      "   -2.         -9.424778   -2.          0.       ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "    9.424778    0.          0.          0.       ]\n",
      " [ -1.4142135   0.         -2.          0.         -2.          0.\n",
      "   -2.          0.         -2.        -12.566371 ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.         12.566371    0.       ]]\n",
      "nv for B:\n",
      "[[1.       ]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]]\n",
      "v for B:\n",
      "[[1.       ]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]]\n",
      "A Comparison:\n",
      " True\n",
      "B Comparison:\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "nv_Fourier_A, nv_Fourier_B = build_Fourier(N=N, fourier_type=\"fru\")\n",
    "Fourier_A, Fourier_B = build_Fourier_V(N=N, fourier_type=\"fru\")\n",
    "print(f\"nv for A:\\n{nv_Fourier_A}\")\n",
    "print(f\"v for A:\\n{Fourier_A}\")\n",
    "print(f\"nv for B:\\n{nv_Fourier_B}\")\n",
    "print(f\"v for B:\\n{Fourier_B}\")\n",
    "print(f\"A Comparison:\\n {jnp.allclose(nv_Fourier_A, Fourier_A)}\")\n",
    "print(f\"B Comparison:\\n {jnp.allclose(nv_Fourier_B, Fourier_B)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_HiPPO(\n",
    "    N, v=\"nv\", measure=\"legs\", lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Instantiates the HiPPO matrix of a given order using a particular measure.\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "        v (str): choose between this repo's implementation or hazy research's implementation.\n",
    "        measure (str):\n",
    "            choose between\n",
    "                - HiPPO w/ Translated Legendre (LegT) - legt\n",
    "                - HiPPO w/ Translated Laguerre (LagT) - lagt\n",
    "                - HiPPO w/ Scaled Legendre (LegS) - legs\n",
    "                - HiPPO w/ Fourier basis - fourier\n",
    "                    - FRU: Fourier Recurrent Unit\n",
    "                    - FouT: Translated Fourier\n",
    "        lambda_n (int): The amount of tilt applied to the HiPPO-LegS basis, determines between LegS and LMU.\n",
    "        fourier_type (str): chooses between the following:\n",
    "            - FRU: Fourier Recurrent Unit - fru\n",
    "            - FouT: Translated Fourier - fout\n",
    "            - FourD: Fourier Decay - fourd\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The HiPPO matrix multiplied by -1.\n",
    "        B (jnp.ndarray): The other corresponding state space matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    A = None\n",
    "    B = None\n",
    "    if measure == \"legt\":\n",
    "        if v == \"nv\":\n",
    "            A, B = build_LegT(N=N, lambda_n=lambda_n)\n",
    "        else:\n",
    "            A, B = build_LegT_V(N=N, lambda_n=lambda_n)\n",
    "\n",
    "    elif measure == \"lagt\":\n",
    "        if v == \"nv\":\n",
    "            A, B = build_LagT(alpha=alpha, beta=beta, N=N)\n",
    "        else:\n",
    "            A, B = build_LagT_V(alpha=alpha, beta=beta, N=N)\n",
    "\n",
    "    elif measure == \"legs\":\n",
    "        if v == \"nv\":\n",
    "            A, B = build_LegS(N=N)\n",
    "        else:\n",
    "            A, B = build_LegS_V(N=N)\n",
    "\n",
    "    elif measure == \"fourier\":\n",
    "        if v == \"nv\":\n",
    "            A, B = build_Fourier(N=N, fourier_type=fourier_type)\n",
    "        else:\n",
    "            A, B = build_Fourier_V(N=N, fourier_type=fourier_type)\n",
    "\n",
    "    elif measure == \"random\":\n",
    "        A = jnp.random.randn(N, N) / N\n",
    "        B = jnp.random.randn(N, 1)\n",
    "\n",
    "    elif measure == \"diagonal\":\n",
    "        A = -jnp.diag(jnp.exp(jnp.random.randn(N)))\n",
    "        B = jnp.random.randn(N, 1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid HiPPO type\")\n",
    "\n",
    "    A_copy = A.copy()\n",
    "    B_copy = B.copy()\n",
    "\n",
    "    return jnp.array(A_copy), B_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      " [[ -1.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -1.7320508  -1.9999999   0.          0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -2.2360678  -3.872983   -3.          0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -2.6457512  -4.582576   -5.91608    -4.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -3.         -5.196152   -6.7082047  -7.9372544  -5.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -3.3166246  -5.744562   -7.4161987  -8.774965   -9.949874   -6.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -3.6055512  -6.244998   -8.062259   -9.539392  -10.816654  -11.958261\n",
      "   -7.          0.          0.          0.       ]\n",
      " [ -3.8729832  -6.708204   -8.6602545 -10.246951  -11.61895   -12.845232\n",
      "  -13.964239   -7.9999995   0.          0.       ]\n",
      " [ -4.1231055  -7.141428   -9.219545  -10.908712  -12.369316  -13.674794\n",
      "  -14.866069  -15.9687195  -9.          0.       ]\n",
      " [ -4.3588986  -7.5498343  -9.746795  -11.532562  -13.076696  -14.456831\n",
      "  -15.716233  -16.88194   -17.9722    -10.       ]]\n",
      "v:\n",
      " [[ -1.          0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -1.7320508  -2.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -2.2360678  -3.872983   -2.9999995   0.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -4.          0.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -5.          0.\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874   -5.9999995\n",
      "    0.          0.          0.          0.       ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261\n",
      "   -7.0000005   0.          0.          0.       ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232\n",
      "  -13.964239   -8.          0.          0.       ]\n",
      " [ -4.1231055  -7.141428   -9.219543  -10.908711  -12.369316  -13.674793\n",
      "  -14.866068  -15.968719   -9.          0.       ]\n",
      " [ -4.3588986  -7.549834   -9.746793  -11.532561  -13.076696  -14.456831\n",
      "  -15.716232  -16.88194   -17.972198   -9.999999 ]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65736/3992732761.py:14: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in arange is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  q = jnp.arange(\n"
     ]
    }
   ],
   "source": [
    "nv_A, nv_B = make_HiPPO(\n",
    "    N=N, v=\"nv\", measure=\"legs\", lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1\n",
    ")\n",
    "v_A, v_B = make_HiPPO(\n",
    "    N=N, v=\"v\", measure=\"legs\", lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1\n",
    ")\n",
    "print(f\"nv:\\n\", nv_A)\n",
    "print(f\"v:\\n\", v_A)\n",
    "# print(f\"nv:\\n\", nv_B)\n",
    "# print(f\"v:\\n\", v_B)\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_A, v_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_B, v_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Old utilities for parallel scan implementation of Linear RNNs. \"\"\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "### Utilities\n",
    "\n",
    "\n",
    "def shift_up(a, s=None, drop=True, dim=0):\n",
    "    assert dim == 0\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(a[0, ...])\n",
    "    s = s.unsqueeze(dim)\n",
    "    if drop:\n",
    "        a = a[:-1, ...]\n",
    "    return torch.cat((s, a), dim=dim)\n",
    "\n",
    "\n",
    "def interleave(a, b, uneven=False, dim=0):\n",
    "    \"\"\"Interleave two tensors of same shape\"\"\"\n",
    "    # assert(a.shape == b.shape)\n",
    "    assert dim == 0  # TODO temporary to make handling uneven case easier\n",
    "    if dim < 0:\n",
    "        dim = N + dim\n",
    "    if uneven:\n",
    "        a_ = a[-1:, ...]\n",
    "        a = a[:-1, ...]\n",
    "    c = torch.stack((a, b), dim + 1)\n",
    "    out_shape = list(a.shape)\n",
    "    out_shape[dim] *= 2\n",
    "    c = c.view(out_shape)\n",
    "    if uneven:\n",
    "        c = torch.cat((c, a_), dim=dim)\n",
    "    return c\n",
    "\n",
    "\n",
    "def batch_mult(A, u, has_batch=None):\n",
    "    \"\"\"Matrix mult A @ u with special case to save memory if u has additional batch dim\n",
    "\n",
    "    The batch dimension is assumed to be the second dimension\n",
    "    A : (L, ..., N, N)\n",
    "    u : (L, [B], ..., N)\n",
    "    has_batch: True, False, or None. If None, determined automatically\n",
    "\n",
    "    Output:\n",
    "    x : (L, [B], ..., N)\n",
    "      A @ u broadcasted appropriately\n",
    "    \"\"\"\n",
    "\n",
    "    if has_batch is None:\n",
    "        has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    if has_batch:\n",
    "        u = u.permute([0] + list(range(2, len(u.shape))) + [1])\n",
    "    else:\n",
    "        u = u.unsqueeze(-1)\n",
    "    v = A @ u\n",
    "    if has_batch:\n",
    "        v = v.permute([0] + [len(u.shape) - 1] + list(range(1, len(u.shape) - 1)))\n",
    "    else:\n",
    "        v = v[..., 0]\n",
    "    return v\n",
    "\n",
    "\n",
    "### Main unrolling functions\n",
    "\n",
    "\n",
    "def unroll(A, u):\n",
    "    \"\"\"\n",
    "    A : (..., N, N) # TODO I think this can't take batch dimension?\n",
    "    u : (L, ..., N)\n",
    "    output : x (..., N) # TODO a lot of these shapes are wrong\n",
    "    x[i, ...] = A^{i} @ u[0, ...] + ... + A @ u[i-1, ...] + u[i, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    m = u.new_zeros(u.shape[1:])\n",
    "    outputs = []\n",
    "    for u_ in torch.unbind(u, dim=0):\n",
    "        m = F.linear(m, A) + u_\n",
    "        outputs.append(m)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive(A, u):\n",
    "    \"\"\"Bottom-up divide-and-conquer version of unroll.\"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_(A, u):\n",
    "        if u.shape[0] == 1:\n",
    "            return u\n",
    "\n",
    "        u_evens = u[0::2, ...]\n",
    "        u_odds = u[1::2, ...]\n",
    "\n",
    "        # u2 = F.linear(u_evens, A) + u_odds\n",
    "        u2 = (A @ u_evens.unsqueeze(-1)).squeeze(-1) + u_odds\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_odds = parallel_unroll_recursive_(A2, u2)\n",
    "        # x_evens = F.linear(shift_up(x_odds), A) + u_evens\n",
    "        x_evens = (A @ shift_up(x_odds).unsqueeze(-1)).squeeze(-1) + u_evens\n",
    "\n",
    "        x = interleave(x_evens, x_odds, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n) / math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N - u.shape[0],) + u.shape[1:])), dim=0)\n",
    "\n",
    "    return parallel_unroll_recursive_(A, u)[:n, ...]\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive_br(A, u):\n",
    "    \"\"\"Same as parallel_unroll_recursive but uses bit reversal for locality.\"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_br_(A, u):\n",
    "        n = u.shape[0]\n",
    "        if n == 1:\n",
    "            return u\n",
    "\n",
    "        m = n // 2\n",
    "        u_0 = u[:m, ...]\n",
    "        u_1 = u[m:, ...]\n",
    "\n",
    "        u2 = F.linear(u_0, A) + u_1\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_1 = parallel_unroll_recursive_br_(A2, u2)\n",
    "        x_0 = F.linear(shift_up(x_1), A) + u_0\n",
    "\n",
    "        # x = torch.cat((x_0, x_1), dim=0) # is there a way to do this with cat?\n",
    "        x = interleave(x_0, x_1, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n) / math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N - u.shape[0],) + u.shape[1:])), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    x = parallel_unroll_recursive_br_(A, u)\n",
    "    return x[:n, ...]\n",
    "\n",
    "\n",
    "def parallel_unroll_iterative(A, u):\n",
    "    \"\"\"Bottom-up divide-and-conquer version of unroll, implemented iteratively\"\"\"\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n) / math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N - u.shape[0],) + u.shape[1:])), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    # Main recursive loop, flattened\n",
    "    us = []  # stores the u_0 terms in the recursive version\n",
    "    N_ = N\n",
    "    As = []  # stores the A matrices\n",
    "    for l in range(m):\n",
    "        N_ = N_ // 2\n",
    "        As.append(A)\n",
    "        u_0 = u[:N_, ...]\n",
    "        us.append(u_0)\n",
    "        u = F.linear(u_0, A) + u[N_:, ...]\n",
    "        A = A @ A\n",
    "    x_0 = []\n",
    "    x = u  # x_1\n",
    "    for l in range(m - 1, -1, -1):\n",
    "        x_0 = F.linear(shift_up(x), As[l]) + us[l]\n",
    "        x = interleave(x_0, x, dim=0)\n",
    "\n",
    "    return x[:n, ...]\n",
    "\n",
    "\n",
    "def variable_unroll_sequential(A, u, s=None, variable=True):\n",
    "    \"\"\"Unroll with variable (in time/length) transitions A.\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] @ s + A[i..1] @ u[0] + ... + A[i] @ u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        s = batch_mult(A_.unsqueeze(0), s.unsqueeze(0), has_batch)[0]\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def variable_unroll(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    \"\"\"Bottom-up divide-and-conquer version of variable_unroll.\"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        return variable_unroll_sequential(A, u, s, variable)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1 = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1 = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = A_1 @ A_0_\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll(A_10, u_10, s, variable, recurse_limit)\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "    x = interleave(\n",
    "        x_0, x_1, uneven, dim=0\n",
    "    )  # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "\n",
    "def variable_unroll_general_sequential(A, u, s, op, variable=True):\n",
    "    \"\"\"Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        s = op(A_, s)\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def variable_unroll_matrix_sequential(A, u, s=None, variable=True):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    return variable_unroll_general_sequential(A, u, s, op, variable=True)\n",
    "\n",
    "\n",
    "def variable_unroll_toeplitz_sequential(A, u, s=None, variable=True, pad=False):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        ret = variable_unroll_general_sequential(\n",
    "            A, u, s, triangular_toeplitz_multiply_padded, variable=True\n",
    "        )\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    return variable_unroll_general_sequential(\n",
    "        A, u, s, triangular_toeplitz_multiply, variable=True\n",
    "    )\n",
    "\n",
    "\n",
    "### General parallel scan functions with generic binary composition operators\n",
    "\n",
    "\n",
    "def variable_unroll_general(\n",
    "    A, u, s, op, compose_op=None, sequential_op=None, variable=True, recurse_limit=16\n",
    "):\n",
    "    \"\"\"Bottom-up divide-and-conquer version of variable_unroll.\n",
    "\n",
    "    compose is an optional function that defines how to compose A without multiplying by a leaf u\n",
    "    \"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        if sequential_op is None:\n",
    "            sequential_op = op\n",
    "        return variable_unroll_general_sequential(A, u, s, sequential_op, variable)\n",
    "\n",
    "    if compose_op is None:\n",
    "        compose_op = op\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1 = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1 = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = op(A_1, u_0_)  # batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = compose_op(A_1, A_0_)\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll_general(\n",
    "        A_10,\n",
    "        u_10,\n",
    "        s,\n",
    "        op,\n",
    "        compose_op,\n",
    "        sequential_op,\n",
    "        variable=variable,\n",
    "        recurse_limit=recurse_limit,\n",
    "    )\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = op(A_0, x_0)  # batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "    x = interleave(\n",
    "        x_0, x_1, uneven, dim=0\n",
    "    )  # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "\n",
    "def variable_unroll_matrix(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "    op = lambda x, y: batch_mult(x, y, has_batch)\n",
    "    sequential_op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[\n",
    "        0\n",
    "    ]\n",
    "    matmul = lambda x, y: x @ y\n",
    "    return variable_unroll_general(\n",
    "        A,\n",
    "        u,\n",
    "        s,\n",
    "        op,\n",
    "        compose_op=matmul,\n",
    "        sequential_op=sequential_op,\n",
    "        variable=variable,\n",
    "        recurse_limit=recurse_limit,\n",
    "    )\n",
    "\n",
    "\n",
    "def variable_unroll_toeplitz(A, u, s=None, variable=True, recurse_limit=8, pad=False):\n",
    "    \"\"\"Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (L, [B], ..., N) same shape as u\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "    # Add the batch dimension to A if necessary\n",
    "    A_batch_dims = len(A.shape) - int(variable)\n",
    "    u_batch_dims = len(u.shape) - 1\n",
    "    if u_batch_dims > A_batch_dims:\n",
    "        # assert u_batch_dims == A_batch_dims + 1\n",
    "        if variable:\n",
    "            while len(A.shape) < len(u.shape):\n",
    "                A = A.unsqueeze(1)\n",
    "        # else:\n",
    "        #     A = A.unsqueeze(0)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        op = triangular_toeplitz_multiply_padded()\n",
    "        ret = variable_unroll_general(\n",
    "            A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit\n",
    "        )\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    op = triangular_toeplitz_multiply\n",
    "    ret = variable_unroll_general(\n",
    "        A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit\n",
    "    )\n",
    "    return ret\n",
    "\n",
    "\n",
    "### Testing\n",
    "\n",
    "\n",
    "def test_correctness():\n",
    "    print(\"Testing Correctness\\n====================\")\n",
    "\n",
    "    # Test sequential unroll\n",
    "    L = 3\n",
    "    A = torch.Tensor([[1, 1], [1, 0]])\n",
    "    u = torch.ones((L, 2))\n",
    "    x = unroll(A, u)\n",
    "    assert torch.isclose(x, torch.Tensor([[1.0, 1.0], [3.0, 2.0], [6.0, 4.0]])).all()\n",
    "\n",
    "    # Test utilities\n",
    "    assert torch.isclose(\n",
    "        shift_up(x), torch.Tensor([[0.0, 0.0], [1.0, 1.0], [3.0, 2.0]])\n",
    "    ).all()\n",
    "    assert torch.isclose(\n",
    "        interleave(x, x),\n",
    "        torch.Tensor(\n",
    "            [[1.0, 1.0], [1.0, 1.0], [3.0, 2.0], [3.0, 2.0], [6.0, 4.0], [6.0, 4.0]]\n",
    "        ),\n",
    "    ).all()\n",
    "\n",
    "    # Test parallel unroll\n",
    "    x = parallel_unroll_recursive(A, u)\n",
    "    assert torch.isclose(x, torch.Tensor([[1.0, 1.0], [3.0, 2.0], [6.0, 4.0]])).all()\n",
    "\n",
    "    # Powers\n",
    "    L = 12\n",
    "    A = torch.Tensor([[1, 0, 0], [2, 1, 0], [3, 3, 1]])\n",
    "    u = torch.ones((L, 3))\n",
    "    x = parallel_unroll_recursive(A, u)\n",
    "    print(\"recursive\", x)\n",
    "    x = parallel_unroll_recursive_br(A, u)\n",
    "    print(\"recursive_br\", x)\n",
    "    x = parallel_unroll_iterative(A, u)\n",
    "    print(\"iterative_br\", x)\n",
    "\n",
    "    A = A.repeat((L, 1, 1))\n",
    "    s = torch.zeros(3)\n",
    "    print(\"A shape\", A.shape)\n",
    "    x = variable_unroll_sequential(A, u, s)\n",
    "    print(\"variable_unroll\", x)\n",
    "    x = variable_unroll(A, u, s)\n",
    "    print(\"parallel_variable_unroll\", x)\n",
    "\n",
    "\n",
    "def generate_data(L, N, B=None, cuda=True):\n",
    "    A = torch.eye(N) + torch.normal(0, 1, size=(N, N)) / (N**0.5) / L\n",
    "    u = torch.normal(0, 1, size=(L, B, N))\n",
    "\n",
    "    # device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    device = torch.device(\"cuda:0\") if cuda else torch.device(\"cpu\")\n",
    "    A = A.to(device)\n",
    "    u = u.to(device)\n",
    "    return A, u\n",
    "\n",
    "\n",
    "def test_stability():\n",
    "    print(\"Testing Stability\\n====================\")\n",
    "    L = 256\n",
    "    N = L // 2\n",
    "    B = 100\n",
    "    A, u = generate_data(L, N, B)\n",
    "\n",
    "    x = unroll(A, u)\n",
    "    x1 = parallel_unroll_recursive(A, u)\n",
    "    x2 = parallel_unroll_recursive_br(A, u)\n",
    "    x3 = parallel_unroll_iterative(A, u)\n",
    "    print(\"norm error\", torch.norm(x - x1))\n",
    "    print(\"norm error\", torch.norm(x - x2))\n",
    "    print(\"norm error\", torch.norm(x - x3))\n",
    "\n",
    "    print(\"max error\", torch.max(torch.abs(x - x1)))\n",
    "    print(\"max error\", torch.max(torch.abs(x - x2)))\n",
    "    print(\"max error\", torch.max(torch.abs(x - x3)))\n",
    "\n",
    "    A = A.repeat((L, 1, 1))\n",
    "    x = variable_unroll_sequential(A, u)\n",
    "    x_ = variable_unroll(A, u)\n",
    "    # x_ = variable_unroll_matrix_sequential(A, u)\n",
    "    x_ = variable_unroll_matrix(A, u)\n",
    "    print(x - x_)\n",
    "    abserr = torch.abs(x - x_)\n",
    "    relerr = abserr / (torch.abs(x) + 1e-8)\n",
    "    print(\"norm abs error\", torch.norm(abserr))\n",
    "    print(\"max abs error\", torch.max(abserr))\n",
    "    print(\"norm rel error\", torch.norm(relerr))\n",
    "    print(\"max rel error\", torch.max(relerr))\n",
    "\n",
    "\n",
    "def test_toeplitz():\n",
    "    from model.toeplitz import construct_toeplitz\n",
    "\n",
    "    def summarize(name, x, x_, showdiff=False):\n",
    "        print(name, \"stats\")\n",
    "        if showdiff:\n",
    "            print(x - x_)\n",
    "        abserr = torch.abs(x - x_)\n",
    "        relerr = abserr / (torch.abs(x) + 1e-8)\n",
    "        print(\"  norm abs error\", torch.norm(abserr))\n",
    "        print(\"  max abs error\", torch.max(abserr))\n",
    "        print(\"  norm rel error\", torch.norm(relerr))\n",
    "        print(\"  max rel error\", torch.max(relerr))\n",
    "\n",
    "    print(\"Testing Toeplitz\\n====================\")\n",
    "    L = 512\n",
    "    N = L // 2\n",
    "    B = 100\n",
    "    A, u = generate_data(L, N, B)\n",
    "\n",
    "    A = A[..., 0]\n",
    "    A = construct_toeplitz(A)\n",
    "\n",
    "    # print(\"SHAPES\", A.shape, u.shape)\n",
    "\n",
    "    # Static A\n",
    "    x = unroll(A, u)\n",
    "    x_ = variable_unroll(A, u, variable=False)\n",
    "    summarize(\"nonvariable matrix original\", x, x_, showdiff=False)\n",
    "    x_ = variable_unroll_matrix(A, u, variable=False)\n",
    "    summarize(\"nonvariable matrix general\", x, x_, showdiff=False)\n",
    "    x_ = variable_unroll_toeplitz(A[..., 0], u, variable=False)\n",
    "    summarize(\"nonvariable toeplitz\", x, x_, showdiff=False)\n",
    "\n",
    "    # Sequential\n",
    "    A = A.repeat((L, 1, 1))\n",
    "    for _ in range(1):\n",
    "        x_ = variable_unroll_sequential(A, u)\n",
    "        summarize(\"variable unroll sequential\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_matrix_sequential(A, u)\n",
    "        summarize(\"variable matrix sequential\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_toeplitz_sequential(A[..., 0], u, pad=True)\n",
    "        summarize(\"variable toeplitz sequential\", x, x_, showdiff=False)\n",
    "\n",
    "    # Parallel\n",
    "    for _ in range(1):\n",
    "        x_ = variable_unroll(A, u)\n",
    "        summarize(\"variable matrix original\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_matrix(A, u)\n",
    "        summarize(\"variable matrix general\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_toeplitz(A[..., 0], u, pad=True, recurse_limit=8)\n",
    "        summarize(\"variable toeplitz\", x, x_, showdiff=False)\n",
    "\n",
    "\n",
    "def test_speed(variable=False, it=1):\n",
    "    print(\"Testing Speed\\n====================\")\n",
    "    N = 256\n",
    "    L = 1024\n",
    "    B = 100\n",
    "    A, u = generate_data(L, N, B)\n",
    "    As = A.repeat((L, 1, 1))\n",
    "\n",
    "    u.requires_grad = True\n",
    "    As.requires_grad = True\n",
    "    for _ in range(it):\n",
    "        x = unroll(A, u)\n",
    "        x = torch.sum(x)\n",
    "        x.backward()\n",
    "\n",
    "        x = parallel_unroll_recursive(A, u)\n",
    "        x = torch.sum(x)\n",
    "        x.backward()\n",
    "\n",
    "        # parallel_unroll_recursive_br(A, u)\n",
    "        # parallel_unroll_iterative(A, u)\n",
    "\n",
    "    for _ in range(it):\n",
    "        if variable:\n",
    "            x = variable_unroll_sequential(As, u, variable=True, recurse_limit=16)\n",
    "            x = torch.sum(x)\n",
    "            x.backward()\n",
    "            x = variable_unroll(As, u, variable=True, recurse_limit=16)\n",
    "            x = torch.sum(x)\n",
    "            x.backward()\n",
    "        else:\n",
    "            variable_unroll_sequential(A, u, variable=False, recurse_limit=16)\n",
    "            variable_unroll(A, u, variable=False, recurse_limit=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LegT(nn.Module):\n",
    "    def __init__(self, N, dt=1.0, discretization=\"bilinear\"):\n",
    "        \"\"\"\n",
    "        N: the order of the HiPPO projection\n",
    "        dt: discretization step size - should be roughly inverse to the length of the sequence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        # A, B = transition('lmu', N)\n",
    "        A, B = make_HiPPO(\n",
    "            N=self.N,\n",
    "            v=\"v\",\n",
    "            measure=\"legt\",\n",
    "            lambda_n=1,\n",
    "            fourier_type=\"fru\",\n",
    "            alpha=0,\n",
    "            beta=1,\n",
    "        )\n",
    "        C = np.ones((1, N))\n",
    "        D = np.zeros((1,))\n",
    "        # dt, discretization options\n",
    "        A, B, _, _, _ = signal.cont2discrete((A, B, C, D), dt=dt, method=discretization)\n",
    "\n",
    "        B = B.squeeze(-1)\n",
    "\n",
    "        self.register_buffer(\"A\", torch.Tensor(A))  # (N, N)\n",
    "        self.register_buffer(\"B\", torch.Tensor(B))  # (N,)\n",
    "\n",
    "        # vals = np.linspace(0.0, 1.0, 1./dt)\n",
    "        vals = np.arange(0.0, 1.0, dt)\n",
    "        self.eval_matrix = torch.Tensor(\n",
    "            ss.eval_legendre(np.arange(N)[:, None], 1 - 2 * vals).T\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = inputs * self.B  # (length, ..., N)\n",
    "\n",
    "        c = torch.zeros(u.shape[1:])\n",
    "        cs = []\n",
    "        for f in inputs:\n",
    "            c = F.linear(c, self.A) + self.B * f\n",
    "            cs.append(c)\n",
    "        return torch.stack(cs, dim=0)\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        return (self.eval_matrix @ c.unsqueeze(-1)).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LegS(nn.Module):\n",
    "    \"\"\"Vanilla HiPPO-LegS model (scale invariant instead of time invariant)\"\"\"\n",
    "\n",
    "    def __init__(self, N, max_length=1024, measure=\"legs\", discretization=\"bilinear\"):\n",
    "        \"\"\"\n",
    "        max_length: maximum sequence length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        A, B = make_HiPPO(\n",
    "            N=self.N,\n",
    "            v=\"v\",\n",
    "            measure=measure,\n",
    "            lambda_n=1,\n",
    "            fourier_type=\"fru\",\n",
    "            alpha=0,\n",
    "            beta=1,\n",
    "        )\n",
    "        # A, B = transition(measure, N)\n",
    "        B = B.squeeze(-1)\n",
    "        A_stacked = np.empty((max_length, N, N), dtype=A.dtype)\n",
    "        B_stacked = np.empty((max_length, N), dtype=B.dtype)\n",
    "        for t in range(1, max_length + 1):\n",
    "            At = A / t\n",
    "            Bt = B / t\n",
    "            if discretization == \"forward\":\n",
    "                A_stacked[t - 1] = np.eye(N) + At\n",
    "                B_stacked[t - 1] = Bt\n",
    "            elif discretization == \"backward\":\n",
    "                A_stacked[t - 1] = la.solve_triangular(\n",
    "                    np.eye(N) - At, np.eye(N), lower=True\n",
    "                )\n",
    "                B_stacked[t - 1] = la.solve_triangular(np.eye(N) - At, Bt, lower=True)\n",
    "            elif discretization == \"bilinear\":\n",
    "                A_stacked[t - 1] = np.linalg.lstsq(\n",
    "                    np.eye(N) - At / 2, np.eye(N) + At / 2, rcond=None\n",
    "                )[\n",
    "                    0\n",
    "                ]  # TODO: Referencing this: https://stackoverflow.com/questions/64527098/numpy-linalg-linalgerror-singular-matrix-error-when-trying-to-solve\n",
    "                B_stacked[t - 1] = np.linalg.lstsq(np.eye(N) - At / 2, Bt, rcond=None)[\n",
    "                    0\n",
    "                ]\n",
    "            else:  # ZOH\n",
    "                A_stacked[t - 1] = la.expm(A * (math.log(t + 1) - math.log(t)))\n",
    "                B_stacked[t - 1] = la.solve_triangular(\n",
    "                    A, A_stacked[t - 1] @ B - B, lower=True\n",
    "                )\n",
    "        self.A_stacked = torch.Tensor(A_stacked.copy())  # (max_length, N, N)\n",
    "        self.B_stacked = torch.Tensor(B_stacked.copy())  # (max_length, N)\n",
    "        vals = np.linspace(0.0, 1.0, max_length)\n",
    "        self.eval_matrix = torch.from_numpy(\n",
    "            np.asarray(\n",
    "                ((B[:, None] * ss.eval_legendre(np.arange(N)[:, None], 2 * vals - 1)).T)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, fast=False):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "        result = None\n",
    "\n",
    "        L = inputs.shape[0]\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = torch.transpose(inputs, 0, -2)\n",
    "        u = u * self.B_stacked[:L]\n",
    "        u = torch.transpose(u, 0, -2)  # (length, ..., N)\n",
    "\n",
    "        if fast:\n",
    "            result = variable_unroll_matrix(self.A_stacked[:L], u)\n",
    "\n",
    "        else:\n",
    "            result = variable_unroll_matrix_sequential(self.A_stacked[:L], u)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        a = self.eval_matrix @ c.unsqueeze(-1)\n",
    "        return a.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO(jnn.Module):\n",
    "    \"\"\"\n",
    "    class that constructs HiPPO model using the defined measure.\n",
    "\n",
    "    Args:\n",
    "        N (int): order of the HiPPO projection, aka the number of coefficients to describe the matrix\n",
    "        max_length (int): maximum sequence length to be input\n",
    "        measure (str): the measure used to define which way to instantiate the HiPPO matrix\n",
    "        step (float): step size used for descretization\n",
    "        GBT_alpha (float): represents which descretization transformation to use based off the alpha value\n",
    "        seq_L (int): length of the sequence to be used for training\n",
    "        v (str): choice of vectorized or non-vectorized function instantiation\n",
    "            - 'v': vectorized\n",
    "            - 'nv': non-vectorized\n",
    "        lambda_n (float): value associated with the tilt of legt\n",
    "            - 1: tilt on legt\n",
    "            - \\sqrt(2n+1)(-1)^{N}: tilt associated with the legendre memory unit (LMU)\n",
    "        fourier_type (str): choice of fourier measures\n",
    "            - fru: fourier recurrent unit measure (FRU) - 'fru'\n",
    "            - fout: truncated Fourier (FouT) - 'fout'\n",
    "            - fourd: decaying fourier transform - 'fourd'\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "    \"\"\"\n",
    "\n",
    "    N: int\n",
    "    max_length: int\n",
    "    measure: str\n",
    "    step: float\n",
    "    GBT_alpha: float\n",
    "    seq_L: int\n",
    "    v: str\n",
    "    lambda_n: float\n",
    "    fourier_type: str\n",
    "    alpha: float\n",
    "    beta: float\n",
    "\n",
    "    def setup(self):\n",
    "        A, B = make_HiPPO(\n",
    "            N=self.N,\n",
    "            v=self.v,\n",
    "            measure=self.measure,\n",
    "            lambda_n=self.lambda_n,\n",
    "            fourier_type=self.fourier_type,\n",
    "            alpha=self.alpha,\n",
    "            beta=self.beta,\n",
    "        )\n",
    "\n",
    "        self.A = A\n",
    "        self.B = B  # .squeeze(-1)\n",
    "        self.C = jnp.ones((1, self.N)).squeeze(0)\n",
    "        self.D = jnp.zeros((1,))\n",
    "\n",
    "        if self.measure == \"legt\":\n",
    "            L = self.seq_L\n",
    "            vals = jnp.arange(0.0, 1.0, L)\n",
    "            #n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 1 - 2 * vals\n",
    "            self.eval_matrix = jax.scipy.special.lpmn_values(\n",
    "                m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "            ).T  # ss.eval_legendre(n, x).T\n",
    "\n",
    "        elif self.measure == \"legs\":\n",
    "            L = self.max_length\n",
    "            vals = jnp.linspace(0.0, 1.0, L)\n",
    "            #n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 2 * vals - 1\n",
    "            self.eval_matrix = (\n",
    "                B[:, None]\n",
    "                * jax.scipy.special.lpmn_values(\n",
    "                    m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "                )\n",
    "            ).T  # ss.eval_legendre(n, x)).T\n",
    "        else:\n",
    "            raise ValueError(\"invalid measure\")\n",
    "\n",
    "    def __call__(self, u, init_state=None, kernel=False):\n",
    "        print(f\"u shape:\\n{u.shape}\")\n",
    "        print(f\"u:\\n{u}\")\n",
    "        if not kernel:\n",
    "            if init_state is None:\n",
    "                init_state = jnp.zeros((self.N,))\n",
    "\n",
    "            Ab, Bb, Cb, Db = self.collect_SSM_vars(\n",
    "                self.A, self.B, self.C, self.D, u, alpha=self.GBT_alpha\n",
    "            )\n",
    "            c_k = self.scan_SSM(Ab, Bb, Cb, Db, u, x0=init_state)[1]\n",
    "        else:\n",
    "            Ab, Bb, Cb, Db = self.discretize(\n",
    "                self.A, self.B, self.C, self.D, step=self.step, alpha=self.GBT_alpha\n",
    "            )\n",
    "            c_k = self.causal_convolution(\n",
    "                u, self.K_conv(Ab, Bb, Cb, Db, L=self.max_length)\n",
    "            )\n",
    "\n",
    "        return c_k\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        \"\"\"\n",
    "        Uses coeffecients to reconstruct the signal\n",
    "\n",
    "        Args:\n",
    "            c (jnp.ndarray): coefficients of the HiPPO projection\n",
    "\n",
    "        Returns:\n",
    "            reconstructed signal\n",
    "        \"\"\"\n",
    "        return (self.eval_matrix @ jnp.expand_dims(c, -1)).squeeze(-1)\n",
    "\n",
    "    def discretize(self, A, B, C, D, step, alpha=0.5):\n",
    "        \"\"\"\n",
    "        function used for discretizing the HiPPO matrix\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            step (float): step size used for discretization\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "                - forward Euler corresponds to  = 0,\n",
    "                - backward Euler corresponds to  = 1,\n",
    "                - bilinear corresponds to  = 0.5,\n",
    "                - Zero-order Hold corresponds to  > 1\n",
    "        \"\"\"\n",
    "        I = jnp.eye(A.shape[0])\n",
    "        GBT = jnp.linalg.inv(I - (step * alpha * A))\n",
    "        GBT_A = GBT @ (I + (step * (1 - alpha) * A))\n",
    "        GBT_B = (step * GBT) @ B\n",
    "\n",
    "        if alpha > 1:  # Zero-order Hold\n",
    "            GBT_A = jax.scipy.linalg.expm(step * A)\n",
    "            GBT_B = (jnp.linalg.inv(A) @ (jax.scipy.linalg.expm(step * A) - I)) @ B\n",
    "\n",
    "        return GBT_A, GBT_B, C, D\n",
    "\n",
    "    def collect_SSM_vars(self, A, B, C, D, u, alpha=0.5):\n",
    "        \"\"\"\n",
    "        turns the continuos HiPPO matrix components into discrete ones\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            u (jnp.ndarray): input signal\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "\n",
    "        Returns:\n",
    "            Ab (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Bb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Cb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Db (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "        \"\"\"\n",
    "        L = u.shape[0]\n",
    "        N = A.shape[0]\n",
    "        assert (\n",
    "            L == self.seq_L\n",
    "        ), f\"sequence length must match, currently {L} != {self.seq_L}\"\n",
    "        assert N == self.N, f\"Order number must match, currently {N} != {self.N}\"\n",
    "\n",
    "        Ab, Bb, Cb, Db = self.discretize(A, B, C, D, step=1.0 / L, alpha=alpha)\n",
    "\n",
    "        return Ab, Bb, Cb, Db\n",
    "\n",
    "    def scan_SSM(self, Ab, Bb, Cb, Db, u, x0):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            u (jnp.ndarray): the input sequence\n",
    "            x0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "\n",
    "        def step(x_k_1, u_k):\n",
    "            \"\"\"\n",
    "            Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "            Args:\n",
    "                x_k_1: previous hidden state\n",
    "                u_k: output from function f at, descritized, time step, k.\n",
    "\n",
    "            Returns:\n",
    "                x_k: current hidden state\n",
    "                y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "            \"\"\"\n",
    "\n",
    "            x_k = (Ab @ x_k_1) + (Bb @ u_k)\n",
    "            y_k = (Cb @ x_k) + (Db @ u_k)\n",
    "\n",
    "            return x_k, y_k\n",
    "\n",
    "        return jax.lax.scan(step, x0, u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # N = 256\n",
    "    # L = 128\n",
    "\n",
    "    N = 8\n",
    "    L = 8\n",
    "\n",
    "    x = torch.randn(L, 1)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegT model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"\\nTesting HiPPO LegT model\")\n",
    "    hippo_legt = HiPPO_LegT(N, dt=1.0 / L)\n",
    "\n",
    "    y = hippo_legt(x)\n",
    "    \n",
    "    print(f\"h-y for LegT:\\n{y}\")\n",
    "    print(f\"h-y shape for LegT:\\n{y.shape}\")\n",
    "    \n",
    "    z = hippo_legt.reconstruct(y)\n",
    "    print(f\"h-z reconstruction for LegT:\\n{z}\")\n",
    "    print(f\"h-z reconstruction shape for LegT:\\n{z.shape}\")\n",
    "\n",
    "    mse = loss(z[-1, 0, :L], x.squeeze(-1))\n",
    "    print(f\"h-MSE shape:\\n{mse}\")\n",
    "    print(f\"end of test for HiPPO LegT model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegS model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"\\nTesting HiPPO LegS model\")\n",
    "    hippo_legs = HiPPO_LegS(N, max_length=L)  # The Gu's\n",
    "\n",
    "    y = hippo_legs(x)\n",
    "\n",
    "    print(f\"h-y for LegS:\\n{y}\")\n",
    "    print(f\"h-y shape for LegS:\\n{y.shape}\")\n",
    "\n",
    "    z = hippo_legs(x, fast=True)\n",
    "    \n",
    "    print(f\"h-z reconstruction for LegS:\\n{hippo_legs.reconstruct(z)}\")\n",
    "    print(f\"h-z reconstruction shape for LegS:\\n{hippo_legs.reconstruct(z).shape}\")\n",
    "\n",
    "    # print(y-z)\n",
    "    print(f\"end of test for HiPPO LegT model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test Generic HiPPO model --------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    the_measure = \"legt\"\n",
    "    print(f\"\\nTesting BRYANS HiPPO-{the_measure} model\")\n",
    "    \n",
    "    hippo_LegS_B = HiPPO(\n",
    "        N=N,\n",
    "        max_length=L,\n",
    "        measure=the_measure,\n",
    "        step=1.0 / L,\n",
    "        GBT_alpha=0.5,\n",
    "        seq_L=L,\n",
    "        v=\"v\",\n",
    "        lambda_n=1.0,\n",
    "        fourier_type=\"fru\",\n",
    "        alpha=0.0,\n",
    "        beta=1.0,\n",
    "    )  # Bryan's\n",
    "\n",
    "    x = jnp.asarray(x)  # convert torch array to jax array\n",
    "    print(f\"input:\\n{x}\")\n",
    "    print(f\"input type:\\n{type(x)}\")\n",
    "\n",
    "    params = hippo_LegS_B.init(key2, x)\n",
    "\n",
    "    c_legs = hippo_LegS_B.apply(params, x)\n",
    "\n",
    "    y_legs = hippo_LegS_B.apply(\n",
    "        {\"params\": params}, c_legs, method=hippo_LegS_B.reconstruct\n",
    "    )\n",
    "    \n",
    "    print(f\"U-c for HiPPO-{the_measure}:\\n{c_legs}\")\n",
    "    print(f\"U-c shape for HiPPO-{the_measure}:\\n{c_legs.shape}\")\n",
    "\n",
    "    print(f\"U-z reconstruction for HiPPO-{the_measure}:\\n{y_legs}\")\n",
    "    print(f\"U-z reconstruction shape for HiPPO-{the_measure}:\\n{y_legs.shape}\")\n",
    "\n",
    "    print(f\"end of test for HiPPO-{the_measure} model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing HiPPO LegT model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65736/2341795512.py:17: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in arange is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  q = jnp.arange(N, dtype=jnp.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h-y for LegT:\n",
      "tensor([[[ 0.0332,  0.0503,  0.0499,  0.0399,  0.0273,  0.0160,  0.0088,\n",
      "           0.0033]],\n",
      "\n",
      "        [[-0.1274, -0.2070, -0.2333, -0.2174, -0.1765, -0.1210, -0.0792,\n",
      "          -0.0339]],\n",
      "\n",
      "        [[-0.1568, -0.1996, -0.1021,  0.0420,  0.1735,  0.2078,  0.2116,\n",
      "           0.1190]],\n",
      "\n",
      "        [[-0.2438, -0.2512, -0.0785,  0.0967,  0.0676, -0.0235, -0.1912,\n",
      "          -0.1867]],\n",
      "\n",
      "        [[-0.1092,  0.0310,  0.3291,  0.3425,  0.1787, -0.0217,  0.0099,\n",
      "           0.1320]],\n",
      "\n",
      "        [[ 0.0685,  0.3793,  0.5278,  0.2409, -0.1336, -0.2052, -0.0263,\n",
      "          -0.0537]],\n",
      "\n",
      "        [[-0.0841,  0.1120, -0.0514, -0.5198, -0.6339, -0.2042, -0.0155,\n",
      "           0.0203]],\n",
      "\n",
      "        [[-0.2915, -0.2243, -0.4608, -0.6547, -0.0180,  0.4374,  0.3761,\n",
      "           0.1401]]])\n",
      "h-y shape for LegT:\n",
      "torch.Size([8, 1, 8])\n",
      "h-z reconstruction for LegT:\n",
      "tensor([[[ 0.2287,  0.0664,  0.0318,  0.0211,  0.0157,  0.0138,  0.0121,\n",
      "           0.0102]],\n",
      "\n",
      "        [[-1.1956, -0.2120, -0.0996, -0.0727, -0.0522, -0.0520, -0.0460,\n",
      "          -0.0351]],\n",
      "\n",
      "        [[ 0.2955, -0.5553, -0.1987, -0.1095, -0.1067, -0.0561, -0.0528,\n",
      "          -0.0688]],\n",
      "\n",
      "        [[-0.8106, -0.4198, -0.5270, -0.2569, -0.1194, -0.1549, -0.1036,\n",
      "          -0.0617]],\n",
      "\n",
      "        [[ 0.8922, -0.0577, -0.3056, -0.3661, -0.2098, -0.0628, -0.0919,\n",
      "          -0.0651]],\n",
      "\n",
      "        [[ 0.7978,  0.6589,  0.0865, -0.2084, -0.2373, -0.1268, -0.0212,\n",
      "          -0.0508]],\n",
      "\n",
      "        [[-1.3765,  0.3294,  0.3701, -0.0361, -0.2913, -0.2911, -0.1691,\n",
      "          -0.0804]],\n",
      "\n",
      "        [[-0.6956, -0.8584,  0.1377,  0.1752, -0.1854, -0.3713, -0.3520,\n",
      "          -0.2402]]])\n",
      "h-z reconstruction shape for LegT:\n",
      "torch.Size([8, 1, 8])\n",
      "h-MSE shape:\n",
      "1.2593638896942139\n",
      "end of test for HiPPO LegT model\n",
      "\n",
      "Testing HiPPO LegS model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65736/4201749953.py:14: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in arange is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  q = jnp.arange(N, dtype=jnp.float64)\n",
      "/tmp/ipykernel_65736/996672122.py:51: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:172.)\n",
      "  self.eval_matrix = torch.from_numpy(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h-y for LegS:\n",
      "tensor([[[ 1.7690e-01,  1.5320e-01,  3.9555e-02,  9.6225e-09, -3.2373e-09,\n",
      "           1.8350e-09, -6.0070e-09,  2.8978e-09]],\n",
      "\n",
      "        [[-4.0724e-01, -6.2344e-01, -4.8056e-01, -1.7725e-01, -2.2332e-02,\n",
      "          -1.6578e-08,  2.2107e-08, -5.1380e-09]],\n",
      "\n",
      "        [[-3.6014e-01, -2.5053e-01,  2.8613e-01,  5.5485e-01,  3.3400e-01,\n",
      "           8.2115e-02,  6.8668e-03, -1.1355e-08]],\n",
      "\n",
      "        [[-4.2847e-01, -2.4500e-01,  1.9341e-01,  1.7247e-02, -4.4776e-01,\n",
      "          -4.4620e-01, -1.7392e-01, -2.9807e-02]],\n",
      "\n",
      "        [[-1.6626e-01,  2.1513e-01,  5.6406e-01,  1.9261e-01, -6.4280e-02,\n",
      "           2.9494e-01,  4.9161e-01,  2.8010e-01]],\n",
      "\n",
      "        [[ 8.9288e-02,  5.3306e-01,  6.0239e-01, -4.2196e-02, -2.4451e-01,\n",
      "           4.1342e-02, -1.4095e-01, -4.6467e-01]],\n",
      "\n",
      "        [[-8.7374e-02,  1.3206e-01, -8.7063e-02, -7.2568e-01, -5.4939e-01,\n",
      "          -4.3873e-02, -1.0809e-01, -3.1308e-02]],\n",
      "\n",
      "        [[-2.8817e-01, -2.0644e-01, -4.2251e-01, -6.9264e-01, -2.9486e-02,\n",
      "           5.1444e-01,  2.6872e-01,  2.1626e-01]]])\n",
      "h-y shape for LegS:\n",
      "torch.Size([8, 1, 8])\n",
      "h-z reconstruction for LegS:\n",
      "tensor([[[-8.1589e-08,  1.0830e-02,  4.3322e-02,  9.7474e-02,  1.7329e-01,\n",
      "           2.7076e-01,  3.8990e-01,  5.3069e-01]],\n",
      "\n",
      "        [[ 3.3340e-07,  3.0538e-02,  9.8714e-02,  1.3421e-01,  1.9837e-02,\n",
      "          -4.0848e-01, -1.3617e+00, -3.0976e+00]],\n",
      "\n",
      "        [[-3.0247e-07,  5.5411e-02,  1.1612e-01, -6.4454e-02, -6.6392e-01,\n",
      "          -1.4409e+00, -1.2451e+00,  2.6128e+00]],\n",
      "\n",
      "        [[ 7.6955e-03,  8.2563e-02,  4.5373e-02, -5.1814e-01, -1.3175e+00,\n",
      "          -1.0490e+00,  5.7830e-01, -3.9404e+00]],\n",
      "\n",
      "        [[-2.7055e-01,  2.2320e-02, -7.1568e-02, -1.0528e+00, -1.2413e+00,\n",
      "           3.6469e-01, -1.4008e-01,  5.6200e+00]],\n",
      "\n",
      "        [[ 1.0455e+00,  3.8679e-01, -6.2674e-01, -1.3204e+00, -1.6410e-02,\n",
      "           1.2098e-01,  1.2669e+00, -6.5634e-01]],\n",
      "\n",
      "        [[-3.6193e-01,  7.1313e-02, -8.0801e-01, -8.4421e-01,  7.8226e-03,\n",
      "           1.0205e+00,  1.1011e+00, -4.2780e+00]],\n",
      "\n",
      "        [[-7.0617e-01, -1.3679e-01, -8.5540e-01, -5.7886e-01,  4.7463e-01,\n",
      "           1.3491e+00, -1.2028e+00,  1.1765e-03]]])\n",
      "h-z reconstruction shape for LegS:\n",
      "torch.Size([8, 1, 8])\n",
      "end of test for HiPPO LegT model\n",
      "\n",
      "Testing BRYANS HiPPO-legt model\n",
      "input:\n",
      "[[ 0.26534566]\n",
      " [-1.2834541 ]\n",
      " [-0.24238579]\n",
      " [-0.6676317 ]\n",
      " [ 1.013689  ]\n",
      " [ 1.4948095 ]\n",
      " [-1.2356801 ]\n",
      " [-1.794154  ]]\n",
      "input type:\n",
      "<class 'jaxlib.xla_extension.DeviceArray'>\n",
      "u shape:\n",
      "(8, 1)\n",
      "u:\n",
      "[[ 0.26534566]\n",
      " [-1.2834541 ]\n",
      " [-0.24238579]\n",
      " [-0.6676317 ]\n",
      " [ 1.013689  ]\n",
      " [ 1.4948095 ]\n",
      " [-1.2356801 ]\n",
      " [-1.794154  ]]\n",
      "u shape:\n",
      "(8, 1)\n",
      "u:\n",
      "[[ 0.26534566]\n",
      " [-1.2834541 ]\n",
      " [-0.24238579]\n",
      " [-0.6676317 ]\n",
      " [ 1.013689  ]\n",
      " [ 1.4948095 ]\n",
      " [-1.2356801 ]\n",
      " [-1.794154  ]]\n",
      "U-c for HiPPO-legt:\n",
      "[ 0.22867261 -1.1956275   0.2954892  -0.8106365   0.8921957   0.7978168\n",
      " -1.3765377  -0.695643  ]\n",
      "U-c shape for HiPPO-legt:\n",
      "(8,)\n",
      "U-z reconstruction for HiPPO-legt:\n",
      "[[0.22867261 0.22867261 0.22867261 0.22867261 0.22867261 0.22867261\n",
      "  0.22867261 0.22867261]]\n",
      "U-z reconstruction shape for HiPPO-legt:\n",
      "(1, 8)\n",
      "end of test for HiPPO-legt model\n"
     ]
    }
   ],
   "source": [
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('s4mer-pkg-jZnBSgjq-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a81e05d1d7f7eae781698b7c1b81c0d771335201ebad1d81045cb177cef974b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
