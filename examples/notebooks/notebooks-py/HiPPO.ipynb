{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiPPO Matrices\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import packages\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import linen as jnn\n",
    "\n",
    "from jax.nn.initializers import lecun_normal, uniform\n",
    "from jax.numpy.linalg import eig, inv, matrix_power\n",
    "from jax.scipy.signal import convolve\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from scipy import linalg as la\n",
    "from scipy import signal\n",
    "from scipy import special as ss\n",
    "\n",
    "import math\n",
    "\n",
    "## setup JAX to use TPUs if available\n",
    "try:\n",
    "    url = (\n",
    "        \"http:\"\n",
    "        + os.environ[\"TPU_NAME\"].split(\":\")[1]\n",
    "        + \":8475/requestversion/tpu_driver_nightly\"\n",
    "    )\n",
    "    resp = requests.post(url)\n",
    "    jax.config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
    "    jax.config.FLAGS.jax_backend_target = os.environ[\"TPU_NAME\"]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "jax.devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS enabled: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(f\"MPS enabled: {torch.backends.mps.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 5\n",
    "rng, key2, key3, key4, key5 = jax.random.split(key, num=num_copies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_SSM(rng, N):\n",
    "    a_r, b_r, c_r = jax.random.split(rng, 3)\n",
    "    A = jax.random.uniform(a_r, (N, N))\n",
    "    B = jax.random.uniform(b_r, (N, 1))\n",
    "    C = jax.random.uniform(c_r, (1, N))\n",
    "\n",
    "    return A, B, C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate The HiPPO Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Legendre (LegT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Legendre (LegT) - vectorized\n",
    "def build_LegT_V(N, lambda_n=1):\n",
    "    \"\"\"\n",
    "    The, vectorized implementation of the, measure derived from the translated Legendre basis.\n",
    "\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "        legt_type (str): Choice between the two different tilts of basis.\n",
    "            - legt: translated Legendre - 'legt'\n",
    "            - lmu: Legendre Memory Unit - 'lmu'\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    q = jnp.arange(N, dtype=jnp.float64)\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    case = jnp.power(-1.0, (n - k))\n",
    "    A = None\n",
    "    B = None\n",
    "\n",
    "    if lambda_n == 1:\n",
    "        A_base = -jnp.sqrt(2 * n + 1) * jnp.sqrt(2 * k + 1)\n",
    "        pre_D = jnp.sqrt(jnp.diag(2 * q + 1))\n",
    "        B = D = jnp.diag(pre_D)[:, None]\n",
    "        A = jnp.where(\n",
    "            k <= n, A_base, A_base * case\n",
    "        )  # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "    elif lambda_n == 2:  # (jnp.sqrt(2*n+1) * jnp.power(-1, n)):\n",
    "        A_base = -(2 * n + 1)\n",
    "        B = jnp.diag((2 * q + 1) * jnp.power(-1, n))[:, None]\n",
    "        A = jnp.where(\n",
    "            k <= n, A_base * case, A_base\n",
    "        )  # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Legendre (LegT) - non-vectorized\n",
    "def build_LegT(N, legt_type=\"legt\"):\n",
    "    \"\"\"\n",
    "    The, non-vectorized implementation of the, measure derived from the translated Legendre basis\n",
    "\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "        legt_type (str): Choice between the two different tilts of basis.\n",
    "            - legt: translated Legendre - 'legt'\n",
    "            - lmu: Legendre Memory Unit - 'lmu'\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    Q = jnp.arange(N, dtype=jnp.float64)\n",
    "    pre_R = 2 * Q + 1\n",
    "    k, n = jnp.meshgrid(Q, Q)\n",
    "\n",
    "    if legt_type == \"legt\":\n",
    "        R = jnp.sqrt(pre_R)\n",
    "        A = R[:, None] * jnp.where(n < k, (-1.0) ** (n - k), 1) * R[None, :]\n",
    "        B = R[:, None]\n",
    "        A = -A\n",
    "\n",
    "        # Halve again for timescale correctness\n",
    "        # A, B = A/2, B/2\n",
    "        # A *= 0.5\n",
    "        # B *= 0.5\n",
    "\n",
    "    elif legt_type == \"lmu\":\n",
    "        R = pre_R[:, None]\n",
    "        A = jnp.where(n < k, -1, (-1.0) ** (n - k + 1)) * R\n",
    "        B = (-1.0) ** Q[:, None] * R\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      " [[-1. -1. -1. -1. -1.]\n",
      " [ 3. -3. -3. -3. -3.]\n",
      " [-5.  5. -5. -5. -5.]\n",
      " [ 7. -7.  7. -7. -7.]\n",
      " [-9.  9. -9.  9. -9.]]\n",
      "v:\n",
      " [[-1. -1. -1. -1. -1.]\n",
      " [ 3. -3. -3. -3. -3.]\n",
      " [-5.  5. -5. -5. -5.]\n",
      " [ 7. -7.  7. -7. -7.]\n",
      " [-9.  9. -9.  9. -9.]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_LegT_A, nv_LegT_B = build_LegT(N=N, legt_type=\"lmu\")\n",
    "LegT_A, LegT_B = build_LegT_V(N=N, lambda_n=2)\n",
    "print(f\"nv:\\n\", nv_LegT_A)\n",
    "print(f\"v:\\n\", LegT_A)\n",
    "# print(f\"nv:\\n\", nv_LegT_B)\n",
    "# print(f\"v:\\n\", LegT_B)\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_LegT_A, LegT_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_LegT_B, LegT_B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Laguerre (LagT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Laguerre (LagT) - non-vectorized\n",
    "def build_LagT_V(alpha, beta, N):\n",
    "    \"\"\"\n",
    "    The, vectorized implementation of the, measure derived from the translated Laguerre basis.\n",
    "\n",
    "    Args:\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    L = jnp.exp(\n",
    "        0.5 * (ss.gammaln(jnp.arange(N) + alpha + 1) - ss.gammaln(jnp.arange(N) + 1))\n",
    "    )\n",
    "    inv_L = 1.0 / L[:, None]\n",
    "    pre_A = (jnp.eye(N) * ((1 + beta) / 2)) + jnp.tril(jnp.ones((N, N)), -1)\n",
    "    pre_B = ss.binom(alpha + jnp.arange(N), jnp.arange(N))[:, None]\n",
    "\n",
    "    A = -inv_L * pre_A * L[None, :]\n",
    "    B = (\n",
    "        jnp.exp(-0.5 * ss.gammaln(1 - alpha))\n",
    "        * jnp.power(beta, (1 - alpha) / 2)\n",
    "        * inv_L\n",
    "        * pre_B\n",
    "    )\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translated Laguerre (LagT) - non-vectorized\n",
    "def build_LagT(alpha, beta, N):\n",
    "    \"\"\"\n",
    "    The, non-vectorized implementation of the, measure derived from the translated Laguerre basis.\n",
    "\n",
    "    Args:\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    A = -jnp.eye(N) * (1 + beta) / 2 - jnp.tril(jnp.ones((N, N)), -1)\n",
    "    B = ss.binom(alpha + jnp.arange(N), jnp.arange(N))[:, None]\n",
    "\n",
    "    L = jnp.exp(\n",
    "        0.5 * (ss.gammaln(jnp.arange(N) + alpha + 1) - ss.gammaln(jnp.arange(N) + 1))\n",
    "    )\n",
    "    A = (1.0 / L[:, None]) * A * L[None, :]\n",
    "    B = (\n",
    "        (1.0 / L[:, None])\n",
    "        * B\n",
    "        * jnp.exp(-0.5 * ss.gammaln(1 - alpha))\n",
    "        * beta ** ((1 - alpha) / 2)\n",
    "    )\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      "[[-1. -0. -0. -0. -0.]\n",
      " [-1. -1. -0. -0. -0.]\n",
      " [-1. -1. -1. -0. -0.]\n",
      " [-1. -1. -1. -1. -0.]\n",
      " [-1. -1. -1. -1. -1.]]\n",
      "v:\n",
      "[[-1. -0. -0. -0. -0.]\n",
      " [-1. -1. -0. -0. -0.]\n",
      " [-1. -1. -1. -0. -0.]\n",
      " [-1. -1. -1. -1. -0.]\n",
      " [-1. -1. -1. -1. -1.]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_LagT_A, nv_LagT_B = build_LagT(alpha=0, beta=1, N=N)\n",
    "LagT_A, LagT_B = build_LagT_V(alpha=0, beta=1, N=N)\n",
    "print(f\"nv:\\n{nv_LagT_A}\")\n",
    "print(f\"v:\\n{LagT_A}\")\n",
    "# print(f\"nv:\\n{nv_LagT_B}\")\n",
    "# print(f\"v:\\n{LagT_B}\")\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_LagT_A, LagT_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_LagT_B, LagT_B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Legendre (LegS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Legendre (LegS) vectorized\n",
    "def build_LegS_V(N):\n",
    "    \"\"\"\n",
    "    The, vectorized implementation of the, measure derived from the Scaled Legendre basis.\n",
    "\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    q = jnp.arange(N, dtype=jnp.float64)\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    pre_D = jnp.sqrt(jnp.diag(2 * q + 1))\n",
    "    B = D = jnp.diag(pre_D)[:, None]\n",
    "\n",
    "    A_base = (-jnp.sqrt(2 * n + 1)) * jnp.sqrt(2 * k + 1)\n",
    "    case_2 = (n + 1) / (2 * n + 1)\n",
    "\n",
    "    A = jnp.where(n > k, A_base, 0.0)  # if n > k, then A_base is used, otherwise 0\n",
    "    A = jnp.where(\n",
    "        n == k, (A_base * case_2), A\n",
    "    )  # if n == k, then A_base is used, otherwise A\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Legendre (LegS), non-vectorized\n",
    "def build_LegS(N):\n",
    "    \"\"\"\n",
    "    The, non-vectorized implementation of the, measure derived from the Scaled Legendre basis.\n",
    "\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    q = jnp.arange(\n",
    "        N, dtype=jnp.float64\n",
    "    )  # q represents the values 1, 2, ..., N each column has\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "    r = 2 * q + 1\n",
    "    M = -(jnp.where(n >= k, r, 0) - jnp.diag(q))  # represents the state matrix M\n",
    "    D = jnp.sqrt(\n",
    "        jnp.diag(2 * q + 1)\n",
    "    )  # represents the diagonal matrix D $D := \\text{diag}[(2n+1)^{\\frac{1}{2}}]^{N-1}_{n=0}$\n",
    "    A = D @ M @ jnp.linalg.inv(D)\n",
    "    B = jnp.diag(D)[:, None]\n",
    "    B = (\n",
    "        B.copy()\n",
    "    )  # Otherwise \"UserWarning: given NumPY array is not writeable...\" after torch.as_tensor(B)\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      "[[-1.         0.         0.         0.         0.       ]\n",
      " [-1.7320508 -1.9999999  0.         0.         0.       ]\n",
      " [-2.236068  -3.8729835 -3.         0.         0.       ]\n",
      " [-2.6457512 -4.582576  -5.9160795 -4.         0.       ]\n",
      " [-3.        -5.196152  -6.708204  -7.9372544 -5.       ]]\n",
      "v:\n",
      "[[-1.         0.         0.         0.         0.       ]\n",
      " [-1.7320508 -2.         0.         0.         0.       ]\n",
      " [-2.236068  -3.8729832 -3.         0.         0.       ]\n",
      " [-2.6457512 -4.5825753 -5.9160795 -4.         0.       ]\n",
      " [-3.        -5.196152  -6.7082043 -7.937254  -5.       ]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_LegS_A, nv_LegS_B = build_LegS(N=N)\n",
    "LegS_A, LegS_B = build_LegS_V(N=N)\n",
    "print(f\"nv:\\n{nv_LegS_A}\")\n",
    "print(f\"v:\\n{LegS_A}\")\n",
    "# print(f\"nv:\\n{nv_LegS_B}\")\n",
    "# print(f\"v:\\n{LegS_B}\")\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_LegS_A, LegS_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_LegS_B, LegS_B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourier Basis OPs and functions - vectorized\n",
    "def build_Fourier_V(N, fourier_type=\"fru\"):\n",
    "    \"\"\"\n",
    "    Vectorized measure implementations derived from fourier basis.\n",
    "\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "        fourier_type (str): The type of Fourier measure.\n",
    "            - FRU: Fourier Recurrent Unit - fru\n",
    "            - FouT: truncated Fourier - fout\n",
    "            - fouD: decayed Fourier - foud\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    q = jnp.arange((N // 2) * 2, dtype=jnp.float64)\n",
    "    k, n = jnp.meshgrid(q, q)\n",
    "\n",
    "    n_odd = n % 2 == 0\n",
    "    k_odd = k % 2 == 0\n",
    "\n",
    "    case_1 = (n == k) & (n == 0)\n",
    "    case_2_3 = ((k == 0) & (n_odd)) | ((n == 0) & (k_odd))\n",
    "    case_4 = (n_odd) & (k_odd)\n",
    "    case_5 = (n - k == 1) & (k_odd)\n",
    "    case_6 = (k - n == 1) & (n_odd)\n",
    "\n",
    "    A = None\n",
    "    B = None\n",
    "\n",
    "    if fourier_type == \"fru\":  # Fourier Recurrent Unit (FRU) - vectorized\n",
    "        A = jnp.diag(\n",
    "            jnp.stack([jnp.zeros(N // 2), jnp.zeros(N // 2)], axis=-1).reshape(-1)\n",
    "        )\n",
    "        B = jnp.zeros(A.shape[1], dtype=jnp.float64)\n",
    "        q = jnp.arange((N // 2) * 2, dtype=jnp.float64)\n",
    "\n",
    "        A = jnp.where(\n",
    "            case_1,\n",
    "            -1.0,\n",
    "            jnp.where(\n",
    "                case_2_3,\n",
    "                -jnp.sqrt(2),\n",
    "                jnp.where(\n",
    "                    case_4,\n",
    "                    -2,\n",
    "                    jnp.where(\n",
    "                        case_5,\n",
    "                        jnp.pi * (n // 2),\n",
    "                        jnp.where(case_6, -jnp.pi * (k // 2), 0.0),\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        B = B.at[::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "    elif fourier_type == \"fout\":  # truncated Fourier (FouT) - vectorized\n",
    "        A = jnp.diag(\n",
    "            jnp.stack([jnp.zeros(N // 2), jnp.zeros(N // 2)], axis=-1).reshape(-1)\n",
    "        )\n",
    "        B = jnp.zeros(A.shape[1], dtype=jnp.float64)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "        n_odd = n % 2 == 0\n",
    "        k_odd = k % 2 == 0\n",
    "\n",
    "        A = jnp.where(\n",
    "            case_1,\n",
    "            -1.0,\n",
    "            jnp.where(\n",
    "                case_2_3,\n",
    "                -jnp.sqrt(2),\n",
    "                jnp.where(\n",
    "                    case_4,\n",
    "                    -2,\n",
    "                    jnp.where(\n",
    "                        case_5,\n",
    "                        jnp.pi * (n // 2),\n",
    "                        jnp.where(case_6, -jnp.pi * (k // 2), 0.0),\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        B = B.at[::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        A = 2 * A\n",
    "        B = 2 * B\n",
    "\n",
    "    elif fourier_type == \"fourd\":\n",
    "        A = jnp.diag(\n",
    "            jnp.stack([jnp.zeros(N // 2), jnp.zeros(N // 2)], axis=-1).reshape(-1)\n",
    "        )\n",
    "        B = jnp.zeros(A.shape[1], dtype=jnp.float64)\n",
    "\n",
    "        A = jnp.where(\n",
    "            case_1,\n",
    "            -1.0,\n",
    "            jnp.where(\n",
    "                case_2_3,\n",
    "                -jnp.sqrt(2),\n",
    "                jnp.where(\n",
    "                    case_4,\n",
    "                    -2,\n",
    "                    jnp.where(\n",
    "                        case_5,\n",
    "                        2 * jnp.pi * (n // 2),\n",
    "                        jnp.where(case_6, 2 * -jnp.pi * (k // 2), 0.0),\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        B = B.at[::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        A = 0.5 * A\n",
    "        B = 0.5 * B\n",
    "\n",
    "    B = B[:, None]\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Fourier(N, fourier_type=\"fru\"):\n",
    "    \"\"\"\n",
    "    Non-vectorized measure implementations derived from fourier basis.\n",
    "\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "        fourier_type (str): The type of Fourier measure.\n",
    "            - FRU: Fourier Recurrent Unit - fru\n",
    "            - FouT: truncated Fourier - fout\n",
    "            - fouD: decayed Fourier - foud\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The A HiPPO matrix.\n",
    "        B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    freqs = jnp.arange(N // 2)\n",
    "\n",
    "    if fourier_type == \"fru\":  # Fourier Recurrent Unit (FRU) - non-vectorized\n",
    "        d = jnp.stack([jnp.zeros(N // 2), freqs], axis=-1).reshape(-1)[1:]\n",
    "        A = jnp.pi * (-jnp.diag(d, 1) + jnp.diag(d, -1))\n",
    "\n",
    "        B = jnp.zeros(A.shape[1])\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        A = A - B[:, None] * B[None, :]\n",
    "        B = B[:, None]\n",
    "\n",
    "    elif fourier_type == \"fout\":  # truncated Fourier (FouT) - non-vectorized\n",
    "        freqs *= 2\n",
    "        d = jnp.stack([jnp.zeros(N // 2), freqs], axis=-1).reshape(-1)[1:]\n",
    "        A = jnp.pi * (-jnp.diag(d, 1) + jnp.diag(d, -1))\n",
    "\n",
    "        B = jnp.zeros(A.shape[1])\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        # Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case\n",
    "        A = A - B[:, None] * B[None, :] * 2\n",
    "        B = B[:, None] * 2\n",
    "\n",
    "    elif fourier_type == \"fourd\":\n",
    "        d = jnp.stack([jnp.zeros(N // 2), freqs], axis=-1).reshape(-1)[1:]\n",
    "        A = jnp.pi * (-jnp.diag(d, 1) + jnp.diag(d, -1))\n",
    "\n",
    "        B = jnp.zeros(A.shape[1])\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        # Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case\n",
    "        A = A - 0.5 * B[:, None] * B[None, :]\n",
    "        B = 0.5 * B[:, None]\n",
    "\n",
    "    return A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      "[[-1.         0.        -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999 -3.1415927]\n",
      " [ 0.         0.         3.1415927  0.       ]]\n",
      "v:\n",
      "[[-1.        -0.        -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -2.        -3.1415927]\n",
      " [ 0.         0.         3.1415927  0.       ]]\n",
      "A Comparison:\n",
      " True\n",
      "B Comparison:\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "nv_Fourier_A, nv_Fourier_B = build_Fourier(N=N, fourier_type=\"fru\")\n",
    "Fourier_A, Fourier_B = build_Fourier_V(N=N, fourier_type=\"fru\")\n",
    "print(f\"nv:\\n{nv_Fourier_A}\")\n",
    "print(f\"v:\\n{Fourier_A}\")\n",
    "# print(f\"nv:\\n{nv_Fourier_B}\")\n",
    "# print(f\"v:\\n{Fourier_B}\")\n",
    "print(f\"A Comparison:\\n {jnp.allclose(nv_Fourier_A, Fourier_A)}\")\n",
    "print(f\"B Comparison:\\n {jnp.allclose(nv_Fourier_B, Fourier_B)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_HiPPO(\n",
    "    N, v=\"nv\", measure=\"legs\", lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Instantiates the HiPPO matrix of a given order using a particular measure.\n",
    "    Args:\n",
    "        N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "        v (str): choose between this repo's implementation or hazy research's implementation.\n",
    "        measure (str):\n",
    "            choose between\n",
    "                - HiPPO w/ Translated Legendre (LegT) - legt\n",
    "                - HiPPO w/ Translated Laguerre (LagT) - lagt\n",
    "                - HiPPO w/ Scaled Legendre (LegS) - legs\n",
    "                - HiPPO w/ Fourier basis - fourier\n",
    "                    - FRU: Fourier Recurrent Unit\n",
    "                    - FouT: Translated Fourier\n",
    "        lambda_n (int): The amount of tilt applied to the HiPPO-LegS basis, determines between LegS and LMU.\n",
    "        fourier_type (str): chooses between the following:\n",
    "            - FRU: Fourier Recurrent Unit - fru\n",
    "            - FouT: Translated Fourier - fout\n",
    "            - FourD: Fourier Decay - fourd\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "\n",
    "    Returns:\n",
    "        A (jnp.ndarray): The HiPPO matrix multiplied by -1.\n",
    "        B (jnp.ndarray): The other corresponding state space matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    A = None\n",
    "    B = None\n",
    "    if measure == \"legt\":\n",
    "        if v == \"nv\":\n",
    "            A, B = build_LegT(N=N, lambda_n=lambda_n)\n",
    "        else:\n",
    "            A, B = build_LegT_V(N=N, lambda_n=lambda_n)\n",
    "\n",
    "    elif measure == \"lagt\":\n",
    "        if v == \"nv\":\n",
    "            A, B = build_LagT(alpha=alpha, beta=beta, N=N)\n",
    "        else:\n",
    "            A, B = build_LagT_V(alpha=alpha, beta=beta, N=N)\n",
    "\n",
    "    elif measure == \"legs\":\n",
    "        if v == \"nv\":\n",
    "            A, B = build_LegS(N=N)\n",
    "        else:\n",
    "            A, B = build_LegS_V(N=N)\n",
    "\n",
    "    elif measure == \"fourier\":\n",
    "        if v == \"nv\":\n",
    "            A, B = build_Fourier(N=N, fourier_type=fourier_type)\n",
    "        else:\n",
    "            A, B = build_Fourier_V(N=N, fourier_type=fourier_type)\n",
    "\n",
    "    elif measure == \"random\":\n",
    "        A = jnp.random.randn(N, N) / N\n",
    "        B = jnp.random.randn(N, 1)\n",
    "\n",
    "    elif measure == \"diagonal\":\n",
    "        A = -jnp.diag(jnp.exp(jnp.random.randn(N)))\n",
    "        B = jnp.random.randn(N, 1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid HiPPO type\")\n",
    "\n",
    "    A_copy = A.copy()\n",
    "    B_copy = B.copy()\n",
    "\n",
    "    return jnp.array(A_copy), B_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv:\n",
      " [[-1.         0.         0.         0.         0.       ]\n",
      " [-1.7320508 -1.9999999  0.         0.         0.       ]\n",
      " [-2.236068  -3.8729835 -3.         0.         0.       ]\n",
      " [-2.6457512 -4.582576  -5.9160795 -4.         0.       ]\n",
      " [-3.        -5.196152  -6.708204  -7.9372544 -5.       ]]\n",
      "v:\n",
      " [[-1.         0.         0.         0.         0.       ]\n",
      " [-1.7320508 -2.         0.         0.         0.       ]\n",
      " [-2.236068  -3.8729832 -3.         0.         0.       ]\n",
      " [-2.6457512 -4.5825753 -5.9160795 -4.         0.       ]\n",
      " [-3.        -5.196152  -6.7082043 -7.937254  -5.       ]]\n",
      "A Comparison:\n",
      "  True\n",
      "B Comparison:\n",
      "  True\n"
     ]
    }
   ],
   "source": [
    "nv_A, nv_B = make_HiPPO(\n",
    "    N=N, v=\"nv\", measure=\"legs\", lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1\n",
    ")\n",
    "v_A, v_B = make_HiPPO(\n",
    "    N=N, v=\"v\", measure=\"legs\", lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1\n",
    ")\n",
    "print(f\"nv:\\n\", nv_A)\n",
    "print(f\"v:\\n\", v_A)\n",
    "# print(f\"nv:\\n\", nv_B)\n",
    "# print(f\"v:\\n\", v_B)\n",
    "print(f\"A Comparison:\\n \", jnp.allclose(nv_A, v_A))\n",
    "print(f\"B Comparison:\\n \", jnp.allclose(nv_B, v_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Old utilities for parallel scan implementation of Linear RNNs. \"\"\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "### Utilities\n",
    "\n",
    "\n",
    "def shift_up(a, s=None, drop=True, dim=0):\n",
    "    assert dim == 0\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(a[0, ...])\n",
    "    s = s.unsqueeze(dim)\n",
    "    if drop:\n",
    "        a = a[:-1, ...]\n",
    "    return torch.cat((s, a), dim=dim)\n",
    "\n",
    "\n",
    "def interleave(a, b, uneven=False, dim=0):\n",
    "    \"\"\"Interleave two tensors of same shape\"\"\"\n",
    "    # assert(a.shape == b.shape)\n",
    "    assert dim == 0  # TODO temporary to make handling uneven case easier\n",
    "    if dim < 0:\n",
    "        dim = N + dim\n",
    "    if uneven:\n",
    "        a_ = a[-1:, ...]\n",
    "        a = a[:-1, ...]\n",
    "    c = torch.stack((a, b), dim + 1)\n",
    "    out_shape = list(a.shape)\n",
    "    out_shape[dim] *= 2\n",
    "    c = c.view(out_shape)\n",
    "    if uneven:\n",
    "        c = torch.cat((c, a_), dim=dim)\n",
    "    return c\n",
    "\n",
    "\n",
    "def batch_mult(A, u, has_batch=None):\n",
    "    \"\"\"Matrix mult A @ u with special case to save memory if u has additional batch dim\n",
    "\n",
    "    The batch dimension is assumed to be the second dimension\n",
    "    A : (L, ..., N, N)\n",
    "    u : (L, [B], ..., N)\n",
    "    has_batch: True, False, or None. If None, determined automatically\n",
    "\n",
    "    Output:\n",
    "    x : (L, [B], ..., N)\n",
    "      A @ u broadcasted appropriately\n",
    "    \"\"\"\n",
    "\n",
    "    if has_batch is None:\n",
    "        has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    if has_batch:\n",
    "        u = u.permute([0] + list(range(2, len(u.shape))) + [1])\n",
    "    else:\n",
    "        u = u.unsqueeze(-1)\n",
    "    v = A @ u\n",
    "    if has_batch:\n",
    "        v = v.permute([0] + [len(u.shape) - 1] + list(range(1, len(u.shape) - 1)))\n",
    "    else:\n",
    "        v = v[..., 0]\n",
    "    return v\n",
    "\n",
    "\n",
    "### Main unrolling functions\n",
    "\n",
    "\n",
    "def unroll(A, u):\n",
    "    \"\"\"\n",
    "    A : (..., N, N) # TODO I think this can't take batch dimension?\n",
    "    u : (L, ..., N)\n",
    "    output : x (..., N) # TODO a lot of these shapes are wrong\n",
    "    x[i, ...] = A^{i} @ u[0, ...] + ... + A @ u[i-1, ...] + u[i, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    m = u.new_zeros(u.shape[1:])\n",
    "    outputs = []\n",
    "    for u_ in torch.unbind(u, dim=0):\n",
    "        m = F.linear(m, A) + u_\n",
    "        outputs.append(m)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive(A, u):\n",
    "    \"\"\"Bottom-up divide-and-conquer version of unroll.\"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_(A, u):\n",
    "        if u.shape[0] == 1:\n",
    "            return u\n",
    "\n",
    "        u_evens = u[0::2, ...]\n",
    "        u_odds = u[1::2, ...]\n",
    "\n",
    "        # u2 = F.linear(u_evens, A) + u_odds\n",
    "        u2 = (A @ u_evens.unsqueeze(-1)).squeeze(-1) + u_odds\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_odds = parallel_unroll_recursive_(A2, u2)\n",
    "        # x_evens = F.linear(shift_up(x_odds), A) + u_evens\n",
    "        x_evens = (A @ shift_up(x_odds).unsqueeze(-1)).squeeze(-1) + u_evens\n",
    "\n",
    "        x = interleave(x_evens, x_odds, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n) / math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N - u.shape[0],) + u.shape[1:])), dim=0)\n",
    "\n",
    "    return parallel_unroll_recursive_(A, u)[:n, ...]\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive_br(A, u):\n",
    "    \"\"\"Same as parallel_unroll_recursive but uses bit reversal for locality.\"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_br_(A, u):\n",
    "        n = u.shape[0]\n",
    "        if n == 1:\n",
    "            return u\n",
    "\n",
    "        m = n // 2\n",
    "        u_0 = u[:m, ...]\n",
    "        u_1 = u[m:, ...]\n",
    "\n",
    "        u2 = F.linear(u_0, A) + u_1\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_1 = parallel_unroll_recursive_br_(A2, u2)\n",
    "        x_0 = F.linear(shift_up(x_1), A) + u_0\n",
    "\n",
    "        # x = torch.cat((x_0, x_1), dim=0) # is there a way to do this with cat?\n",
    "        x = interleave(x_0, x_1, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n) / math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N - u.shape[0],) + u.shape[1:])), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    x = parallel_unroll_recursive_br_(A, u)\n",
    "    return x[:n, ...]\n",
    "\n",
    "\n",
    "def parallel_unroll_iterative(A, u):\n",
    "    \"\"\"Bottom-up divide-and-conquer version of unroll, implemented iteratively\"\"\"\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n) / math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N - u.shape[0],) + u.shape[1:])), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    # Main recursive loop, flattened\n",
    "    us = []  # stores the u_0 terms in the recursive version\n",
    "    N_ = N\n",
    "    As = []  # stores the A matrices\n",
    "    for l in range(m):\n",
    "        N_ = N_ // 2\n",
    "        As.append(A)\n",
    "        u_0 = u[:N_, ...]\n",
    "        us.append(u_0)\n",
    "        u = F.linear(u_0, A) + u[N_:, ...]\n",
    "        A = A @ A\n",
    "    x_0 = []\n",
    "    x = u  # x_1\n",
    "    for l in range(m - 1, -1, -1):\n",
    "        x_0 = F.linear(shift_up(x), As[l]) + us[l]\n",
    "        x = interleave(x_0, x, dim=0)\n",
    "\n",
    "    return x[:n, ...]\n",
    "\n",
    "\n",
    "def variable_unroll_sequential(A, u, s=None, variable=True):\n",
    "    \"\"\"Unroll with variable (in time/length) transitions A.\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] @ s + A[i..1] @ u[0] + ... + A[i] @ u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        s = batch_mult(A_.unsqueeze(0), s.unsqueeze(0), has_batch)[0]\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def variable_unroll(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    \"\"\"Bottom-up divide-and-conquer version of variable_unroll.\"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        return variable_unroll_sequential(A, u, s, variable)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1 = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1 = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = A_1 @ A_0_\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll(A_10, u_10, s, variable, recurse_limit)\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "    x = interleave(\n",
    "        x_0, x_1, uneven, dim=0\n",
    "    )  # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "\n",
    "def variable_unroll_general_sequential(A, u, s, op, variable=True):\n",
    "    \"\"\"Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        s = op(A_, s)\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def variable_unroll_matrix_sequential(A, u, s=None, variable=True):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    return variable_unroll_general_sequential(A, u, s, op, variable=True)\n",
    "\n",
    "\n",
    "def variable_unroll_toeplitz_sequential(A, u, s=None, variable=True, pad=False):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        ret = variable_unroll_general_sequential(\n",
    "            A, u, s, triangular_toeplitz_multiply_padded, variable=True\n",
    "        )\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    return variable_unroll_general_sequential(\n",
    "        A, u, s, triangular_toeplitz_multiply, variable=True\n",
    "    )\n",
    "\n",
    "\n",
    "### General parallel scan functions with generic binary composition operators\n",
    "\n",
    "\n",
    "def variable_unroll_general(\n",
    "    A, u, s, op, compose_op=None, sequential_op=None, variable=True, recurse_limit=16\n",
    "):\n",
    "    \"\"\"Bottom-up divide-and-conquer version of variable_unroll.\n",
    "\n",
    "    compose is an optional function that defines how to compose A without multiplying by a leaf u\n",
    "    \"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        if sequential_op is None:\n",
    "            sequential_op = op\n",
    "        return variable_unroll_general_sequential(A, u, s, sequential_op, variable)\n",
    "\n",
    "    if compose_op is None:\n",
    "        compose_op = op\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1 = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1 = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = op(A_1, u_0_)  # batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = compose_op(A_1, A_0_)\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll_general(\n",
    "        A_10,\n",
    "        u_10,\n",
    "        s,\n",
    "        op,\n",
    "        compose_op,\n",
    "        sequential_op,\n",
    "        variable=variable,\n",
    "        recurse_limit=recurse_limit,\n",
    "    )\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = op(A_0, x_0)  # batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "    x = interleave(\n",
    "        x_0, x_1, uneven, dim=0\n",
    "    )  # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "\n",
    "def variable_unroll_matrix(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "    op = lambda x, y: batch_mult(x, y, has_batch)\n",
    "    sequential_op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[\n",
    "        0\n",
    "    ]\n",
    "    matmul = lambda x, y: x @ y\n",
    "    return variable_unroll_general(\n",
    "        A,\n",
    "        u,\n",
    "        s,\n",
    "        op,\n",
    "        compose_op=matmul,\n",
    "        sequential_op=sequential_op,\n",
    "        variable=variable,\n",
    "        recurse_limit=recurse_limit,\n",
    "    )\n",
    "\n",
    "\n",
    "def variable_unroll_toeplitz(A, u, s=None, variable=True, recurse_limit=8, pad=False):\n",
    "    \"\"\"Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (L, [B], ..., N) same shape as u\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "    # Add the batch dimension to A if necessary\n",
    "    A_batch_dims = len(A.shape) - int(variable)\n",
    "    u_batch_dims = len(u.shape) - 1\n",
    "    if u_batch_dims > A_batch_dims:\n",
    "        # assert u_batch_dims == A_batch_dims + 1\n",
    "        if variable:\n",
    "            while len(A.shape) < len(u.shape):\n",
    "                A = A.unsqueeze(1)\n",
    "        # else:\n",
    "        #     A = A.unsqueeze(0)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        op = triangular_toeplitz_multiply_padded()\n",
    "        ret = variable_unroll_general(\n",
    "            A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit\n",
    "        )\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    op = triangular_toeplitz_multiply\n",
    "    ret = variable_unroll_general(\n",
    "        A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit\n",
    "    )\n",
    "    return ret\n",
    "\n",
    "\n",
    "### Testing\n",
    "\n",
    "\n",
    "def test_correctness():\n",
    "    print(\"Testing Correctness\\n====================\")\n",
    "\n",
    "    # Test sequential unroll\n",
    "    L = 3\n",
    "    A = torch.Tensor([[1, 1], [1, 0]])\n",
    "    u = torch.ones((L, 2))\n",
    "    x = unroll(A, u)\n",
    "    assert torch.isclose(x, torch.Tensor([[1.0, 1.0], [3.0, 2.0], [6.0, 4.0]])).all()\n",
    "\n",
    "    # Test utilities\n",
    "    assert torch.isclose(\n",
    "        shift_up(x), torch.Tensor([[0.0, 0.0], [1.0, 1.0], [3.0, 2.0]])\n",
    "    ).all()\n",
    "    assert torch.isclose(\n",
    "        interleave(x, x),\n",
    "        torch.Tensor(\n",
    "            [[1.0, 1.0], [1.0, 1.0], [3.0, 2.0], [3.0, 2.0], [6.0, 4.0], [6.0, 4.0]]\n",
    "        ),\n",
    "    ).all()\n",
    "\n",
    "    # Test parallel unroll\n",
    "    x = parallel_unroll_recursive(A, u)\n",
    "    assert torch.isclose(x, torch.Tensor([[1.0, 1.0], [3.0, 2.0], [6.0, 4.0]])).all()\n",
    "\n",
    "    # Powers\n",
    "    L = 12\n",
    "    A = torch.Tensor([[1, 0, 0], [2, 1, 0], [3, 3, 1]])\n",
    "    u = torch.ones((L, 3))\n",
    "    x = parallel_unroll_recursive(A, u)\n",
    "    print(\"recursive\", x)\n",
    "    x = parallel_unroll_recursive_br(A, u)\n",
    "    print(\"recursive_br\", x)\n",
    "    x = parallel_unroll_iterative(A, u)\n",
    "    print(\"iterative_br\", x)\n",
    "\n",
    "    A = A.repeat((L, 1, 1))\n",
    "    s = torch.zeros(3)\n",
    "    print(\"A shape\", A.shape)\n",
    "    x = variable_unroll_sequential(A, u, s)\n",
    "    print(\"variable_unroll\", x)\n",
    "    x = variable_unroll(A, u, s)\n",
    "    print(\"parallel_variable_unroll\", x)\n",
    "\n",
    "\n",
    "def generate_data(L, N, B=None, cuda=True):\n",
    "    A = torch.eye(N) + torch.normal(0, 1, size=(N, N)) / (N**0.5) / L\n",
    "    u = torch.normal(0, 1, size=(L, B, N))\n",
    "\n",
    "    # device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    device = torch.device(\"cuda:0\") if cuda else torch.device(\"cpu\")\n",
    "    A = A.to(device)\n",
    "    u = u.to(device)\n",
    "    return A, u\n",
    "\n",
    "\n",
    "def test_stability():\n",
    "    print(\"Testing Stability\\n====================\")\n",
    "    L = 256\n",
    "    N = L // 2\n",
    "    B = 100\n",
    "    A, u = generate_data(L, N, B)\n",
    "\n",
    "    x = unroll(A, u)\n",
    "    x1 = parallel_unroll_recursive(A, u)\n",
    "    x2 = parallel_unroll_recursive_br(A, u)\n",
    "    x3 = parallel_unroll_iterative(A, u)\n",
    "    print(\"norm error\", torch.norm(x - x1))\n",
    "    print(\"norm error\", torch.norm(x - x2))\n",
    "    print(\"norm error\", torch.norm(x - x3))\n",
    "\n",
    "    print(\"max error\", torch.max(torch.abs(x - x1)))\n",
    "    print(\"max error\", torch.max(torch.abs(x - x2)))\n",
    "    print(\"max error\", torch.max(torch.abs(x - x3)))\n",
    "\n",
    "    A = A.repeat((L, 1, 1))\n",
    "    x = variable_unroll_sequential(A, u)\n",
    "    x_ = variable_unroll(A, u)\n",
    "    # x_ = variable_unroll_matrix_sequential(A, u)\n",
    "    x_ = variable_unroll_matrix(A, u)\n",
    "    print(x - x_)\n",
    "    abserr = torch.abs(x - x_)\n",
    "    relerr = abserr / (torch.abs(x) + 1e-8)\n",
    "    print(\"norm abs error\", torch.norm(abserr))\n",
    "    print(\"max abs error\", torch.max(abserr))\n",
    "    print(\"norm rel error\", torch.norm(relerr))\n",
    "    print(\"max rel error\", torch.max(relerr))\n",
    "\n",
    "\n",
    "def test_toeplitz():\n",
    "    from model.toeplitz import construct_toeplitz\n",
    "\n",
    "    def summarize(name, x, x_, showdiff=False):\n",
    "        print(name, \"stats\")\n",
    "        if showdiff:\n",
    "            print(x - x_)\n",
    "        abserr = torch.abs(x - x_)\n",
    "        relerr = abserr / (torch.abs(x) + 1e-8)\n",
    "        print(\"  norm abs error\", torch.norm(abserr))\n",
    "        print(\"  max abs error\", torch.max(abserr))\n",
    "        print(\"  norm rel error\", torch.norm(relerr))\n",
    "        print(\"  max rel error\", torch.max(relerr))\n",
    "\n",
    "    print(\"Testing Toeplitz\\n====================\")\n",
    "    L = 512\n",
    "    N = L // 2\n",
    "    B = 100\n",
    "    A, u = generate_data(L, N, B)\n",
    "\n",
    "    A = A[..., 0]\n",
    "    A = construct_toeplitz(A)\n",
    "\n",
    "    # print(\"SHAPES\", A.shape, u.shape)\n",
    "\n",
    "    # Static A\n",
    "    x = unroll(A, u)\n",
    "    x_ = variable_unroll(A, u, variable=False)\n",
    "    summarize(\"nonvariable matrix original\", x, x_, showdiff=False)\n",
    "    x_ = variable_unroll_matrix(A, u, variable=False)\n",
    "    summarize(\"nonvariable matrix general\", x, x_, showdiff=False)\n",
    "    x_ = variable_unroll_toeplitz(A[..., 0], u, variable=False)\n",
    "    summarize(\"nonvariable toeplitz\", x, x_, showdiff=False)\n",
    "\n",
    "    # Sequential\n",
    "    A = A.repeat((L, 1, 1))\n",
    "    for _ in range(1):\n",
    "        x_ = variable_unroll_sequential(A, u)\n",
    "        summarize(\"variable unroll sequential\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_matrix_sequential(A, u)\n",
    "        summarize(\"variable matrix sequential\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_toeplitz_sequential(A[..., 0], u, pad=True)\n",
    "        summarize(\"variable toeplitz sequential\", x, x_, showdiff=False)\n",
    "\n",
    "    # Parallel\n",
    "    for _ in range(1):\n",
    "        x_ = variable_unroll(A, u)\n",
    "        summarize(\"variable matrix original\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_matrix(A, u)\n",
    "        summarize(\"variable matrix general\", x, x_, showdiff=False)\n",
    "        x_ = variable_unroll_toeplitz(A[..., 0], u, pad=True, recurse_limit=8)\n",
    "        summarize(\"variable toeplitz\", x, x_, showdiff=False)\n",
    "\n",
    "\n",
    "def test_speed(variable=False, it=1):\n",
    "    print(\"Testing Speed\\n====================\")\n",
    "    N = 256\n",
    "    L = 1024\n",
    "    B = 100\n",
    "    A, u = generate_data(L, N, B)\n",
    "    As = A.repeat((L, 1, 1))\n",
    "\n",
    "    u.requires_grad = True\n",
    "    As.requires_grad = True\n",
    "    for _ in range(it):\n",
    "        x = unroll(A, u)\n",
    "        x = torch.sum(x)\n",
    "        x.backward()\n",
    "\n",
    "        x = parallel_unroll_recursive(A, u)\n",
    "        x = torch.sum(x)\n",
    "        x.backward()\n",
    "\n",
    "        # parallel_unroll_recursive_br(A, u)\n",
    "        # parallel_unroll_iterative(A, u)\n",
    "\n",
    "    for _ in range(it):\n",
    "        if variable:\n",
    "            x = variable_unroll_sequential(As, u, variable=True, recurse_limit=16)\n",
    "            x = torch.sum(x)\n",
    "            x.backward()\n",
    "            x = variable_unroll(As, u, variable=True, recurse_limit=16)\n",
    "            x = torch.sum(x)\n",
    "            x.backward()\n",
    "        else:\n",
    "            variable_unroll_sequential(A, u, variable=False, recurse_limit=16)\n",
    "            variable_unroll(A, u, variable=False, recurse_limit=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LegT(nn.Module):\n",
    "    def __init__(self, N, dt=1.0, discretization=\"bilinear\"):\n",
    "        \"\"\"\n",
    "        N: the order of the HiPPO projection\n",
    "        dt: discretization step size - should be roughly inverse to the length of the sequence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        # A, B = transition('lmu', N)\n",
    "        A, B = make_HiPPO(\n",
    "            N=self.N,\n",
    "            v=\"v\",\n",
    "            measure=\"legt\",\n",
    "            lambda_n=1,\n",
    "            fourier_type=\"fru\",\n",
    "            alpha=0,\n",
    "            beta=1,\n",
    "        )\n",
    "        C = np.ones((1, N))\n",
    "        D = np.zeros((1,))\n",
    "        # dt, discretization options\n",
    "        A, B, _, _, _ = signal.cont2discrete((A, B, C, D), dt=dt, method=discretization)\n",
    "\n",
    "        B = B.squeeze(-1)\n",
    "\n",
    "        self.register_buffer(\"A\", torch.Tensor(A))  # (N, N)\n",
    "        self.register_buffer(\"B\", torch.Tensor(B))  # (N,)\n",
    "\n",
    "        # vals = np.linspace(0.0, 1.0, 1./dt)\n",
    "        vals = np.arange(0.0, 1.0, dt)\n",
    "        self.eval_matrix = torch.Tensor(\n",
    "            ss.eval_legendre(np.arange(N)[:, None], 1 - 2 * vals).T\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = inputs * self.B  # (length, ..., N)\n",
    "\n",
    "        c = torch.zeros(u.shape[1:])\n",
    "        cs = []\n",
    "        for f in inputs:\n",
    "            c = F.linear(c, self.A) + self.B * f\n",
    "            cs.append(c)\n",
    "        return torch.stack(cs, dim=0)\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        return (self.eval_matrix @ c.unsqueeze(-1)).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LegS(nn.Module):\n",
    "    \"\"\"Vanilla HiPPO-LegS model (scale invariant instead of time invariant)\"\"\"\n",
    "\n",
    "    def __init__(self, N, max_length=1024, measure=\"legs\", discretization=\"bilinear\"):\n",
    "        \"\"\"\n",
    "        max_length: maximum sequence length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        A, B = make_HiPPO(\n",
    "            N=self.N,\n",
    "            v=\"v\",\n",
    "            measure=measure,\n",
    "            lambda_n=1,\n",
    "            fourier_type=\"fru\",\n",
    "            alpha=0,\n",
    "            beta=1,\n",
    "        )\n",
    "        # A, B = transition(measure, N)\n",
    "        B = B.squeeze(-1)\n",
    "        A_stacked = np.empty((max_length, N, N), dtype=A.dtype)\n",
    "        B_stacked = np.empty((max_length, N), dtype=B.dtype)\n",
    "        for t in range(1, max_length + 1):\n",
    "            At = A / t\n",
    "            Bt = B / t\n",
    "            if discretization == \"forward\":\n",
    "                A_stacked[t - 1] = np.eye(N) + At\n",
    "                B_stacked[t - 1] = Bt\n",
    "            elif discretization == \"backward\":\n",
    "                A_stacked[t - 1] = la.solve_triangular(\n",
    "                    np.eye(N) - At, np.eye(N), lower=True\n",
    "                )\n",
    "                B_stacked[t - 1] = la.solve_triangular(np.eye(N) - At, Bt, lower=True)\n",
    "            elif discretization == \"bilinear\":\n",
    "                A_stacked[t - 1] = np.linalg.lstsq(\n",
    "                    np.eye(N) - At / 2, np.eye(N) + At / 2, rcond=None\n",
    "                )[\n",
    "                    0\n",
    "                ]  # TODO: Referencing this: https://stackoverflow.com/questions/64527098/numpy-linalg-linalgerror-singular-matrix-error-when-trying-to-solve\n",
    "                B_stacked[t - 1] = np.linalg.lstsq(np.eye(N) - At / 2, Bt, rcond=None)[\n",
    "                    0\n",
    "                ]\n",
    "            else:  # ZOH\n",
    "                A_stacked[t - 1] = la.expm(A * (math.log(t + 1) - math.log(t)))\n",
    "                B_stacked[t - 1] = la.solve_triangular(\n",
    "                    A, A_stacked[t - 1] @ B - B, lower=True\n",
    "                )\n",
    "        self.A_stacked = torch.Tensor(A_stacked.copy())  # (max_length, N, N)\n",
    "        self.B_stacked = torch.Tensor(B_stacked.copy())  # (max_length, N)\n",
    "        vals = np.linspace(0.0, 1.0, max_length)\n",
    "        self.eval_matrix = torch.from_numpy(\n",
    "            np.asarray(\n",
    "                ((B[:, None] * ss.eval_legendre(np.arange(N)[:, None], 2 * vals - 1)).T)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, fast=False):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "        result = None\n",
    "\n",
    "        L = inputs.shape[0]\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = torch.transpose(inputs, 0, -2)\n",
    "        u = u * self.B_stacked[:L]\n",
    "        u = torch.transpose(u, 0, -2)  # (length, ..., N)\n",
    "\n",
    "        if fast:\n",
    "            result = variable_unroll_matrix(self.A_stacked[:L], u)\n",
    "\n",
    "        else:\n",
    "            result = variable_unroll_matrix_sequential(self.A_stacked[:L], u)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        a = self.eval_matrix @ c.unsqueeze(-1)\n",
    "        return a.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO(jnn.Module):\n",
    "    \"\"\"\n",
    "    class that constructs HiPPO model using the defined measure.\n",
    "\n",
    "    Args:\n",
    "        N (int): order of the HiPPO projection, aka the number of coefficients to describe the matrix\n",
    "        max_length (int): maximum sequence length to be input\n",
    "        measure (str): the measure used to define which way to instantiate the HiPPO matrix\n",
    "        step (float): step size used for descretization\n",
    "        GBT_alpha (float): represents which descretization transformation to use based off the alpha value\n",
    "        seq_L (int): length of the sequence to be used for training\n",
    "        v (str): choice of vectorized or non-vectorized function instantiation\n",
    "            - 'v': vectorized\n",
    "            - 'nv': non-vectorized\n",
    "        lambda_n (float): value associated with the tilt of legt\n",
    "            - 1: tilt on legt\n",
    "            - \\sqrt(2n+1)(-1)^{N}: tilt associated with the legendre memory unit (LMU)\n",
    "        fourier_type (str): choice of fourier measures\n",
    "            - fru: fourier recurrent unit measure (FRU) - 'fru'\n",
    "            - fout: truncated Fourier (FouT) - 'fout'\n",
    "            - fourd: decaying fourier transform - 'fourd'\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "    \"\"\"\n",
    "\n",
    "    N: int\n",
    "    max_length: int\n",
    "    measure: str\n",
    "    step: float\n",
    "    GBT_alpha: float\n",
    "    seq_L: int\n",
    "    v: str\n",
    "    lambda_n: float\n",
    "    fourier_type: str\n",
    "    alpha: float\n",
    "    beta: float\n",
    "\n",
    "    def setup(self):\n",
    "        A, B = make_HiPPO(\n",
    "            N=self.N,\n",
    "            v=self.v,\n",
    "            measure=self.measure,\n",
    "            lambda_n=self.lambda_n,\n",
    "            fourier_type=self.fourier_type,\n",
    "            alpha=self.alpha,\n",
    "            beta=self.beta,\n",
    "        )\n",
    "\n",
    "        self.A = A\n",
    "        self.B = B  # .squeeze(-1)\n",
    "        self.C = jnp.ones((1, self.N)).squeeze(0)\n",
    "        self.D = jnp.zeros((1,))\n",
    "\n",
    "        if self.measure == \"legt\":\n",
    "            L = self.seq_L\n",
    "            vals = jnp.arange(0.0, 1.0, L)\n",
    "            n = jnp.arange(self.N)[:, None]\n",
    "            x = 1 - 2 * vals\n",
    "            self.eval_matrix = ss.eval_legendre(n, x).T\n",
    "\n",
    "        elif self.measure == \"legs\":\n",
    "            L = self.max_length\n",
    "            vals = jnp.linspace(0.0, 1.0, L)\n",
    "            n = jnp.arange(self.N)[:, None]\n",
    "            x = 2 * vals - 1\n",
    "            self.eval_matrix = (\n",
    "                B[:, None]\n",
    "                * jax.scipy.special.lpmn_values(\n",
    "                    m=self.N - 1, n=self.N - 1, z=x, is_normalized=False\n",
    "                )\n",
    "            ).T  # ss.eval_legendre(n, x)).T\n",
    "        else:\n",
    "            raise ValueError(\"invalid measure\")\n",
    "\n",
    "    def __call__(self, u, init_state=None, kernel=False):\n",
    "        print(f\"u shape:\\n{u.shape}\")\n",
    "        print(f\"u:\\n{u}\")\n",
    "        if not kernel:\n",
    "            if init_state is None:\n",
    "                init_state = jnp.zeros((self.N,))\n",
    "\n",
    "            Ab, Bb, Cb, Db = self.collect_SSM_vars(\n",
    "                self.A, self.B, self.C, self.D, u, alpha=self.GBT_alpha\n",
    "            )\n",
    "            c_k = self.scan_SSM(Ab, Bb, Cb, Db, u, x0=init_state)[1]\n",
    "        else:\n",
    "            Ab, Bb, Cb, Db = self.discretize(\n",
    "                self.A, self.B, self.C, self.D, step=self.step, alpha=self.GBT_alpha\n",
    "            )\n",
    "            c_k = self.causal_convolution(\n",
    "                u, self.K_conv(Ab, Bb, Cb, Db, L=self.max_length)\n",
    "            )\n",
    "\n",
    "        return c_k\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        \"\"\"\n",
    "        Uses coeffecients to reconstruct the signal\n",
    "\n",
    "        Args:\n",
    "            c (jnp.ndarray): coefficients of the HiPPO projection\n",
    "\n",
    "        Returns:\n",
    "            reconstructed signal\n",
    "        \"\"\"\n",
    "        return (self.eval_matrix @ jnp.expand_dims(c, -1)).squeeze(-1)\n",
    "\n",
    "    def discretize(self, A, B, C, D, step, alpha=0.5):\n",
    "        \"\"\"\n",
    "        function used for discretizing the HiPPO matrix\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            step (float): step size used for discretization\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "                - forward Euler corresponds to α = 0,\n",
    "                - backward Euler corresponds to α = 1,\n",
    "                - bilinear corresponds to α = 0.5,\n",
    "                - Zero-order Hold corresponds to α > 1\n",
    "        \"\"\"\n",
    "        I = jnp.eye(A.shape[0])\n",
    "        GBT = jnp.linalg.inv(I - (step * alpha * A))\n",
    "        GBT_A = GBT @ (I + (step * (1 - alpha) * A))\n",
    "        GBT_B = (step * GBT) @ B\n",
    "\n",
    "        if alpha > 1:  # Zero-order Hold\n",
    "            GBT_A = jax.scipy.linalg.expm(step * A)\n",
    "            GBT_B = (jnp.linalg.inv(A) @ (jax.scipy.linalg.expm(step * A) - I)) @ B\n",
    "\n",
    "        return GBT_A, GBT_B, C, D\n",
    "\n",
    "    def collect_SSM_vars(self, A, B, C, D, u, alpha=0.5):\n",
    "        \"\"\"\n",
    "        turns the continuos HiPPO matrix components into discrete ones\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            u (jnp.ndarray): input signal\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "\n",
    "        Returns:\n",
    "            Ab (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Bb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Cb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Db (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "        \"\"\"\n",
    "        L = u.shape[0]\n",
    "        N = A.shape[0]\n",
    "        assert (\n",
    "            L == self.seq_L\n",
    "        ), f\"sequence length must match, currently {L} != {self.seq_L}\"\n",
    "        assert N == self.N, f\"Order number must match, currently {N} != {self.N}\"\n",
    "\n",
    "        Ab, Bb, Cb, Db = self.discretize(A, B, C, D, step=1.0 / L, alpha=alpha)\n",
    "\n",
    "        return Ab, Bb, Cb, Db\n",
    "\n",
    "    def scan_SSM(self, Ab, Bb, Cb, Db, u, x0):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            u (jnp.ndarray): the input sequence\n",
    "            x0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "\n",
    "        def step(x_k_1, u_k):\n",
    "            \"\"\"\n",
    "            Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "            Args:\n",
    "                x_k_1: previous hidden state\n",
    "                u_k: output from function f at, descritized, time step, k.\n",
    "\n",
    "            Returns:\n",
    "                x_k: current hidden state\n",
    "                y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "            \"\"\"\n",
    "\n",
    "            x_k = (Ab @ x_k_1) + (Bb @ u_k)\n",
    "            y_k = (Cb @ x_k) + (Db @ u_k)\n",
    "\n",
    "            print(f\"x_k_1.shape:\\n{x_k_1.shape}\")\n",
    "            print(f\"u_k.shape:\\n{u_k.shape}\")\n",
    "            print(f\"x0.shape:\\n{x0.shape}\")\n",
    "            print(f\"x_k.shape:\\n{x_k.shape}\\n\\n\")\n",
    "            \n",
    "            print(f\"First Part:\\n{Ab @ x_k_1}\")\n",
    "            print(f\"First Part shape:\\n{(Ab @ x_k_1).shape}\")\n",
    "            print(f\"Second Part:\\n{Bb @ u_k}\")\n",
    "            print(f\"Second Part shape:\\n{(Bb @ u_k).shape}\")\n",
    "            print(f\"Third Part:\\n{Cb @ x_k}\")\n",
    "            print(f\"Third Part shape:\\n{(Cb @ x_k).shape}\")\n",
    "            print(f\"Fourth Part:\\n{Db @ u_k}\")\n",
    "            print(f\"Fourth Part shape:\\n{(Db @ u_k).shape}\\n\\n\")\n",
    "            \n",
    "            print(f\"Ab.shape:\\n{Ab.shape}\")\n",
    "            print(f\"Bb.shape:\\n{Bb.shape}\")\n",
    "            print(f\"Cb.shape:\\n{Cb.shape}\")\n",
    "            print(f\"Db.shape:\\n{Db.shape}\")\n",
    "\n",
    "            return x_k, y_k\n",
    "\n",
    "        return jax.lax.scan(step, x0, u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # N = 256\n",
    "    # L = 128\n",
    "\n",
    "    N = 8\n",
    "    L = 8\n",
    "\n",
    "    x = torch.randn(L, 1)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegT model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"\\nTesting HiPPO LegT model\")\n",
    "    hippo_legt = HiPPO_LegT(N, dt=1.0 / L)\n",
    "\n",
    "    y = hippo_legt(x)\n",
    "\n",
    "    print(f\"h-y shape for LegT:\\n{y.shape}\")\n",
    "    z = hippo_legt.reconstruct(y)\n",
    "    print(f\"h-z shape for LegT:\\n{z.shape}\")\n",
    "\n",
    "    mse = loss(z[-1, 0, :L], x.squeeze(-1))\n",
    "    print(f\"h-MSE shape:\\n{mse}\")\n",
    "    print(f\"end of test for HiPPO LegT model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegS model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"\\nTesting HiPPO LegS model\")\n",
    "    hippo_legs = HiPPO_LegS(N, max_length=L)  # The Gu's\n",
    "\n",
    "    y = hippo_legs(x)\n",
    "\n",
    "    print(f\"h-y shape for LegS:\\n{y.shape}\")\n",
    "    print(f\"h-y for LegS:\\n{y}\")\n",
    "\n",
    "    z = hippo_legs(x, fast=True)\n",
    "\n",
    "    print(f\"h-reconstruction shape for LegS:\\n{hippo_legs.reconstruct(z).shape}\")\n",
    "    print(f\"h-reconstruction for LegS:\\n{hippo_legs.reconstruct(z)}\")\n",
    "\n",
    "    # print(y-z)\n",
    "    print(f\"end of test for HiPPO LegT model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test Generic HiPPO model --------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"\\nTesting BRYANS HiPPO LegS model\")\n",
    "    hippo_LegS_B = HiPPO(\n",
    "        N=N,\n",
    "        max_length=L,\n",
    "        measure=\"legs\",\n",
    "        step=1.0 / L,\n",
    "        GBT_alpha=0.5,\n",
    "        seq_L=L,\n",
    "        v=\"v\",\n",
    "        lambda_n=1.0,\n",
    "        fourier_type=\"fru\",\n",
    "        alpha=0.0,\n",
    "        beta=1.0,\n",
    "    )  # Bryan's\n",
    "\n",
    "    x = jnp.asarray(x)  # convert torch array to jax array\n",
    "    print(f\"input:\\n{x}\")\n",
    "    print(f\"input type:\\n{type(x)}\")\n",
    "\n",
    "    params = hippo_LegS_B.init(key2, x)\n",
    "\n",
    "    c_legs = hippo_LegS_B.apply(params, x)\n",
    "\n",
    "    y_legs = hippo_LegS_B.apply(\n",
    "        {\"params\": params}, c_legs, method=hippo_LegS_B.reconstruct\n",
    "    )\n",
    "\n",
    "    print(f\"U-c shape for LegS:\\n{c_legs.shape}\")\n",
    "    print(f\"U-c for LegS:\\n{c_legs}\")\n",
    "\n",
    "    print(f\"U-reconstruction shape for LegS:\\n{y_legs.shape}\")\n",
    "    print(f\"U-reconstruction for LegS:\\n{y_legs}\")\n",
    "\n",
    "    print(f\"end of test for HiPPO LegS model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing HiPPO LegT model\n",
      "h-y shape for LegT:\n",
      "torch.Size([8, 1, 8])\n",
      "h-z shape for LegT:\n",
      "torch.Size([8, 1, 8])\n",
      "h-MSE shape:\n",
      "1.1421599388122559\n",
      "end of test for HiPPO LegT model\n",
      "\n",
      "Testing HiPPO LegS model\n",
      "h-y shape for LegS:\n",
      "torch.Size([8, 1, 8])\n",
      "h-y for LegS:\n",
      "tensor([[[-2.7446e-01, -2.3769e-01, -6.1371e-02, -1.1901e-09,  6.4179e-09,\n",
      "          -2.5385e-09,  7.2246e-09, -1.1264e-08]],\n",
      "\n",
      "        [[ 1.4653e-01,  4.0688e-01,  4.3554e-01,  1.8329e-01,  2.3093e-02,\n",
      "           1.1321e-08, -4.7390e-09, -5.2686e-09]],\n",
      "\n",
      "        [[-2.3691e-01, -2.9466e-01, -4.7471e-01, -6.0028e-01, -3.5358e-01,\n",
      "          -8.6425e-02, -7.2272e-03, -3.0617e-08]],\n",
      "\n",
      "        [[-1.4082e-01, -4.3652e-02,  5.9600e-02,  3.0324e-01,  6.2659e-01,\n",
      "           5.2063e-01,  1.9414e-01,  3.2861e-02]],\n",
      "\n",
      "        [[ 1.6996e-01,  4.1947e-01,  4.5468e-01,  3.7694e-01,  1.1032e-01,\n",
      "          -4.3024e-01, -6.0270e-01, -3.2559e-01]],\n",
      "\n",
      "        [[ 2.4650e-01,  4.1326e-01,  1.9473e-01, -1.3827e-01, -4.1470e-01,\n",
      "          -4.5559e-01,  7.7408e-02,  5.4249e-01]],\n",
      "\n",
      "        [[ 8.8454e-02,  7.0419e-02, -2.7523e-01, -4.9872e-01, -3.8355e-01,\n",
      "           1.8365e-02,  4.8247e-01,  2.6082e-01]],\n",
      "\n",
      "        [[-7.6456e-02, -1.9913e-01, -4.7261e-01, -3.9758e-01,  3.1658e-02,\n",
      "           4.1292e-01,  3.8525e-01, -2.1570e-01]]])\n",
      "h-reconstruction shape for LegS:\n",
      "torch.Size([8, 1, 8])\n",
      "h-reconstruction for LegS:\n",
      "tensor([[[ 1.4901e-08, -1.6804e-02, -6.7214e-02, -1.5123e-01, -2.6886e-01,\n",
      "          -4.2009e-01, -6.0493e-01, -8.2337e-01]],\n",
      "\n",
      "        [[-2.9802e-08, -4.8391e-02, -1.6933e-01, -2.9009e-01, -2.8950e-01,\n",
      "           2.1037e-03,  8.0286e-01,  2.3794e+00]],\n",
      "\n",
      "        [[-4.1723e-07, -9.0927e-02, -2.5480e-01, -2.3857e-01,  1.3391e-01,\n",
      "           5.9354e-01, -8.9830e-02, -4.7704e+00]],\n",
      "\n",
      "        [[-8.4851e-03, -1.4159e-01, -2.7226e-01,  2.0837e-02,  4.4007e-01,\n",
      "          -2.4860e-01, -1.8113e+00,  5.1529e+00]],\n",
      "\n",
      "        [[ 3.0867e-01, -9.8992e-02, -2.7272e-01,  3.2195e-01,  7.3483e-02,\n",
      "          -1.3365e+00,  1.4600e+00, -1.6196e+00]],\n",
      "\n",
      "        [[-1.2231e+00, -5.5643e-01,  1.9961e-01,  3.4669e-01, -1.1138e+00,\n",
      "           6.8896e-01,  2.1785e+00,  6.5695e-01]],\n",
      "\n",
      "        [[ 1.8840e-01, -2.9262e-01,  2.5867e-01, -3.3798e-01, -2.4780e-01,\n",
      "           1.6838e+00,  4.6496e-01, -6.4549e-02]],\n",
      "\n",
      "        [[ 1.2135e+00,  5.7739e-02,  1.6639e-04, -5.2740e-01,  9.0508e-01,\n",
      "           1.1737e+00, -1.3241e+00, -5.1190e-01]]])\n",
      "end of test for HiPPO LegT model\n",
      "\n",
      "Testing BRYANS HiPPO LegS model\n",
      "input:\n",
      "[[-0.41168588]\n",
      " [ 0.7780102 ]\n",
      " [-1.1955127 ]\n",
      " [ 0.1954897 ]\n",
      " [ 1.5684515 ]\n",
      " [ 0.6674991 ]\n",
      " [-0.93884534]\n",
      " [-1.3132855 ]]\n",
      "input type:\n",
      "<class 'jaxlib.xla_extension.DeviceArray'>\n",
      "u shape:\n",
      "(8, 1)\n",
      "u:\n",
      "[[-0.41168588]\n",
      " [ 0.7780102 ]\n",
      " [-1.1955127 ]\n",
      " [ 0.1954897 ]\n",
      " [ 1.5684515 ]\n",
      " [ 0.6674991 ]\n",
      " [-0.93884534]\n",
      " [-1.3132855 ]]\n",
      "x_k_1.shape:\n",
      "(8,)\n",
      "u_k.shape:\n",
      "(1,)\n",
      "x0.shape:\n",
      "(8,)\n",
      "x_k.shape:\n",
      "(8,)\n",
      "\n",
      "\n",
      "First Part:\n",
      "Traced<ShapedArray(float32[8])>with<DynamicJaxprTrace(level=1/0)>\n",
      "First Part shape:\n",
      "(8,)\n",
      "Second Part:\n",
      "Traced<ShapedArray(float32[8])>with<DynamicJaxprTrace(level=1/0)>\n",
      "Second Part shape:\n",
      "(8,)\n",
      "Third Part:\n",
      "Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "Third Part shape:\n",
      "()\n",
      "Fourth Part:\n",
      "Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "Fourth Part shape:\n",
      "()\n",
      "\n",
      "\n",
      "Ab.shape:\n",
      "(8, 8)\n",
      "Bb.shape:\n",
      "(8, 1)\n",
      "Cb.shape:\n",
      "(8,)\n",
      "Db.shape:\n",
      "(1,)\n",
      "u shape:\n",
      "(8, 1)\n",
      "u:\n",
      "[[-0.41168588]\n",
      " [ 0.7780102 ]\n",
      " [-1.1955127 ]\n",
      " [ 0.1954897 ]\n",
      " [ 1.5684515 ]\n",
      " [ 0.6674991 ]\n",
      " [-0.93884534]\n",
      " [-1.3132855 ]]\n",
      "x_k_1.shape:\n",
      "(8,)\n",
      "u_k.shape:\n",
      "(1,)\n",
      "x0.shape:\n",
      "(8,)\n",
      "x_k.shape:\n",
      "(8,)\n",
      "\n",
      "\n",
      "First Part:\n",
      "Traced<ShapedArray(float32[8])>with<DynamicJaxprTrace(level=1/0)>\n",
      "First Part shape:\n",
      "(8,)\n",
      "Second Part:\n",
      "Traced<ShapedArray(float32[8])>with<DynamicJaxprTrace(level=1/0)>\n",
      "Second Part shape:\n",
      "(8,)\n",
      "Third Part:\n",
      "Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "Third Part shape:\n",
      "()\n",
      "Fourth Part:\n",
      "Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "Fourth Part shape:\n",
      "()\n",
      "\n",
      "\n",
      "Ab.shape:\n",
      "(8, 8)\n",
      "Bb.shape:\n",
      "(8, 1)\n",
      "Cb.shape:\n",
      "(8,)\n",
      "Db.shape:\n",
      "(1,)\n",
      "U-c shape for LegS:\n",
      "(8,)\n",
      "U-c for LegS:\n",
      "[-0.35283348  0.8156554  -1.4061145   0.7366574   1.0933992  -0.10503935\n",
      " -0.64139473 -0.3927725 ]\n",
      "U-reconstruction shape for LegS:\n",
      "(8, 8)\n",
      "U-reconstruction for LegS:\n",
      "[[-3.5283348e-01  3.5283348e-01 -3.5283348e-01  3.5283345e-01\n",
      "  -3.5283345e-01  3.5283339e-01 -3.5283336e-01  3.5283330e-01]\n",
      " [-3.5283348e-01 -7.3669970e-01 -2.5949163e+00  4.1217289e+00\n",
      "   1.0418338e+02 -5.7011957e+02 -1.3590353e+03  4.1105977e+04]\n",
      " [-3.5283348e-01 -1.1252224e+00 -5.9796629e+00 -5.0637131e+00\n",
      "   2.8642032e+02 -7.6013660e+02 -1.3011814e+04  1.7663841e+05]\n",
      " [-3.5283348e-01 -1.3478616e+00 -8.4751310e+00 -1.9934471e+01\n",
      "   3.7711832e+02 -5.4771820e+01 -2.4383842e+04  2.3400680e+05]\n",
      " [-3.5283348e-01 -1.4486712e+00 -9.6736450e+00 -3.2988403e+01\n",
      "   3.2328229e+02  8.3830139e+02 -2.3169750e+04  1.4572431e+05]\n",
      " [-3.5283348e-01 -1.4276510e+00 -9.2619267e+00 -3.7748947e+01\n",
      "   1.6173340e+02  1.0621282e+03 -1.0954537e+04  2.8940625e+04]\n",
      " [-3.5283348e-01 -1.2407473e+00 -6.8323016e+00 -2.8765118e+01\n",
      "   1.9514923e+00  4.3120746e+02 -8.7676013e+02 -4.2095762e+03]\n",
      " [-3.5283348e-01 -3.5283348e-01 -3.5283348e-01 -3.5283345e-01\n",
      "  -3.5283345e-01 -3.5283339e-01 -3.5283336e-01 -3.5283330e-01]]\n",
      "end of test for HiPPO LegS model\n"
     ]
    }
   ],
   "source": [
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "438252231bd0ead6588590f345d61a2faed731bfef8d506d9281b2d5303c1900"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
