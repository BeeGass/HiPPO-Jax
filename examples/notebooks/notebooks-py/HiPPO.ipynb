{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiPPO Matrices\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StreamExecutorGpuDevice(id=0, process_index=0)]\n",
      "The Device: gpu\n"
     ]
    }
   ],
   "source": [
    "## import packages\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import linen as jnn\n",
    "\n",
    "from jax.nn.initializers import lecun_normal, uniform\n",
    "from jax.numpy.linalg import eig, inv, matrix_power\n",
    "from jax.scipy.signal import convolve\n",
    "\n",
    "# import modules \n",
    "from src.models.hippo.gu_transition import GuTransMatrix\n",
    "\n",
    "import requests\n",
    "\n",
    "from scipy import linalg as la\n",
    "from scipy import signal\n",
    "from scipy import special as ss\n",
    "\n",
    "import math\n",
    "\n",
    "print(jax.devices())\n",
    "print(f\"The Device: {jax.lib.xla_bridge.get_backend().platform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS enabled: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(f\"MPS enabled: {torch.backends.mps.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 5\n",
    "rng, key2, key3, key4, key5 = jax.random.split(key, num=num_copies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_SSM(rng, N):\n",
    "    a_r, b_r, c_r = jax.random.split(rng, 3)\n",
    "    A = jax.random.uniform(a_r, (N, N))\n",
    "    B = jax.random.uniform(b_r, (N, 1))\n",
    "    C = jax.random.uniform(c_r, (1, N))\n",
    "\n",
    "    return A, B, C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate The HiPPO Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransMatrix:\n",
    "    def __init__(\n",
    "        self, N, measure=\"legs\", lambda_n=1, fourier_type=\"fru\", alpha=0, beta=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiates the HiPPO matrix of a given order using a particular measure.\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            v (str): choose between this repo's implementation or hazy research's implementation.\n",
    "            measure (str):\n",
    "                choose between\n",
    "                    - HiPPO w/ Translated Legendre (LegT) - legt\n",
    "                    - HiPPO w/ Translated Laguerre (LagT) - lagt\n",
    "                    - HiPPO w/ Scaled Legendre (LegS) - legs\n",
    "                    - HiPPO w/ Fourier basis - fourier\n",
    "                        - FRU: Fourier Recurrent Unit\n",
    "                        - FouT: Translated Fourier\n",
    "            lambda_n (int): The amount of tilt applied to the HiPPO-LegS basis, determines between LegS and LMU.\n",
    "            fourier_type (str): chooses between the following:\n",
    "                - FRU: Fourier Recurrent Unit - fru\n",
    "                - FouT: Translated Fourier - fout\n",
    "                - FourD: Fourier Decay - fourd\n",
    "            alpha (float): The order of the Laguerre basis.\n",
    "            beta (float): The scale of the Laguerre basis.\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The HiPPO matrix multiplied by -1.\n",
    "            B (jnp.ndarray): The other corresponding state space matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        A = None\n",
    "        B = None\n",
    "        if measure == \"legt\":\n",
    "            A, B = self.build_LegT(N=N, lambda_n=lambda_n)\n",
    "\n",
    "        elif measure == \"lagt\":\n",
    "            A, B = self.build_LagT(alpha=alpha, beta=beta, N=N)\n",
    "\n",
    "        elif measure == \"legs\":\n",
    "            A, B = self.build_LegS(N=N)\n",
    "\n",
    "        elif measure == \"fourier\":\n",
    "            A, B = self.build_Fourier(N=N, fourier_type=fourier_type)\n",
    "\n",
    "        elif measure == \"random\":\n",
    "            A = jnp.random.randn(N, N) / N\n",
    "            B = jnp.random.randn(N, 1)\n",
    "\n",
    "        elif measure == \"diagonal\":\n",
    "            A = -jnp.diag(jnp.exp(jnp.random.randn(N)))\n",
    "            B = jnp.random.randn(N, 1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid HiPPO type\")\n",
    "\n",
    "        self.A_matrix = (A.copy()).astype(np.float32)\n",
    "        self.B_matrix = (B.copy()).astype(np.float32)\n",
    "\n",
    "    # Translated Legendre (LegT) - vectorized\n",
    "    @staticmethod\n",
    "    def build_LegT(N, lambda_n=1):\n",
    "        \"\"\"\n",
    "        The, vectorized implementation of the, measure derived from the translated Legendre basis.\n",
    "\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            legt_type (str): Choice between the two different tilts of basis.\n",
    "                - legt: translated Legendre - 'legt'\n",
    "                - lmu: Legendre Memory Unit - 'lmu'\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        q = jnp.arange(N, dtype=jnp.float32)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "        case = jnp.power(-1.0, (n - k))\n",
    "        A = None\n",
    "        B = None\n",
    "\n",
    "        if lambda_n == 1:\n",
    "            A_base = -jnp.sqrt(2 * n + 1) * jnp.sqrt(2 * k + 1)\n",
    "            pre_D = jnp.sqrt(jnp.diag(2 * q + 1))\n",
    "            B = D = jnp.diag(pre_D)[:, None]\n",
    "            A = jnp.where(\n",
    "                k <= n, A_base, A_base * case\n",
    "            )  # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "        elif lambda_n == 2:  # (jnp.sqrt(2*n+1) * jnp.power(-1, n)):\n",
    "            A_base = -(2 * n + 1)\n",
    "            B = jnp.diag((2 * q + 1) * jnp.power(-1, n))[:, None]\n",
    "            A = jnp.where(\n",
    "                k <= n, A_base * case, A_base\n",
    "            )  # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "        return A, B\n",
    "\n",
    "    # Translated Laguerre (LagT) - non-vectorized\n",
    "    @staticmethod\n",
    "    def build_LagT(alpha, beta, N):\n",
    "        \"\"\"\n",
    "        The, vectorized implementation of the, measure derived from the translated Laguerre basis.\n",
    "\n",
    "        Args:\n",
    "            alpha (float): The order of the Laguerre basis.\n",
    "            beta (float): The scale of the Laguerre basis.\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        L = jnp.exp(\n",
    "            0.5\n",
    "            * (ss.gammaln(jnp.arange(N) + alpha + 1) - ss.gammaln(jnp.arange(N) + 1))\n",
    "        )\n",
    "        inv_L = 1.0 / L[:, None]\n",
    "        pre_A = (jnp.eye(N) * ((1 + beta) / 2)) + jnp.tril(jnp.ones((N, N)), -1)\n",
    "        pre_B = ss.binom(alpha + jnp.arange(N), jnp.arange(N))[:, None]\n",
    "\n",
    "        A = -inv_L * pre_A * L[None, :]\n",
    "        B = (\n",
    "            jnp.exp(-0.5 * ss.gammaln(1 - alpha))\n",
    "            * jnp.power(beta, (1 - alpha) / 2)\n",
    "            * inv_L\n",
    "            * pre_B\n",
    "        )\n",
    "\n",
    "        return A, B\n",
    "\n",
    "    # Scaled Legendre (LegS) vectorized\n",
    "    @staticmethod\n",
    "    def build_LegS(N):\n",
    "        \"\"\"\n",
    "        The, vectorized implementation of the, measure derived from the Scaled Legendre basis.\n",
    "\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        q = jnp.arange(N, dtype=jnp.float32)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "        pre_D = jnp.sqrt(jnp.diag(2 * q + 1))\n",
    "        B = D = jnp.diag(pre_D)[:, None]\n",
    "\n",
    "        A_base = jnp.sqrt(2 * n + 1) * jnp.sqrt(2 * k + 1)\n",
    "        case_2 = (n + 1) / (2 * n + 1)\n",
    "\n",
    "        A = jnp.where(n > k, \n",
    "                      A_base, \n",
    "                      jnp.where(n == k, \n",
    "                                n+1, \n",
    "                                0.0\n",
    "                                )\n",
    "                      )\n",
    "        \n",
    "        return -A.astype(np.float32), B.astype(np.float32)\n",
    "\n",
    "    # Fourier Basis OPs and functions - vectorized\n",
    "    @staticmethod\n",
    "    def build_Fourier(N, fourier_type=\"fru\"):\n",
    "        \"\"\"\n",
    "        Vectorized measure implementations derived from fourier basis.\n",
    "\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            fourier_type (str): The type of Fourier measure.\n",
    "                - FRU: Fourier Recurrent Unit - fru\n",
    "                - FouT: truncated Fourier - fout\n",
    "                - fouD: decayed Fourier - foud\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        A = jnp.diag(\n",
    "            jnp.stack([jnp.zeros(N // 2), jnp.zeros(N // 2)], axis=-1).reshape(-1)[1:],\n",
    "            1,\n",
    "        )\n",
    "        B = jnp.zeros(A.shape[1], dtype=jnp.float32)\n",
    "\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        q = jnp.arange(A.shape[1], dtype=jnp.float32)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "\n",
    "        n_odd = n % 2 == 0\n",
    "        k_odd = k % 2 == 0\n",
    "\n",
    "        case_1 = (n == k) & (n == 0)\n",
    "        case_2_3 = ((k == 0) & (n_odd)) | ((n == 0) & (k_odd))\n",
    "        case_4 = (n_odd) & (k_odd)\n",
    "        case_5 = (n - k == 1) & (k_odd)\n",
    "        case_6 = (k - n == 1) & (n_odd)\n",
    "\n",
    "        if fourier_type == \"fru\":  # Fourier Recurrent Unit (FRU) - vectorized\n",
    "            A = jnp.where(\n",
    "                case_1,\n",
    "                -1.0,\n",
    "                jnp.where(\n",
    "                    case_2_3,\n",
    "                    -jnp.sqrt(2),\n",
    "                    jnp.where(\n",
    "                        case_4,\n",
    "                        -2,\n",
    "                        jnp.where(\n",
    "                            case_5,\n",
    "                            jnp.pi * (n // 2),\n",
    "                            jnp.where(case_6, -jnp.pi * (k // 2), 0.0),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        elif fourier_type == \"fout\":  # truncated Fourier (FouT) - vectorized\n",
    "            A = jnp.where(\n",
    "                case_1,\n",
    "                -1.0,\n",
    "                jnp.where(\n",
    "                    case_2_3,\n",
    "                    -jnp.sqrt(2),\n",
    "                    jnp.where(\n",
    "                        case_4,\n",
    "                        -2,\n",
    "                        jnp.where(\n",
    "                            case_5,\n",
    "                            jnp.pi * (n // 2),\n",
    "                            jnp.where(case_6, -jnp.pi * (k // 2), 0.0),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            A = 2 * A\n",
    "            B = 2 * B\n",
    "\n",
    "        elif fourier_type == \"fourd\":\n",
    "            A = jnp.where(\n",
    "                case_1,\n",
    "                -1.0,\n",
    "                jnp.where(\n",
    "                    case_2_3,\n",
    "                    -jnp.sqrt(2),\n",
    "                    jnp.where(\n",
    "                        case_4,\n",
    "                        -2,\n",
    "                        jnp.where(\n",
    "                            case_5,\n",
    "                            2 * jnp.pi * (n // 2),\n",
    "                            jnp.where(case_6, 2 * -jnp.pi * (k // 2), 0.0),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            A = 0.5 * A\n",
    "            B = 0.5 * B\n",
    "\n",
    "        B = B[:, None]\n",
    "\n",
    "        return A.astype(jnp.float32), B.astype(jnp.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Legendre (LegT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LegT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LegT():\n",
    "    legt_matrices = TransMatrix(N=8, measure=\"legt\", lambda_n=1.0)\n",
    "    A, B = legt_matrices.A_matrix, legt_matrices.B_matrix\n",
    "    gu_legt_matrices = GuTransMatrix(N=8, measure=\"legt\", lambda_n=1.0)\n",
    "    gu_A, gu_B = gu_legt_matrices.A_matrix, gu_legt_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/Documents/Coding/s4mer/src/models/hippo/gu_transition.py:114: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in arange is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  Q = jnp.arange(N, dtype=jnp.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -1.          1.7320508  -2.2360678   2.6457512  -3.          3.3166246\n",
      "   -3.6055512   3.8729832]\n",
      " [ -1.7320508  -3.          3.872983   -4.5825753   5.196152   -5.744562\n",
      "    6.244998   -6.708204 ]\n",
      " [ -2.2360678  -3.872983   -4.999999    5.916079   -6.7082033   7.4161973\n",
      "   -8.062257    8.660253 ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -6.9999995   7.937254   -8.774963\n",
      "    9.5393915 -10.24695  ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -9.          9.949874\n",
      "  -10.816654   11.61895  ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874  -10.999999\n",
      "   11.958261  -12.845232 ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261\n",
      "  -13.         13.964239 ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232\n",
      "  -13.964239  -14.999999 ]]\n",
      "Gu's A:\n",
      " [[ -1.          1.7320508  -2.2360678   2.6457512  -3.          3.3166246\n",
      "   -3.6055512   3.8729832]\n",
      " [ -1.7320508  -3.          3.872983   -4.5825753   5.196152   -5.744562\n",
      "    6.244998   -6.708204 ]\n",
      " [ -2.2360678  -3.872983   -4.999999    5.916079   -6.7082033   7.4161973\n",
      "   -8.062257    8.660253 ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -6.9999995   7.937254   -8.774963\n",
      "    9.5393915 -10.24695  ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -9.          9.949874\n",
      "  -10.816654   11.61895  ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874  -10.999999\n",
      "   11.958261  -12.845232 ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261\n",
      "  -13.         13.964239 ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232\n",
      "  -13.964239  -14.999999 ]]\n",
      "B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n",
      "Gu's B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n"
     ]
    }
   ],
   "source": [
    "test_LegT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LMU():\n",
    "    lmu_matrices = TransMatrix(\n",
    "        N=8, measure=\"legt\", lambda_n=2.0\n",
    "    )  # change lambda so resulting matrix is in the form of LMU\n",
    "    A, B = lmu_matrices.A_matrix, lmu_matrices.B_matrix\n",
    "    gu_lmu_matrices = GuTransMatrix(\n",
    "        N=8, measure=\"legt\", lambda_n=2.0\n",
    "    )  # change lambda so resulting matrix is in the form of LMU\n",
    "    gu_A, gu_B = gu_lmu_matrices.A_matrix, gu_lmu_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/Documents/Coding/s4mer/src/models/hippo/gu_transition.py:114: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in arange is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  Q = jnp.arange(N, dtype=jnp.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.]\n",
      " [ -5.   5.  -5.  -5.  -5.  -5.  -5.  -5.]\n",
      " [  7.  -7.   7.  -7.  -7.  -7.  -7.  -7.]\n",
      " [ -9.   9.  -9.   9.  -9.  -9.  -9.  -9.]\n",
      " [ 11. -11.  11. -11.  11. -11. -11. -11.]\n",
      " [-13.  13. -13.  13. -13.  13. -13. -13.]\n",
      " [ 15. -15.  15. -15.  15. -15.  15. -15.]]\n",
      "Gu's A:\n",
      " [[ -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.]\n",
      " [ -5.   5.  -5.  -5.  -5.  -5.  -5.  -5.]\n",
      " [  7.  -7.   7.  -7.  -7.  -7.  -7.  -7.]\n",
      " [ -9.   9.  -9.   9.  -9.  -9.  -9.  -9.]\n",
      " [ 11. -11.  11. -11.  11. -11. -11. -11.]\n",
      " [-13.  13. -13.  13. -13.  13. -13. -13.]\n",
      " [ 15. -15.  15. -15.  15. -15.  15. -15.]]\n",
      "B:\n",
      " [[  1.]\n",
      " [ -3.]\n",
      " [  5.]\n",
      " [ -7.]\n",
      " [  9.]\n",
      " [-11.]\n",
      " [ 13.]\n",
      " [-15.]]\n",
      "Gu's B:\n",
      " [[  1.]\n",
      " [ -3.]\n",
      " [  5.]\n",
      " [ -7.]\n",
      " [  9.]\n",
      " [-11.]\n",
      " [ 13.]\n",
      " [-15.]]\n"
     ]
    }
   ],
   "source": [
    "test_LMU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Laguerre (LagT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LagT():\n",
    "    lagt_matrices = TransMatrix(\n",
    "        N=8,\n",
    "        measure=\"lagt\",\n",
    "        alpha=0.0,  # change resulting tilt through alpha and beta\n",
    "        beta=1.0,\n",
    "    )  # change resulting tilt through alpha and beta\n",
    "    A, B = lagt_matrices.A_matrix, lagt_matrices.B_matrix\n",
    "    gu_lagt_matrices = GuTransMatrix(\n",
    "        N=8,\n",
    "        measure=\"lagt\",\n",
    "        alpha=0.0,  # change resulting tilt through alpha and beta\n",
    "        beta=1.0,\n",
    "    )  # change resulting tilt through alpha and beta\n",
    "    gu_A, gu_B = gu_lagt_matrices.A_matrix, gu_lagt_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-1.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -0.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -0.        ]\n",
      " [-0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976\n",
      "  -0.99999976 -1.        ]]\n",
      "Gu's A:\n",
      " [[-1.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -0.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -0.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -0.        ]\n",
      " [-0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976\n",
      "  -0.99999976 -1.        ]]\n",
      "B:\n",
      " [[1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Gu's B:\n",
      " [[1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n"
     ]
    }
   ],
   "source": [
    "test_LagT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Legendre (LegS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LegS():\n",
    "    legs_matrices = TransMatrix(N=8, measure=\"legs\")\n",
    "    A, B = legs_matrices.A_matrix, legs_matrices.B_matrix\n",
    "    gu_legs_matrices = GuTransMatrix(N=8, measure=\"legs\")\n",
    "    gu_A, gu_B = gu_legs_matrices.A_matrix, gu_legs_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/Documents/Coding/s4mer/src/models/hippo/gu_transition.py:80: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in arange is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  q = jnp.arange(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -1.         -0.         -0.         -0.         -0.         -0.\n",
      "   -0.         -0.       ]\n",
      " [ -1.7320508  -2.         -0.         -0.         -0.         -0.\n",
      "   -0.         -0.       ]\n",
      " [ -2.2360678  -3.872983   -3.         -0.         -0.         -0.\n",
      "   -0.         -0.       ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -4.         -0.         -0.\n",
      "   -0.         -0.       ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -5.         -0.\n",
      "   -0.         -0.       ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874   -6.\n",
      "   -0.         -0.       ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261\n",
      "   -7.         -0.       ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232\n",
      "  -13.964239   -8.       ]]\n",
      "Gu's A:\n",
      " [[ -1.          0.          0.          0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -1.7320508  -1.9999999   0.          0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.2360678  -3.872983   -3.          0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.6457512  -4.582576   -5.91608    -4.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -3.         -5.196152   -6.7082047  -7.9372544  -5.          0.\n",
      "    0.          0.       ]\n",
      " [ -3.3166246  -5.744562   -7.4161987  -8.774965   -9.949874   -6.\n",
      "    0.          0.       ]\n",
      " [ -3.6055512  -6.244998   -8.062259   -9.539392  -10.816654  -11.958261\n",
      "   -7.          0.       ]\n",
      " [ -3.8729832  -6.708204   -8.6602545 -10.246951  -11.61895   -12.845232\n",
      "  -13.964239   -7.9999995]]\n",
      "B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n",
      "Gu's B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n"
     ]
    }
   ],
   "source": [
    "test_LegS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Recurrent Unit (FRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FRU():\n",
    "    fru_matrices = TransMatrix(N=8, measure=\"fourier\", fourier_type=\"fru\")\n",
    "    A, B = fru_matrices.A_matrix, fru_matrices.B_matrix\n",
    "    gu_fru_matrices = GuTransMatrix(N=8, measure=\"fourier\", fourier_type=\"fru\")\n",
    "    gu_A, gu_B = gu_fru_matrices.A_matrix, gu_fru_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-1.        -0.        -1.4142135  0.        -1.4142135  0.\n",
      "  -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   0.         0.       ]\n",
      " [-1.4142135  0.        -2.        -3.1415927 -2.         0.\n",
      "  -2.         0.       ]\n",
      " [ 0.         0.         3.1415927  0.         0.         0.\n",
      "   0.         0.       ]\n",
      " [-1.4142135  0.        -2.         0.        -2.        -6.2831855\n",
      "  -2.         0.       ]\n",
      " [ 0.         0.         0.         0.         6.2831855  0.\n",
      "   0.         0.       ]\n",
      " [-1.4142135  0.        -2.         0.        -2.         0.\n",
      "  -2.        -9.424778 ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   9.424778   0.       ]]\n",
      "Gu's A:\n",
      " [[-1.         0.        -1.4142135  0.        -1.4142135  0.\n",
      "  -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999 -3.1415927 -1.9999999  0.\n",
      "  -1.9999999  0.       ]\n",
      " [ 0.         0.         3.1415927  0.         0.         0.\n",
      "   0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999  0.        -1.9999999 -6.2831855\n",
      "  -1.9999999  0.       ]\n",
      " [ 0.         0.         0.         0.         6.2831855  0.\n",
      "   0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999  0.        -1.9999999  0.\n",
      "  -1.9999999 -9.424778 ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   9.424778   0.       ]]\n",
      "B:\n",
      " [[1.       ]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]]\n",
      "Gu's B:\n",
      " [[1.       ]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]]\n"
     ]
    }
   ],
   "source": [
    "test_FRU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated Fourier (FouT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FouT():\n",
    "    fout_matrices = TransMatrix(N=8, measure=\"fourier\", fourier_type=\"fout\")\n",
    "    A, B = fout_matrices.A_matrix, fout_matrices.B_matrix\n",
    "    gu_fout_matrices = GuTransMatrix(N=8, measure=\"fourier\", fourier_type=\"fout\")\n",
    "    gu_A, gu_B = gu_fout_matrices.A_matrix, gu_fout_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -2.         -0.         -2.828427    0.         -2.828427    0.\n",
      "   -2.828427    0.       ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.828427    0.         -4.         -6.2831855  -4.          0.\n",
      "   -4.          0.       ]\n",
      " [  0.          0.          6.2831855   0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.828427    0.         -4.          0.         -4.        -12.566371\n",
      "   -4.          0.       ]\n",
      " [  0.          0.          0.          0.         12.566371    0.\n",
      "    0.          0.       ]\n",
      " [ -2.828427    0.         -4.          0.         -4.          0.\n",
      "   -4.        -18.849556 ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "   18.849556    0.       ]]\n",
      "Gu's A:\n",
      " [[ -2.          0.         -2.828427    0.         -2.828427    0.\n",
      "   -2.828427    0.       ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.828427    0.         -3.9999998  -6.2831855  -3.9999998   0.\n",
      "   -3.9999998   0.       ]\n",
      " [  0.          0.          6.2831855   0.          0.          0.\n",
      "    0.          0.       ]\n",
      " [ -2.828427    0.         -3.9999998   0.         -3.9999998 -12.566371\n",
      "   -3.9999998   0.       ]\n",
      " [  0.          0.          0.          0.         12.566371    0.\n",
      "    0.          0.       ]\n",
      " [ -2.828427    0.         -3.9999998   0.         -3.9999998   0.\n",
      "   -3.9999998 -18.849556 ]\n",
      " [  0.          0.          0.          0.          0.          0.\n",
      "   18.849556    0.       ]]\n",
      "B:\n",
      " [[2.      ]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]]\n",
      "Gu's B:\n",
      " [[2.      ]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]]\n"
     ]
    }
   ],
   "source": [
    "test_FouT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier With Decay (FourD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FourD():\n",
    "    fourd_matrices = TransMatrix(N=8, measure=\"fourier\", fourier_type=\"fourd\")\n",
    "    A, B = fourd_matrices.A_matrix, fourd_matrices.B_matrix\n",
    "    gu_fourd_matrices = GuTransMatrix(N=8, measure=\"fourier\", fourier_type=\"fourd\")\n",
    "    gu_A, gu_B = gu_fourd_matrices.A_matrix, gu_fourd_matrices.B_matrix\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-0.5        -0.         -0.70710677  0.         -0.70710677  0.\n",
      "  -0.70710677  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.70710677  0.         -1.         -3.1415927  -1.          0.\n",
      "  -1.          0.        ]\n",
      " [ 0.          0.          3.1415927   0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.70710677  0.         -1.          0.         -1.         -6.2831855\n",
      "  -1.          0.        ]\n",
      " [ 0.          0.          0.          0.          6.2831855   0.\n",
      "   0.          0.        ]\n",
      " [-0.70710677  0.         -1.          0.         -1.          0.\n",
      "  -1.         -9.424778  ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   9.424778    0.        ]]\n",
      "Gu's A:\n",
      " [[-0.5         0.         -0.70710677  0.         -0.70710677  0.\n",
      "  -0.70710677  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.70710677  0.         -0.99999994 -3.1415927  -0.99999994  0.\n",
      "  -0.99999994  0.        ]\n",
      " [ 0.          0.          3.1415927   0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.70710677  0.         -0.99999994  0.         -0.99999994 -6.2831855\n",
      "  -0.99999994  0.        ]\n",
      " [ 0.          0.          0.          0.          6.2831855   0.\n",
      "   0.          0.        ]\n",
      " [-0.70710677  0.         -0.99999994  0.         -0.99999994  0.\n",
      "  -0.99999994 -9.424778  ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   9.424778    0.        ]]\n",
      "B:\n",
      " [[0.5       ]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]]\n",
      "Gu's B:\n",
      " [[0.5       ]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "test_FourD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities For Gu HiPPO Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_up(a, s=None, drop=True, dim=0):\n",
    "    assert dim == 0\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(a[0, ...])\n",
    "    s = s.unsqueeze(dim)\n",
    "    if drop:\n",
    "        a = a[:-1, ...]\n",
    "    return torch.cat((s, a), dim=dim)\n",
    "\n",
    "def interleave(a, b, uneven=False, dim=0):\n",
    "    \"\"\" Interleave two tensors of same shape \"\"\"\n",
    "    # assert(a.shape == b.shape)\n",
    "    assert dim == 0 # TODO temporary to make handling uneven case easier\n",
    "    if dim < 0:\n",
    "        dim = N + dim\n",
    "    if uneven:\n",
    "        a_ = a[-1:, ...]\n",
    "        a = a[:-1, ...]\n",
    "    c = torch.stack((a, b), dim+1)\n",
    "    out_shape = list(a.shape)\n",
    "    out_shape[dim] *= 2\n",
    "    c = c.view(out_shape)\n",
    "    if uneven:\n",
    "        c = torch.cat((c, a_), dim=dim)\n",
    "    return c\n",
    "\n",
    "def batch_mult(A, u, has_batch=None):\n",
    "    \"\"\" Matrix mult A @ u with special case to save memory if u has additional batch dim\n",
    "\n",
    "    The batch dimension is assumed to be the second dimension\n",
    "    A : (L, ..., N, N)\n",
    "    u : (L, [B], ..., N)\n",
    "    has_batch: True, False, or None. If None, determined automatically\n",
    "\n",
    "    Output:\n",
    "    x : (L, [B], ..., N)\n",
    "      A @ u broadcasted appropriately\n",
    "    \"\"\"\n",
    "\n",
    "    if has_batch is None:\n",
    "        has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    if has_batch:\n",
    "        u = u.permute([0] + list(range(2, len(u.shape))) + [1])\n",
    "    else:\n",
    "        u = u.unsqueeze(-1)\n",
    "    v = (A @ u)\n",
    "    if has_batch:\n",
    "        v = v.permute([0] + [len(u.shape)-1] + list(range(1, len(u.shape)-1)))\n",
    "    else:\n",
    "        v = v[..., 0]\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "### Main unrolling functions\n",
    "\n",
    "def unroll(A, u):\n",
    "    \"\"\"\n",
    "    A : (..., N, N) # TODO I think this can't take batch dimension?\n",
    "    u : (L, ..., N)\n",
    "    output : x (..., N) # TODO a lot of these shapes are wrong\n",
    "    x[i, ...] = A^{i} @ u[0, ...] + ... + A @ u[i-1, ...] + u[i, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    m = u.new_zeros(u.shape[1:])\n",
    "    outputs = []\n",
    "    for u_ in torch.unbind(u, dim=0):\n",
    "        m = F.linear(m, A) + u_\n",
    "        outputs.append(m)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive(A, u):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of unroll. \"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_(A, u):\n",
    "        if u.shape[0] == 1:\n",
    "            return u\n",
    "\n",
    "        u_evens = u[0::2, ...]\n",
    "        u_odds = u[1::2, ...]\n",
    "\n",
    "        # u2 = F.linear(u_evens, A) + u_odds\n",
    "        u2 = (A @ u_evens.unsqueeze(-1)).squeeze(-1) + u_odds\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_odds = parallel_unroll_recursive_(A2, u2)\n",
    "        # x_evens = F.linear(shift_up(x_odds), A) + u_evens\n",
    "        x_evens = (A @ shift_up(x_odds).unsqueeze(-1)).squeeze(-1) + u_evens\n",
    "\n",
    "        x = interleave(x_evens, x_odds, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n)/math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N-u.shape[0],) + u.shape[1:] )), dim=0)\n",
    "\n",
    "    return parallel_unroll_recursive_(A, u)[:n, ...]\n",
    "\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive_br(A, u):\n",
    "    \"\"\" Same as parallel_unroll_recursive but uses bit reversal for locality. \"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_br_(A, u):\n",
    "        n = u.shape[0]\n",
    "        if n == 1:\n",
    "            return u\n",
    "\n",
    "        m = n//2\n",
    "        u_0 = u[:m, ...]\n",
    "        u_1 = u[m:, ...]\n",
    "\n",
    "        u2 = F.linear(u_0, A) + u_1\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_1 = parallel_unroll_recursive_br_(A2, u2)\n",
    "        x_0 = F.linear(shift_up(x_1), A) + u_0\n",
    "\n",
    "        # x = torch.cat((x_0, x_1), dim=0) # is there a way to do this with cat?\n",
    "        x = interleave(x_0, x_1, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n)/math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N-u.shape[0],) + u.shape[1:] )), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    x = parallel_unroll_recursive_br_(A, u)\n",
    "    return x[:n, ...]\n",
    "\n",
    "def parallel_unroll_iterative(A, u):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of unroll, implemented iteratively \"\"\"\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n)/math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N-u.shape[0],) + u.shape[1:] )), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    # Main recursive loop, flattened\n",
    "    us = [] # stores the u_0 terms in the recursive version\n",
    "    N_ = N\n",
    "    As = [] # stores the A matrices\n",
    "    for l in range(m):\n",
    "        N_ = N_ // 2\n",
    "        As.append(A)\n",
    "        u_0 = u[:N_, ...]\n",
    "        us.append(u_0)\n",
    "        u = F.linear(u_0, A) + u[N_:, ...]\n",
    "        A = A @ A\n",
    "    x_0 = []\n",
    "    x = u # x_1\n",
    "    for l in range(m-1, -1, -1):\n",
    "        x_0 = F.linear(shift_up(x), As[l]) + us[l]\n",
    "        x = interleave(x_0, x, dim=0)\n",
    "\n",
    "    return x[:n, ...]\n",
    "\n",
    "\n",
    "def variable_unroll_sequential(A, u, s=None, variable=True):\n",
    "    \"\"\" Unroll with variable (in time/length) transitions A.\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] @ s + A[i..1] @ u[0] + ... + A[i] @ u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        # s = F.linear(s, A_) + u_\n",
    "        s = batch_mult(A_.unsqueeze(0), s.unsqueeze(0), has_batch)[0]\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def variable_unroll(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of variable_unroll. \"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        return variable_unroll_sequential(A, u, s, variable)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1  = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1  = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = A_1 @ A_0_\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll(A_10, u_10, s, variable, recurse_limit)\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "\n",
    "    x = interleave(x_0, x_1, uneven, dim=0) # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "def variable_unroll_general_sequential(A, u, s, op, variable=True):\n",
    "    \"\"\" Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        s = op(A_, s)\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "def variable_unroll_matrix_sequential(A, u, s=None, variable=True):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    return variable_unroll_general_sequential(A, u, s, op, variable=True)\n",
    "\n",
    "def variable_unroll_toeplitz_sequential(A, u, s=None, variable=True, pad=False):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        ret = variable_unroll_general_sequential(A, u, s, triangular_toeplitz_multiply_padded, variable=True)\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    return variable_unroll_general_sequential(A, u, s, triangular_toeplitz_multiply, variable=True)\n",
    "\n",
    "\n",
    "\n",
    "### General parallel scan functions with generic binary composition operators\n",
    "\n",
    "def variable_unroll_general(A, u, s, op, compose_op=None, sequential_op=None, variable=True, recurse_limit=16):\n",
    "    \"\"\" Bottom-up divide-and-conquer version of variable_unroll.\n",
    "\n",
    "    compose is an optional function that defines how to compose A without multiplying by a leaf u\n",
    "    \"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        if sequential_op is None:\n",
    "            sequential_op = op\n",
    "        return variable_unroll_general_sequential(A, u, s, sequential_op, variable)\n",
    "\n",
    "    if compose_op is None:\n",
    "        compose_op = op\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1 = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1 = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = op(A_1, u_0_) # batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = compose_op(A_1, A_0_)\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll_general(A_10, u_10, s, op, compose_op, sequential_op, variable=variable, recurse_limit=recurse_limit)\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = op(A_0, x_0) # batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "\n",
    "    x = interleave(x_0, x_1, uneven, dim=0) # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "def variable_unroll_matrix(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "    op = lambda x, y: batch_mult(x, y, has_batch)\n",
    "    sequential_op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    matmul = lambda x, y: x @ y\n",
    "    return variable_unroll_general(A, u, s, op, compose_op=matmul, sequential_op=sequential_op, variable=variable, recurse_limit=recurse_limit)\n",
    "\n",
    "def variable_unroll_toeplitz(A, u, s=None, variable=True, recurse_limit=8, pad=False):\n",
    "    \"\"\" Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (L, [B], ..., N) same shape as u\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "    # Add the batch dimension to A if necessary\n",
    "    A_batch_dims = len(A.shape) - int(variable)\n",
    "    u_batch_dims = len(u.shape)-1\n",
    "    if u_batch_dims > A_batch_dims:\n",
    "        # assert u_batch_dims == A_batch_dims + 1\n",
    "        if variable:\n",
    "            while len(A.shape) < len(u.shape):\n",
    "                A = A.unsqueeze(1)\n",
    "        # else:\n",
    "        #     A = A.unsqueeze(0)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        op = triangular_toeplitz_multiply_padded\n",
    "        ret = variable_unroll_general(A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit)\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    op = triangular_toeplitz_multiply\n",
    "    ret = variable_unroll_general(A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gu's HiPPO LegT Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LegT(nn.Module):\n",
    "    def __init__(self, N, dt=1.0, discretization=\"bilinear\", lambda_n=1.0):\n",
    "        \"\"\"\n",
    "        N: the order of the HiPPO projection\n",
    "        dt: discretization step size - should be roughly inverse to the length of the sequence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        # A, B = transition('lmu', N)\n",
    "        legt_matrices = GuTransMatrix(N=N, measure=\"legt\", lambda_n=lambda_n)\n",
    "        A = legt_matrices.A_matrix\n",
    "        B = legt_matrices.B_matrix\n",
    "        C = np.ones((1, N))\n",
    "        D = np.zeros((1,))\n",
    "        # dt, discretization options\n",
    "        A, B, _, _, _ = signal.cont2discrete((A, B, C, D), dt=dt, method=discretization)\n",
    "\n",
    "        B = B.squeeze(-1)\n",
    "\n",
    "        self.register_buffer(\"A\", torch.Tensor(A))  # (N, N)\n",
    "        self.register_buffer(\"B\", torch.Tensor(B))  # (N,)\n",
    "\n",
    "        # vals = np.linspace(0.0, 1.0, 1./dt)\n",
    "        vals = np.arange(0.0, 1.0, dt)\n",
    "        self.eval_matrix = torch.Tensor(\n",
    "            ss.eval_legendre(np.arange(N)[:, None], 1 - 2 * vals).T\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = inputs * self.B  # (length, ..., N)\n",
    "\n",
    "        c = torch.zeros(u.shape[1:])\n",
    "        cs = []\n",
    "        for f in inputs:\n",
    "            c = F.linear(c, self.A) + self.B * f\n",
    "            # print(f\"f:\\n{f}\")\n",
    "            cs.append(c)\n",
    "        return torch.stack(cs, dim=0)\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        return (self.eval_matrix @ c.unsqueeze(-1)).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gu's Scale invariant HiPPO LegS Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LegS(nn.Module):\n",
    "    \"\"\"Vanilla HiPPO-LegS model (scale invariant instead of time invariant)\"\"\"\n",
    "\n",
    "    def __init__(self, N, max_length=1024, measure=\"legs\", discretization=\"bilinear\"):\n",
    "        \"\"\"\n",
    "        max_length: maximum sequence length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        legs_matrices = GuTransMatrix(N=self.N, measure=measure)\n",
    "        A = legs_matrices.A_matrix\n",
    "        B = legs_matrices.B_matrix\n",
    "        # A, B = transition(measure, N)\n",
    "        B = B.squeeze(-1)\n",
    "        A_stacked = np.empty((max_length, N, N), dtype=A.dtype)\n",
    "        B_stacked = np.empty((max_length, N), dtype=B.dtype)\n",
    "        for t in range(1, max_length + 1):\n",
    "            At = A / t\n",
    "            Bt = B / t\n",
    "            if discretization == \"forward\":\n",
    "                A_stacked[t - 1] = np.eye(N) + At\n",
    "                B_stacked[t - 1] = Bt\n",
    "            elif discretization == \"backward\":\n",
    "                A_stacked[t - 1] = la.solve_triangular(\n",
    "                    np.eye(N) - At, np.eye(N), lower=True\n",
    "                )\n",
    "                B_stacked[t - 1] = la.solve_triangular(np.eye(N) - At, Bt, lower=True)\n",
    "            elif discretization == \"bilinear\":\n",
    "                alpha = 0.5\n",
    "                A_stacked[t - 1] = np.linalg.lstsq(\n",
    "                    np.eye(N) - (At * alpha), np.eye(N) + (At * alpha), rcond=None\n",
    "                )[\n",
    "                    0\n",
    "                ]  # TODO: Referencing this: https://stackoverflow.com/questions/64527098/numpy-linalg-linalgerror-singular-matrix-error-when-trying-to-solve\n",
    "                B_stacked[t - 1] = np.linalg.lstsq(\n",
    "                    np.eye(N) - (At * alpha), Bt, rcond=None\n",
    "                )[0]\n",
    "            else:  # ZOH\n",
    "                A_stacked[t - 1] = la.expm(A * (math.log(t + 1) - math.log(t)))\n",
    "                B_stacked[t - 1] = la.solve_triangular(\n",
    "                    A, A_stacked[t - 1] @ B - B, lower=True\n",
    "                )\n",
    "        self.A_stacked = torch.Tensor(A_stacked.copy())  # (max_length, N, N)\n",
    "        self.B_stacked = torch.Tensor(B_stacked.copy())  # (max_length, N)\n",
    "        vals = np.linspace(0.0, 1.0, max_length)\n",
    "        self.eval_matrix = torch.from_numpy(\n",
    "            np.asarray(\n",
    "                ((B[:, None] * ss.eval_legendre(np.arange(N)[:, None], 2 * vals - 1)).T)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, fast=False):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "        result = None\n",
    "\n",
    "        L = inputs.shape[0]\n",
    "\n",
    "        u = inputs.unsqueeze(-1)\n",
    "        u = torch.transpose(u, 0, -2)\n",
    "        u = u * self.B_stacked[:L]  # c_k = A @ c_{k-1} + B @ f_k\n",
    "        print(f\"u - Gu: {u}\")\n",
    "        my_b = torch.Tensor(\n",
    "            [\n",
    "                [6.6666657e-01],\n",
    "                [5.7735050e-01],\n",
    "                [1.4907140e-01],\n",
    "                [-2.3096800e-07],\n",
    "                [-2.7939677e-09],\n",
    "                [2.9616058e-07],\n",
    "                [-2.2817403e-08],\n",
    "                [-8.1490725e-08],\n",
    "            ]\n",
    "        )\n",
    "        u = torch.transpose(u, 0, -2)  # (length, ..., N)\n",
    "\n",
    "        # print(f\"A_stacked: {self.A_stacked[:L]}\")\n",
    "        # print(f\"B_stacked: {self.B_stacked[:L]}\")\n",
    "\n",
    "        if fast:\n",
    "            result = variable_unroll_matrix(self.A_stacked[:L], u)\n",
    "\n",
    "        else:\n",
    "            result = variable_unroll_matrix_sequential(self.A_stacked[:L], u)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        a = self.eval_matrix @ c.unsqueeze(-1)\n",
    "        return a.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Of General HiPPO Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO(jnn.Module):\n",
    "    \"\"\"\n",
    "    class that constructs HiPPO model using the defined measure.\n",
    "\n",
    "    Args:\n",
    "        N (int): order of the HiPPO projection, aka the number of coefficients to describe the matrix\n",
    "        max_length (int): maximum sequence length to be input\n",
    "        measure (str): the measure used to define which way to instantiate the HiPPO matrix\n",
    "        step (float): step size used for descretization\n",
    "        GBT_alpha (float): represents which descretization transformation to use based off the alpha value\n",
    "        seq_L (int): length of the sequence to be used for training\n",
    "        v (str): choice of vectorized or non-vectorized function instantiation\n",
    "            - 'v': vectorized\n",
    "            - 'nv': non-vectorized\n",
    "        lambda_n (float): value associated with the tilt of legt\n",
    "            - 1: tilt on legt\n",
    "            - \\sqrt(2n+1)(-1)^{N}: tilt associated with the legendre memory unit (LMU)\n",
    "        fourier_type (str): choice of fourier measures\n",
    "            - fru: fourier recurrent unit measure (FRU) - 'fru'\n",
    "            - fout: truncated Fourier (FouT) - 'fout'\n",
    "            - fourd: decaying fourier transform - 'fourd'\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "    \"\"\"\n",
    "\n",
    "    N: int\n",
    "    max_length: int\n",
    "    step: float\n",
    "    GBT_alpha: float\n",
    "    seq_L: int\n",
    "    A: jnp.ndarray\n",
    "    B: jnp.ndarray\n",
    "    measure: str\n",
    "\n",
    "    def setup(self):\n",
    "        A = self.A\n",
    "        B = self.B\n",
    "        self.C = jnp.ones((self.N,))\n",
    "        self.D = jnp.zeros((1,))\n",
    "\n",
    "        if self.measure == \"legt\":\n",
    "            L = self.seq_L\n",
    "            vals = jnp.arange(0.0, 1.0, L)\n",
    "            # n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 1 - 2 * vals\n",
    "            self.eval_matrix = jax.scipy.special.lpmn_values(\n",
    "                m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "            ).T  # ss.eval_legendre(n, x).T\n",
    "\n",
    "        elif self.measure == \"lmu\":\n",
    "            raise NotImplementedError(\"LMU measure not implemented yet\")\n",
    "\n",
    "        elif self.measure == \"legs\":\n",
    "            L = self.max_length\n",
    "            vals = jnp.linspace(0.0, 1.0, L)\n",
    "            # n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 2 * vals - 1\n",
    "            self.eval_matrix = (\n",
    "                B[:, None]\n",
    "                * jax.scipy.special.lpmn_values(\n",
    "                    m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "                )\n",
    "            ).T  # ss.eval_legendre(n, x)).T\n",
    "\n",
    "        elif self.measure == \"lagt\":\n",
    "            raise NotImplementedError(\"Translated Laguerre measure not implemented yet\")\n",
    "\n",
    "        elif self.measure == \"fru\":\n",
    "            raise NotImplementedError(\n",
    "                \"Fourier Recurrent Unit measure not implemented yet\"\n",
    "            )\n",
    "\n",
    "        elif self.measure == \"fout\":\n",
    "            raise NotImplementedError(\"Translated Fourier measure not implemented yet\")\n",
    "\n",
    "        elif self.measure == \"fourd\":\n",
    "            raise NotImplementedError(\"Decaying Fourier measure not implemented yet\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"invalid measure\")\n",
    "\n",
    "    def __call__(self, f, init_state=None, t_step=0, kernel=False):\n",
    "        # print(f\"u shape:\\n{f.shape}\")\n",
    "        # print(f\"u:\\n{f}\")\n",
    "        if not kernel:\n",
    "            if init_state is None:\n",
    "                init_state = jnp.zeros((self.N, 1))\n",
    "\n",
    "            # Ab, Bb, Cb, Db = self.collect_SSM_vars(\n",
    "            #     self.A, self.B, self.C, self.D, f, t_step=t_step, alpha=self.GBT_alpha\n",
    "            # )\n",
    "            c_k, y_k, GBT_A, GBT_B = self.loop_SSM(\n",
    "                A=self.A,\n",
    "                B=self.B,\n",
    "                C=self.C,\n",
    "                D=self.D,\n",
    "                c_0=init_state,\n",
    "                f=f,\n",
    "                alpha=self.GBT_alpha,\n",
    "            )\n",
    "            # c_k, y_k = self.scan_SSM(Ab=Ab, Bb=Bb, Cb=Cb, Db=Db, c_0=init_state, f=f)\n",
    "\n",
    "        else:\n",
    "            Ab, Bb, Cb, Db = self.discretize(\n",
    "                self.A, self.B, self.C, self.D, step=self.step, alpha=self.GBT_alpha\n",
    "            )\n",
    "            c_k, y_k = self.causal_convolution(\n",
    "                f, self.K_conv(Ab, Bb, Cb, Db, L=self.max_length)\n",
    "            )\n",
    "\n",
    "        return c_k, y_k, GBT_A, GBT_B\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        \"\"\"\n",
    "        Uses coeffecients to reconstruct the signal\n",
    "\n",
    "        Args:\n",
    "            c (jnp.ndarray): coefficients of the HiPPO projection\n",
    "\n",
    "        Returns:\n",
    "            reconstructed signal\n",
    "        \"\"\"\n",
    "        return (self.eval_matrix @ jnp.expand_dims(c, -1)).squeeze(-1)\n",
    "\n",
    "    def discretize(self, A, B, C, D, step, alpha=0.5):\n",
    "        \"\"\"\n",
    "        function used for discretizing the HiPPO matrix\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            step (float): step size used for discretization\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "                - forward Euler corresponds to α = 0,\n",
    "                - backward Euler corresponds to α = 1,\n",
    "                - bilinear corresponds to α = 0.5,\n",
    "                - Zero-order Hold corresponds to α > 1\n",
    "        \"\"\"\n",
    "        I = jnp.eye(A.shape[0])\n",
    "        step_size = 1 / step\n",
    "        part1 = (I - (step_size * alpha * A)).astype(jnp.float32)\n",
    "        part2 = (I + (step_size * (1 - alpha) * A)).astype(jnp.float32)\n",
    "\n",
    "        GBT_A = jnp.linalg.lstsq(part1, part2, rcond=None)[0]\n",
    "\n",
    "        base_GBT_B = jnp.linalg.lstsq(part1, B, rcond=None)[0]\n",
    "        GBT_B = step_size * base_GBT_B\n",
    "\n",
    "        if alpha > 1:  # Zero-order Hold\n",
    "            GBT_A = jax.scipy.linalg.expm(step_size * A)\n",
    "            GBT_B = (jnp.linalg.inv(A) @ (jax.scipy.linalg.expm(step_size * A) - I)) @ B\n",
    "\n",
    "        return (\n",
    "            GBT_A.astype(jnp.float32),\n",
    "            GBT_B.astype(jnp.float32),\n",
    "            C.astype(jnp.float32),\n",
    "            D.astype(jnp.float32),\n",
    "        )\n",
    "\n",
    "    def collect_SSM_vars(self, A, B, C, D, f, t_step=0, alpha=0.5):\n",
    "        \"\"\"\n",
    "        turns the continuos HiPPO matrix components into discrete ones\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            f (jnp.ndarray): input signal\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "\n",
    "        Returns:\n",
    "            Ab (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Bb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Cb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Db (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "        \"\"\"\n",
    "        N = A.shape[0]\n",
    "\n",
    "        if t_step == 0:\n",
    "            L = f.shape[0]  # seq_L, 1\n",
    "            assert (\n",
    "                L == self.seq_L\n",
    "            ), f\"sequence length must match, currently {L} != {self.seq_L}\"\n",
    "            assert N == self.N, f\"Order number must match, currently {N} != {self.N}\"\n",
    "        else:\n",
    "            L = t_step\n",
    "            assert t_step >= 1, f\"time step must be greater than 0, currently {t_step}\"\n",
    "            assert N == self.N, f\"Order number must match, currently {N} != {self.N}\"\n",
    "\n",
    "        Ab, Bb, Cb, Db = self.discretize(A, B, C, D, step=L, alpha=alpha)\n",
    "\n",
    "        return Ab.astype(jnp.float32), Bb.astype(jnp.float32), Cb.astype(jnp.float32), Db.astype(jnp.float32)\n",
    "\n",
    "    def scan_SSM(self, Ad, Bd, Cd, Dd, c_0, f):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            f (jnp.ndarray): the input sequence\n",
    "            c_0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "\n",
    "        def step(c_k_1, f_k):\n",
    "            \"\"\"\n",
    "            Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "            Args:\n",
    "                c_k_1: previous hidden state\n",
    "                f_k: output from function f at, descritized, time step, k.\n",
    "                t:\n",
    "\n",
    "            Returns:\n",
    "                c_k: current hidden state\n",
    "                y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "            \"\"\"\n",
    "            part1 = Ad @ c_k_1\n",
    "            part2 = jnp.expand_dims((Bd @ f_k), -1)\n",
    "\n",
    "            c_k = part1 + part2\n",
    "            y_k = Cd @ c_k  # + (Db.T @ f_k)\n",
    "\n",
    "            return c_k, y_k\n",
    "\n",
    "        return jax.lax.scan(step, c_0, f)\n",
    "\n",
    "    def loop_SSM(self, A, B, C, D, c_0, f, alpha=0.5):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            f (jnp.ndarray): the input sequence\n",
    "            c_0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "        GBT_A_lst = []\n",
    "        GBT_B_lst = []\n",
    "        c_k_list = []\n",
    "        y_k_list = []\n",
    "        \n",
    "        c_k = c_0.copy()\n",
    "        for i in range(1, f.shape[0] + 1):\n",
    "            Ad_i, Bd_i, Cd_i, Dd_i = self.collect_SSM_vars(\n",
    "                A=A, B=B, C=C, D=D, f=f, t_step=i, alpha=alpha\n",
    "            )\n",
    "            c_k, y_k = self.loop_step(\n",
    "                Ad=Ad_i, Bd=Bd_i, Cd=Cd_i, Dd=Dd_i, c_k_i=c_k, f_k=f[i - 1][0]\n",
    "            )\n",
    "            c_k_list.append(c_k.copy())\n",
    "            y_k_list.append(y_k.copy())\n",
    "            GBT_A_lst.append(Ad_i.copy())\n",
    "            GBT_B_lst.append(Bd_i.copy())\n",
    "\n",
    "        return c_k_list, y_k_list, GBT_A_lst, GBT_B_lst\n",
    "\n",
    "    def loop_step(self, Ad, Bd, Cd, Dd, c_k_i, f_k):\n",
    "        \"\"\"\n",
    "        Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "        Args:\n",
    "            c_k_i: previous hidden state\n",
    "            f_k: output from function f at, descritized, time step, k.\n",
    "\n",
    "        Returns:\n",
    "            c_k: current hidden state\n",
    "            y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        part1 = Ad @ c_k_i\n",
    "        part2 = Bd * f_k\n",
    "        # print(f\"f_k: {f_k}\")\n",
    "        # print(f\"part1:\\n{part1}\\n\\npart2:\\n{part2}\\n\")\n",
    "        c_k = part1 + part2\n",
    "        y_k = Cd @ c_k  # + (Db.T @ f_k)\n",
    "\n",
    "        return c_k.astype(jnp.float32), y_k.astype(jnp.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hippo_legs_operator(hippo_legs, gu_hippo_legs, random_input, legs_key):\n",
    "    i = 0\n",
    "    x_tensor = torch.tensor(random_input, dtype=torch.float32)\n",
    "    x_jnp = jnp.asarray(x_tensor, dtype=jnp.float32)  # convert torch array to jax array\n",
    "    params = hippo_legs.init(legs_key, f=x_jnp, t_step=(x_jnp.shape[0]))\n",
    "    c_k_list, y_k_list, GBT_A_list, GBT_B_list = hippo_legs.apply(\n",
    "        params, f=x_jnp, t_step=(x_jnp.shape[0])\n",
    "    )\n",
    "    GU_c_k = gu_hippo_legs(x_tensor)\n",
    "    for i, c_k in enumerate(c_k_list):\n",
    "        g_c_k = GU_c_k[i][0]\n",
    "        gu = torch.unsqueeze(g_c_k, -1)\n",
    "        gu_c = jnp.asarray(gu, dtype=jnp.float32)  # convert torch array to jax array\n",
    "        print(f\"HiPPO LegS Test: {jnp.allclose(c_k, gu_c, rtol=1e-04, atol=1e-06)}\")\n",
    "        print(f\"c_k:\\n{c_k}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # N = 256\n",
    "    # L = 128\n",
    "\n",
    "    N = 8\n",
    "    L = 8\n",
    "\n",
    "    np.random.seed(1701)\n",
    "    x_np = np.array(\n",
    "        [\n",
    "            [0.3527],\n",
    "            [0.6617],\n",
    "            [0.2434],\n",
    "            [0.6674],\n",
    "            [1.2293],\n",
    "            [0.0964],\n",
    "            [-2.2756],\n",
    "            [0.5618],\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "    # x = torch.randn(L, 1)\n",
    "    x = torch.tensor(x_np, dtype=torch.float32)\n",
    "\n",
    "    print(f\"THIS IS X:\\n{x}\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegT model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"\\nTesting HiPPO LegT model\")\n",
    "    hippo_legt = HiPPO_LegT(N, dt=1.0 / L)\n",
    "\n",
    "    c_k = hippo_legt(x)\n",
    "\n",
    "    # print(f\"Gu's Coeffiecients for LegT:\\n{c_k}\")\n",
    "    # print(f\"Gu's Coeffiecient shapes for LegT:\\n{c_k.shape}\")\n",
    "\n",
    "    # z = hippo_legt.reconstruct(c_k)\n",
    "    # print(f\"Gu's Reconstruction for LegT:\\n{z}\")\n",
    "    # print(f\"Gu's Reconstruction shape for LegT:\\n{z.shape}\")\n",
    "\n",
    "    # mse = loss(z[-1, 0, :L], x.squeeze(-1))\n",
    "    # print(f\"h-MSE shape:\\n{mse}\")\n",
    "    # print(f\"end of test for HiPPO LegT model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegS model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"\\nTesting HiPPO LegS model\")\n",
    "    gu_hippo_legs = HiPPO_LegS(N, max_length=L)  # The Gu's\n",
    "\n",
    "    c_k = gu_hippo_legs(x, fast=True)\n",
    "\n",
    "    print(f\"Gu's Coeffiecients  for LegS:\\n{c_k}\")\n",
    "    print(f\"Gu's Coeffiecient shapes for LegS:\\n{c_k.shape}\")\n",
    "\n",
    "    # z = hippo_legs.reconstruct(c_k)\n",
    "\n",
    "    # print(f\"Gu's Reconstruction for LegS:\\n{z}\")\n",
    "    # print(f\"Gu's Reconstruction shape for LegS:\\n{z.shape}\")\n",
    "\n",
    "    # print(y-z)\n",
    "    print(f\"end of test for HiPPO LegS model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test Generic HiPPO model --------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    the_measure = \"legs\"\n",
    "    print(f\"\\nTesting BRYANS HiPPO-{the_measure} model\")\n",
    "    legs_matrices = TransMatrix(N=N, measure=the_measure)\n",
    "    A = legs_matrices.A_matrix\n",
    "    B = legs_matrices.B_matrix\n",
    "    hippo_LegS_B = HiPPO(\n",
    "        N=N,\n",
    "        max_length=L,\n",
    "        step=1.0 / L,\n",
    "        GBT_alpha=0.5,\n",
    "        seq_L=L,\n",
    "        A=A,\n",
    "        B=B,\n",
    "        measure=the_measure,\n",
    "    )  # Bryan's\n",
    "    \n",
    "    print(f\"Bryan's Coeffiecients for HiPPO-{the_measure}\")\n",
    "    test_hippo_legs_operator(hippo_legs=hippo_LegS_B, \n",
    "                             gu_hippo_legs=gu_hippo_legs, \n",
    "                             random_input=x_np, \n",
    "                             legs_key=key2)\n",
    "    \n",
    "    # y_legs = hippo_LegS_B.apply(\n",
    "    #     {\"params\": params}, c_k, method=hippo_LegS_B.reconstruct\n",
    "    # )\n",
    "\n",
    "    # print(f\"Bryan's Reconstruction for HiPPO-{the_measure}:\\n{y_legs}\")\n",
    "    # print(f\"Bryan's Reconstruction shape for HiPPO-{the_measure}:\\n{y_legs.shape}\")\n",
    "\n",
    "    print(f\"end of test for HiPPO-{the_measure} model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS X:\n",
      "tensor([[ 0.3527],\n",
      "        [ 0.6617],\n",
      "        [ 0.2434],\n",
      "        [ 0.6674],\n",
      "        [ 1.2293],\n",
      "        [ 0.0964],\n",
      "        [-2.2756],\n",
      "        [ 0.5618]])\n",
      "\n",
      "Testing HiPPO LegT model\n",
      "\n",
      "Testing HiPPO LegS model\n",
      "u - Gu: tensor([[[ 2.3513e-01,  2.0363e-01,  5.2577e-02, -6.8057e-09,  1.1977e-08,\n",
      "          -1.0683e-09, -8.7712e-09,  4.9337e-09],\n",
      "         [ 2.6468e-01,  3.0563e-01,  1.6910e-01,  5.0020e-02,  6.3019e-03,\n",
      "           1.8810e-09, -1.1216e-08,  6.1723e-09],\n",
      "         [ 6.9543e-02,  9.0339e-02,  6.4793e-02,  3.0666e-02,  9.4831e-03,\n",
      "           1.7473e-03,  1.4611e-04, -6.3698e-11],\n",
      "         [ 1.4831e-01,  2.0551e-01,  1.6883e-01,  9.9882e-02,  4.3560e-02,\n",
      "           1.3759e-02,  2.9916e-03,  4.0169e-04],\n",
      "         [ 2.2351e-01,  3.2261e-01,  2.8834e-01,  1.9495e-01,  1.0316e-01,\n",
      "           4.2767e-02,  1.3674e-02,  3.2641e-03],\n",
      "         [ 1.4831e-02,  2.2018e-02,  2.0845e-02,  1.5415e-02,  9.2537e-03,\n",
      "           4.5468e-03,  1.8211e-03,  5.8684e-04],\n",
      "         [-3.0341e-01, -4.5984e-01, -4.5396e-01, -3.5809e-01, -2.3507e-01,\n",
      "          -1.2994e-01, -6.0541e-02, -2.3648e-02],\n",
      "         [ 6.6094e-02,  1.0176e-01,  1.0371e-01,  8.5900e-02,  6.0296e-02,\n",
      "           3.6360e-02,  1.8904e-02,  8.4611e-03]]])\n",
      "Gu's Coeffiecients  for LegS:\n",
      "tensor([[[ 2.3513e-01,  2.0363e-01,  5.2577e-02, -6.8057e-09,  1.1977e-08,\n",
      "          -1.0683e-09, -8.7712e-09,  4.9337e-09]],\n",
      "\n",
      "        [[ 4.0576e-01,  2.6490e-01, -3.3701e-02, -5.6627e-02, -7.1343e-03,\n",
      "          -5.3342e-09, -1.3616e-08,  7.8134e-09]],\n",
      "\n",
      "        [[ 3.5937e-01,  7.2189e-02, -2.2545e-01, -8.6126e-02,  2.5252e-02,\n",
      "           1.1226e-02,  9.3872e-04, -2.7088e-09]],\n",
      "\n",
      "        [[ 4.2782e-01,  1.3816e-01, -6.5221e-02,  1.5500e-01,  1.5606e-01,\n",
      "           2.6968e-02, -4.6502e-03, -1.5067e-03]],\n",
      "\n",
      "        [[ 5.7355e-01,  3.0244e-01,  8.4267e-02,  1.8955e-01,  7.2271e-07,\n",
      "          -1.4422e-01, -7.2802e-02, -1.3106e-02]],\n",
      "\n",
      "        [[ 5.0014e-01,  1.0705e-01, -1.8648e-01, -1.3038e-01, -2.6791e-01,\n",
      "          -1.7971e-01,  4.9144e-02,  8.3597e-02]],\n",
      "\n",
      "        [[ 1.3004e-01, -4.8061e-01, -7.1708e-01, -4.4194e-01, -2.8475e-01,\n",
      "           3.7278e-02,  2.1051e-01,  5.7035e-02]],\n",
      "\n",
      "        [[ 1.8084e-01, -2.9561e-01, -2.3676e-01,  3.0236e-01,  5.1647e-01,\n",
      "           6.1457e-01,  3.6490e-01, -2.4947e-02]]])\n",
      "Gu's Coeffiecient shapes for LegS:\n",
      "torch.Size([8, 1, 8])\n",
      "end of test for HiPPO LegS model\n",
      "\n",
      "Testing BRYANS HiPPO-legs model\n",
      "Bryan's Coeffiecients for HiPPO-legs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16861/301525629.py:46: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:172.)\n",
      "  self.eval_matrix = torch.from_numpy(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u - Gu: tensor([[[ 2.3513e-01,  2.0363e-01,  5.2577e-02, -6.8057e-09,  1.1977e-08,\n",
      "          -1.0683e-09, -8.7712e-09,  4.9337e-09],\n",
      "         [ 2.6468e-01,  3.0563e-01,  1.6910e-01,  5.0020e-02,  6.3019e-03,\n",
      "           1.8810e-09, -1.1216e-08,  6.1723e-09],\n",
      "         [ 6.9543e-02,  9.0339e-02,  6.4793e-02,  3.0666e-02,  9.4831e-03,\n",
      "           1.7473e-03,  1.4611e-04, -6.3698e-11],\n",
      "         [ 1.4831e-01,  2.0551e-01,  1.6883e-01,  9.9882e-02,  4.3560e-02,\n",
      "           1.3759e-02,  2.9916e-03,  4.0169e-04],\n",
      "         [ 2.2351e-01,  3.2261e-01,  2.8834e-01,  1.9495e-01,  1.0316e-01,\n",
      "           4.2767e-02,  1.3674e-02,  3.2641e-03],\n",
      "         [ 1.4831e-02,  2.2018e-02,  2.0845e-02,  1.5415e-02,  9.2537e-03,\n",
      "           4.5468e-03,  1.8211e-03,  5.8684e-04],\n",
      "         [-3.0341e-01, -4.5984e-01, -4.5396e-01, -3.5809e-01, -2.3507e-01,\n",
      "          -1.2994e-01, -6.0541e-02, -2.3648e-02],\n",
      "         [ 6.6094e-02,  1.0176e-01,  1.0371e-01,  8.5900e-02,  6.0296e-02,\n",
      "           3.6360e-02,  1.8904e-02,  8.4611e-03]]])\n",
      "HiPPO LegS Test: True\n",
      "c_k:\n",
      "[[ 2.3513339e-01]\n",
      " [ 2.0363148e-01]\n",
      " [ 5.2577388e-02]\n",
      " [ 9.9200193e-08]\n",
      " [-3.3176224e-08]\n",
      " [ 2.5292765e-08]\n",
      " [-5.1078246e-08]\n",
      " [ 8.7046530e-09]]\n",
      "\n",
      "HiPPO LegS Test: True\n",
      "c_k:\n",
      "[[ 4.0576005e-01]\n",
      " [ 2.6489991e-01]\n",
      " [-3.3700705e-02]\n",
      " [-5.6626637e-02]\n",
      " [-7.1344255e-03]\n",
      " [-5.8341989e-09]\n",
      " [-7.9896303e-09]\n",
      " [ 1.1313681e-07]]\n",
      "\n",
      "HiPPO LegS Test: True\n",
      "c_k:\n",
      "[[ 3.5937142e-01]\n",
      " [ 7.2189435e-02]\n",
      " [-2.2544572e-01]\n",
      " [-8.6125545e-02]\n",
      " [ 2.5251910e-02]\n",
      " [ 1.1225651e-02]\n",
      " [ 9.3876204e-04]\n",
      " [-1.9131585e-08]]\n",
      "\n",
      "HiPPO LegS Test: True\n",
      "c_k:\n",
      "[[ 0.42782217]\n",
      " [ 0.13816187]\n",
      " [-0.06522089]\n",
      " [ 0.15499869]\n",
      " [ 0.15605724]\n",
      " [ 0.02696844]\n",
      " [-0.00465026]\n",
      " [-0.00150671]]\n",
      "\n",
      "HiPPO LegS Test: True\n",
      "c_k:\n",
      "[[ 5.7354540e-01]\n",
      " [ 3.0244142e-01]\n",
      " [ 8.4267408e-02]\n",
      " [ 1.8954913e-01]\n",
      " [ 7.0035458e-07]\n",
      " [-1.4421937e-01]\n",
      " [-7.2801977e-02]\n",
      " [-1.3105482e-02]]\n",
      "\n",
      "HiPPO LegS Test: True\n",
      "c_k:\n",
      "[[ 0.5001384 ]\n",
      " [ 0.10704855]\n",
      " [-0.18648377]\n",
      " [-0.13037537]\n",
      " [-0.26790634]\n",
      " [-0.17971008]\n",
      " [ 0.04914434]\n",
      " [ 0.08359747]]\n",
      "\n",
      "HiPPO LegS Test: True\n",
      "c_k:\n",
      "[[ 0.13003996]\n",
      " [-0.4806142 ]\n",
      " [-0.7170837 ]\n",
      " [-0.44194013]\n",
      " [-0.28475064]\n",
      " [ 0.03727753]\n",
      " [ 0.21050858]\n",
      " [ 0.05703524]]\n",
      "\n",
      "HiPPO LegS Test: True\n",
      "c_k:\n",
      "[[ 0.1808354 ]\n",
      " [-0.29560658]\n",
      " [-0.23676264]\n",
      " [ 0.30235535]\n",
      " [ 0.51646614]\n",
      " [ 0.6145703 ]\n",
      " [ 0.36490217]\n",
      " [-0.02494737]]\n",
      "\n",
      "end of test for HiPPO-legs model\n"
     ]
    }
   ],
   "source": [
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('s4mer-pkg-jZnBSgjq-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a81e05d1d7f7eae781698b7c1b81c0d771335201ebad1d81045cb177cef974b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
