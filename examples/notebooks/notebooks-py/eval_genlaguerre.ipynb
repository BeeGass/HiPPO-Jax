{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiPPO Matrices\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Loading In Necessary Packages](#load-packages)\n",
    "* [Instantiate The HiPPO Matrix](#instantiate-the-hippo-matrix)\n",
    "    * [Translated Legendre (LegT)](#translated-legendre-legt)\n",
    "        * [LegT](#legt)\n",
    "        * [LMU](#lmu)\n",
    "    * [Translated Laguerre (LagT)](#translated-laguerre-lagt)\n",
    "    * [Scaled Legendre (LegS)](#scaled-legendre-legs)\n",
    "    * [Fourier Basis](#fourier-basis)\n",
    "        * [Fourier Recurrent Unit (FRU)](#fourier-recurrent-unit-fru)\n",
    "        * [Truncated Fourier (FouT)](#truncated-fourier-fout)\n",
    "        * [Fourier With Decay (FourD)](#fourier-with-decay-fourd)\n",
    "* [Gu's Linear Time Invariant (LTI) HiPPO Operator](#gus-hippo-legt-operator)\n",
    "* [Gu's Scale invariant (LSI) HiPPO Operator](#gus-scale-invariant-hippo-legs-operator)\n",
    "* [Implementation Of General HiPPO Operator](#implementation-of-general-hippo-operator)\n",
    "* [Test Generalized Bilinear Transform and Zero Order Hold Matrices](#test-generalized-bilinear-transform-and-zero-order-hold-matrices)\n",
    "    * [Testing Forward Euler on GBT matrices](#testing-forward-euler-transform-for-lti-and-lsi)\n",
    "    * [Testing Backward Euler on GBT matrices](#testing-backward-euler-transform-for-lti-and-lsi-on-legs-matrices)\n",
    "    * [Testing Bidirectional on GBT matrices](#testing-lti-and-lsi-operators-with-bidirectional-transform)\n",
    "    * [Testing ZOH on GBT matrices](#testing-zoh-transform-for-lti-and-lsi-on-legs-matrices)\n",
    "* [Testing HiPPO Operators](#test-hippo-operators)\n",
    "    * [Testing Forward Euler on HiPPO Operators](#testing-lti-and-lsi-operators-with-forward-euler-transform)\n",
    "    * [Testing Backward Euler on HiPPO Operators](#testing-lti-and-lsi-operators-with-backward-euler-transform)\n",
    "    * [Testing Bidirectional on HiPPO Operators](#testing-lti-and-lsi-operators-with-bidirectional-transform)\n",
    "    * [Testing ZOH on HiPPO Operators](#testing-lti-and-lsi-operators-with-zoh-transform)\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_path: /home/beegass/Documents/Coding/hippo-jax (copy)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../../../\"))\n",
    "print(f\"module_path: {module_path}\")\n",
    "if module_path not in sys.path:\n",
    "    print(f\"Adding {module_path} to sys.path\")\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "os.environ[\"TF_FORCE_UNIFIED_MEMORY\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax._src.scipy.special import _gen_associated_legendre, gammaln\n",
    "import einops\n",
    "from scipy import special as ss\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]\n",
      "The Device: gpu\n"
     ]
    }
   ],
   "source": [
    "print(jax.devices())\n",
    "print(f\"The Device: {jax.lib.xla_bridge.get_backend().platform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS enabled: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"MPS enabled: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(linewidth=150)\n",
    "np.set_printoptions(linewidth=150)\n",
    "jnp.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 10\n",
    "subkeys = jax.random.split(key, num=num_copies)\n",
    "key = subkeys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "# def genlaguerre_recurrence(n, alpha, x, max_n):\n",
    "#     \"\"\"\n",
    "#     Computes the generalized Laguerre polynomial of degree n with parameter alpha at point x using the recurrence relation.\n",
    "\n",
    "#     Args:\n",
    "#     n: int, the degree of the generalized Laguerre polynomial.\n",
    "#     alpha: float, the parameter of the generalized Laguerre polynomial.\n",
    "#     x: float, the point at which to evaluate the polynomial.\n",
    "#     max_n: int, the maximum degree of n in the batch.\n",
    "\n",
    "#     Returns:\n",
    "#     The value of the generalized Laguerre polynomial of degree n with parameter alpha at point x.\n",
    "#     \"\"\"\n",
    "#     # Initialize the array to store the generalized Laguerre polynomials for all degrees from 0 to max_n\n",
    "#     p = jnp.zeros((max_n + 1,) + x.shape)\n",
    "#     p = p.at[0].set(1.0)  # Set the 0th degree generalized Laguerre polynomial\n",
    "\n",
    "#     # Compute the generalized Laguerre polynomials for degrees 1 to max_n using the recurrence relation\n",
    "#     def body_fun(i, p):\n",
    "#         p_i = ((2 * i + alpha - 1 - x) * p[i - 1] - (i + alpha - 1) * p[i - 2]) / i\n",
    "#         return p.at[i].set(p_i)\n",
    "\n",
    "#     p = jax.lax.fori_loop(1, max_n + 1, body_fun, p)\n",
    "\n",
    "#     return p[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "# def eval_genlaguerre(n, alpha, x, out=None):\n",
    "#     \"\"\"\n",
    "#     Evaluates the generalized Laguerre polynomials of degrees specified in the input array n with parameter alpha at the points specified in the input array x.\n",
    "\n",
    "#     Args:\n",
    "#     n: array-like, the degrees of the generalized Laguerre polynomials.\n",
    "#     alpha: float, the parameter of the generalized Laguerre polynomials.\n",
    "#     x: array-like, the points at which to evaluate the polynomials.\n",
    "#     out: optional, an output array to store the results.\n",
    "\n",
    "#     Returns:\n",
    "#     An array containing the generalized Laguerre polynomial values of the specified degrees with parameter alpha at the specified points.\n",
    "#     \"\"\"\n",
    "#     n = jnp.asarray(n)\n",
    "#     x = jnp.asarray(x)\n",
    "#     max_n = n.max()\n",
    "\n",
    "#     if n.ndim == 1 and x.ndim == 1:\n",
    "#         p = jax.vmap(\n",
    "#             lambda ni: jax.vmap(\n",
    "#                 lambda xi: genlaguerre_recurrence(ni, alpha, xi, max_n)\n",
    "#             )(x)\n",
    "#         )(n)\n",
    "#         p = jnp.diagonal(\n",
    "#             p\n",
    "#         )  # Get the diagonal elements to match the scipy.signal.eval_genlaguerre output\n",
    "#     else:\n",
    "#         p = jax.vmap(\n",
    "#             lambda ni: jax.vmap(\n",
    "#                 lambda xi: genlaguerre_recurrence(ni, alpha, xi, max_n)\n",
    "#             )(x)\n",
    "#         )(n)\n",
    "\n",
    "#     if out is not None:\n",
    "#         out = jnp.asarray(out)\n",
    "#         out = jnp.copy(p, out=out)\n",
    "#         return out\n",
    "#     else:\n",
    "#         return jnp.squeeze(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genlaguerre_recurrence(n, alpha, x):\n",
    "    \"\"\"\n",
    "    Computes the generalized Laguerre polynomial of degree n with parameter alpha at point x using the recurrence relation.\n",
    "    \"\"\"\n",
    "\n",
    "    def body_fun(carry, _):\n",
    "        i, (p_im1, p_i) = carry\n",
    "        p_ip1 = ((2 * i + alpha + 1 - x) * p_i - (i + alpha) * p_im1) / (i + 1)\n",
    "        return (i + 1, (p_i, p_ip1)), None\n",
    "\n",
    "    _, (p_im1, p_i) = jax.lax.scan(body_fun, (1, (1.0, x)), None, n - 1)\n",
    "\n",
    "    return p_i if n > 0 else p_im1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_genlaguerre(n, alpha, x):\n",
    "    \"\"\"\n",
    "    Evaluates the generalized Laguerre polynomials of degrees specified in the input array n with parameter alpha at the points specified in the input array x.\n",
    "    \"\"\"\n",
    "    n = jnp.asarray(n)\n",
    "    x = jnp.asarray(x)\n",
    "\n",
    "    p = jax.vmap(lambda ni, xi: genlaguerre_recurrence(ni, alpha, xi))(n, x)\n",
    "\n",
    "    return jnp.squeeze(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval_genlaguerre():\n",
    "    alpha = 2.0\n",
    "    n = np.arange(10)\n",
    "\n",
    "    print(f\"n = {n}\")\n",
    "    print(f\"n shape = {n.shape}\")\n",
    "\n",
    "    x = np.linspace(-1, 1, n.shape[0])\n",
    "\n",
    "    print(f\"x = {x}\")\n",
    "    print(f\"x shape = {x.shape}\")\n",
    "\n",
    "    y_pred = eval_genlaguerre(n, alpha, x)\n",
    "    y = jnp.array(ss.eval_genlaguerre(n, alpha, x))\n",
    "\n",
    "    print(f\"y_pred = {y_pred}\")\n",
    "    print(f\"y_pred shape = {y_pred.shape}\")\n",
    "    print(f\"y = {y}\")\n",
    "    print(f\"y shape = {y.shape}\")\n",
    "\n",
    "    assert np.allclose(y_pred, y, rtol=1e-5, atol=1e-8), \"Results do not match\"\n",
    "    print(\"Results match\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also did this for evaluating Laguerre polynomials:\n",
    "\n",
    "```python\n",
    "def genlaguerre_recurrence(n, alpha, x, max_n):\n",
    "    \"\"\"\n",
    "    Computes the generalized Laguerre polynomial of degree n with parameter alpha at point x using the recurrence relation.\n",
    "\n",
    "    Args:\n",
    "    n: int, the degree of the generalized Laguerre polynomial.\n",
    "    alpha: float, the parameter of the generalized Laguerre polynomial.\n",
    "    x: float, the point at which to evaluate the polynomial.\n",
    "    max_n: int, the maximum degree of n in the batch.\n",
    "\n",
    "    Returns:\n",
    "    The value of the generalized Laguerre polynomial of degree n with parameter alpha at point x.\n",
    "    \"\"\"\n",
    "    # Initialize the array to store the generalized Laguerre polynomials for all degrees from 0 to max_n\n",
    "    p = jnp.zeros((max_n + 1,) + x.shape)\n",
    "    p = p.at[0].set(1.0)  # Set the 0th degree generalized Laguerre polynomial\n",
    "\n",
    "    # Compute the generalized Laguerre polynomials for degrees 1 to max_n using the recurrence relation\n",
    "    def body_fun(i, p):\n",
    "        p_i = ((2 * i + alpha - 1 - x) * p[i - 1] - (i + alpha - 1) * p[i - 2]) / i\n",
    "        return p.at[i].set(p_i)\n",
    "\n",
    "    p = jax.lax.fori_loop(1, max_n + 1, body_fun, p)\n",
    "\n",
    "    return p[n]\n",
    "```\n",
    "\n",
    "```python\n",
    "def eval_genlaguerre(n, alpha, x, out=None):\n",
    "    \"\"\"\n",
    "    Evaluates the generalized Laguerre polynomials of degrees specified in the input array n with parameter alpha at the points specified in the input array x.\n",
    "\n",
    "    Args:\n",
    "    n: array-like, the degrees of the generalized Laguerre polynomials.\n",
    "    alpha: float, the parameter of the generalized Laguerre polynomials.\n",
    "    x: array-like, the points at which to evaluate the polynomials.\n",
    "    out: optional, an output array to store the results.\n",
    "\n",
    "    Returns:\n",
    "    An array containing the generalized Laguerre polynomial values of the specified degrees with parameter alpha at the specified points.\n",
    "    \"\"\"\n",
    "    n = jnp.asarray(n)\n",
    "    x = jnp.asarray(x)\n",
    "    max_n = n.max()\n",
    "\n",
    "    if n.ndim == 1 and x.ndim == 1:\n",
    "        p = jax.vmap(\n",
    "            lambda ni: jax.vmap(\n",
    "                lambda xi: genlaguerre_recurrence(ni, alpha, xi, max_n)\n",
    "            )(x)\n",
    "        )(n)\n",
    "        p = jnp.diagonal(\n",
    "            p\n",
    "        )  # Get the diagonal elements to match the scipy.signal.eval_genlaguerre output\n",
    "    else:\n",
    "        p = jax.vmap(\n",
    "            lambda ni: jax.vmap(\n",
    "                lambda xi: genlaguerre_recurrence(ni, alpha, xi, max_n)\n",
    "            )(x)\n",
    "        )(n)\n",
    "\n",
    "    if out is not None:\n",
    "        out = jnp.asarray(out)\n",
    "        out = jnp.copy(p, out=out)\n",
    "        return out\n",
    "    else:\n",
    "        return p\n",
    "```\n",
    "\n",
    "```python\n",
    "def test_eval_genlaguerre():\n",
    "    alpha = 2.0\n",
    "    n = np.array([0, 1, 2, 3])\n",
    "\n",
    "    print(f\"n = {n}\")\n",
    "    print(f\"n shape = {n.shape}\")\n",
    "\n",
    "    x = np.linspace(-1, 1, n.shape[0])\n",
    "\n",
    "    print(f\"x = {x}\")\n",
    "    print(f\"x shape = {x.shape}\")\n",
    "\n",
    "    y_pred = eval_genlaguerre(n, alpha, x)\n",
    "    y = jnp.array(ss.eval_genlaguerre(n, alpha, x))\n",
    "\n",
    "    print(f\"y_pred = {y_pred}\")\n",
    "    print(f\"y_pred shape = {y_pred.shape}\")\n",
    "    print(f\"y = {y}\")\n",
    "    print(f\"y shape = {y.shape}\")\n",
    "\n",
    "    assert np.allclose(y_pred, y, rtol=1e-5, atol=1e-8), \"Results do not match\"\n",
    "    print(\"Results match\")\n",
    "```\n",
    "\n",
    "output:\n",
    "```bash\n",
    "n = [0 1 2 3]\n",
    "n shape = (4,)\n",
    "x = [-1.         -0.33333333  0.33333333  1.        ]\n",
    "x shape = (4,)\n",
    "y_pred = [1.        3.3333333 4.7222223 2.3333335]\n",
    "y_pred shape = (4,)\n",
    "y = [1.        3.3333333 4.7222223 2.3333333]\n",
    "y shape = (4,)\n",
    "Results match\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = [0 1 2 3]\n",
      "n shape = (4,)\n",
      "x = [-1.         -0.33333333  0.33333333  1.        ]\n",
      "x shape = (4,)\n",
      "y_pred = [1.        3.3333333 4.7222223 2.3333335]\n",
      "y_pred shape = (4,)\n",
      "y = [1.        3.3333333 4.7222223 2.3333333]\n",
      "y shape = (4,)\n",
      "Results match\n"
     ]
    }
   ],
   "source": [
    "test_eval_genlaguerre()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('s4mer-pkg-rmt3vFtN-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "616abfd6b1e11a599364f0d5228ada514baf1d2a8611f9274dc002b78190c46b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
