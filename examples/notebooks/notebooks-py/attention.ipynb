{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Tests\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_path: /home/beegass/Documents/Coding/HiPPO-Jax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../../../\"))\n",
    "print(f\"module_path: {module_path}\")\n",
    "if module_path not in sys.path:\n",
    "    print(f\"Adding {module_path} to sys.path\")\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "os.environ[\"TF_FORCE_UNIFIED_MEMORY\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.cache/pypoetry/virtualenvs/hippo-pkg-Uqb72G6k-py3.8/lib/python3.8/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "/home/beegass/.cache/pypoetry/virtualenvs/hippo-pkg-Uqb72G6k-py3.8/lib/python3.8/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n"
     ]
    }
   ],
   "source": [
    "## import packages\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import torch.nn as tnn\n",
    "import torch.nn.functional as F\n",
    "from src.models.hippo.hippo import HiPPOLSI, HiPPOLTI\n",
    "from src.data.process import whitesignal\n",
    "import einops\n",
    "from jaxtyping import Array, Float\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]\n",
      "The Device: gpu\n"
     ]
    }
   ],
   "source": [
    "print(jax.devices())\n",
    "print(f\"The Device: {jax.lib.xla_bridge.get_backend().platform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS enabled: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"MPS enabled: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(linewidth=150)\n",
    "np.set_printoptions(linewidth=150)\n",
    "jnp.set_printoptions(linewidth=150)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 10\n",
    "subkeys = jax.random.split(key, num=num_copies)\n",
    "key = subkeys[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f28060877f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Attention Block Unification\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick note on Transformer Block Unification\n",
    "\n",
    "Fun aside is that the MLP blocks of a Transformer are actually identical to the Self-Attention blocks of the transformer, with a few changes:\n",
    "\n",
    "- the query projection is missing, the data itself is the query\n",
    "- the key, value are data-independent parameters (i.e. the MLP block is really just a cross-attention \"soft lookup\" into a fixed {key:value} table\n",
    "- the Softmax (map/reduce non-linearity) is replaced with GeLU (map-only non-linearity)\n",
    "- the final Linear projection back to the residual pathway is missing\n",
    "\n",
    "This immediately suggests a unification of these blocks into a more general Transformer \"superblock\", that is simply wired up to the residual pathway either in parallel (e.g. as in all the heads of a multi-headed self-attention), or in series (as usually done from block to block otherwise). It also suggests in-between generalizations, e.g. multi-headed attention suggests the equivalent use of \"groups\" in Linear (or Conv) layers. Alternatively, attention could be done over two pools of nodes simultaneously: those where key,value are data-dependent and those that aren't, dispensing with the need for a distinction.\n",
    "\n",
    "**TLDR**: A much simpler Transformer with a single type of block wired up to a residual pathway in both parallel and in series is possible but to my knowledge has not yet been convincingly achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "Parameters"
    ]
   },
   "outputs": [],
   "source": [
    "B, T, C = 8, 512, 128  # batch, sequence length, channels\n",
    "n_head = 4\n",
    "n_embd = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "Data"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "Model"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key shape: torch.Size([8, 512, 32])\n",
      "query shape: torch.Size([8, 512, 32])\n",
      "value shape: torch.Size([8, 512, 32])\n",
      "context shape: torch.Size([8, 512, 32])\n",
      "projected context shape: torch.Size([8, 512, 128])\n"
     ]
    }
   ],
   "source": [
    "# self-attention block of a single Head\n",
    "d_head = C // n_head  # head size, 128/4 = 32\n",
    "key = tnn.Linear(C, d_head, bias=False)\n",
    "query = tnn.Linear(C, d_head, bias=False)\n",
    "value = tnn.Linear(C, d_head, bias=False)\n",
    "proj = tnn.Linear(d_head, C, bias=False)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "v = value(x)\n",
    "print(f\"key shape: {k.shape}\")\n",
    "print(f\"query shape: {q.shape}\")\n",
    "print(f\"value shape: {v.shape}\")\n",
    "att = torch.softmax(q @ k.transpose(-2, -1), dim=-1)\n",
    "y = att @ v\n",
    "print(f\"context shape: {y.shape}\")\n",
    "r = proj(\n",
    "    y\n",
    ")  # standard self-attention blocks have one more Linear when back to residual pathway\n",
    "print(f\"projected context shape: {r.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# typical linear block on a Transformer\n",
    "layer1 = tnn.Linear(C, C * 4, bias=False)\n",
    "layer2 = tnn.Linear(C * 4, C, bias=False)\n",
    "l1 = F.gelu(layer1(x))\n",
    "l2 = layer2(l1)  # projects back to residual pathway\n",
    "l2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear block is actually attention over a fixed (not data-dependent) {k:v} dict\n",
    "q = x  # change 1: query is simply the input\n",
    "k = layer1.weight  # key and value are data-independent learnable parameters\n",
    "v = layer2.weight.T\n",
    "att = F.gelu(q @ k.transpose(-2, -1))  # change 2: using gelu instead of softmax\n",
    "y = att @ v\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(l2 == y).all()  # cool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flax Attention Block Unification\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Dot-Product Attention: Compute the dot products of the query with all keys,\n",
    "    and apply a softmax function to obtain the weights on the values\n",
    "\n",
    "    Attributes:\n",
    "        n_head: The number of attention heads.\n",
    "        d_model: The dimension of the input.\n",
    "        dtype: The data type of the computation. Default is jnp.float32.\n",
    "    \"\"\"\n",
    "\n",
    "    n_head: int  # number of heads the attention is split into\n",
    "    d_model: int  # dimension of the input, aka n_embd or C which is the size of the embedding.\n",
    "    dtype: jnp.dtype = jnp.float32  # data type of the computation (default: float32)\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        # Check if d_model is divisible by n_head to ensure the input can be evenly distributed among all heads\n",
    "        assert self.d_model % self.n_head == 0\n",
    "\n",
    "        # Compute the size of each head by dividing the input dimension by the number of heads. head size, e.g. 128/4 = 32\n",
    "        self.d_head = self.d_model // self.n_head\n",
    "\n",
    "        # Create dense layers for key, query, and value with the dimension size of each head.\n",
    "        self.key = nn.Dense(\n",
    "            self.d_head,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),  # Weights with Xavier uniform init\n",
    "            bias_init=nn.initializers.zeros,  # Bias init with zeros\n",
    "            name=f\"key_layer\",\n",
    "        )\n",
    "        self.query = nn.Dense(\n",
    "            self.d_head,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),  # Weights with Xavier uniform init\n",
    "            bias_init=nn.initializers.zeros,  # Bias init with zeros\n",
    "            name=f\"query_layer\",\n",
    "        )\n",
    "        self.value = nn.Dense(\n",
    "            self.d_head,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),  # Weights with Xavier uniform init\n",
    "            bias_init=nn.initializers.zeros,  # Bias init with zeros\n",
    "            name=f\"value_layer\",\n",
    "        )\n",
    "\n",
    "        # Create a dense layer for projecting the output back to the original dimension size.\n",
    "        self.proj = nn.Dense(\n",
    "            self.d_model,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),  # Weights with Xavier uniform init\n",
    "            bias_init=nn.initializers.zeros,  # Bias init with zeros\n",
    "            name=f\"proj_layer\",\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        query: Float[Array, \"*batch d_model\"],\n",
    "        key: Float[Array, \"*batch d_model\"],\n",
    "        value: Float[Array, \"*batch d_model\"],\n",
    "        mask: Optional[Float[Array, \"*batch d_model\"]] = None,\n",
    "    ) -> Tuple[Float[Array, \"*batch d_model\"], Float[Array, \"*batch d_model\"]]:\n",
    "        \"\"\"\n",
    "        Call method, used for calculating the forward pass for the Self Attention Module.\n",
    "\n",
    "        Args:\n",
    "            query (jnp.ndarray):\n",
    "                Shape: (batch d_model)\n",
    "                The query tensor.\n",
    "\n",
    "            key (jnp.ndarray):\n",
    "                Shape: (batch d_model)\n",
    "                The key tensor.\n",
    "\n",
    "            value (jnp.ndarray):\n",
    "                Shape: (batch d_model)\n",
    "                The value tensor.\n",
    "\n",
    "        Returns:\n",
    "            r (jnp.ndarray):\n",
    "                Shape: (batch d_model)\n",
    "                The projected context tensor back to the original dimension size.\n",
    "        \"\"\"\n",
    "        _k = self.key(key)\n",
    "        q = self.query(query)\n",
    "        v = self.value(value)\n",
    "        k = einops.rearrange(_k, \"... i j -> ... j i\")\n",
    "\n",
    "        # Calculate the attention scores by taking the dot product of query and key\n",
    "        score = (q @ k) / jnp.sqrt(self.d_head)\n",
    "\n",
    "        # Masking to avoid performing attention on padding token indices.\n",
    "        if mask is not None:\n",
    "            # Set the score for all padding token indices to a large negative value\n",
    "            score = jnp.where(\n",
    "                mask == 0, -9e15, score\n",
    "            )  # -9e15 is a very large negative number\n",
    "\n",
    "        # then apply softmax to get probabilities.\n",
    "        attn = nn.softmax(score, axis=-1)\n",
    "\n",
    "        # Multiply the attention scores with the value to get the context\n",
    "        context = attn @ v\n",
    "\n",
    "        # Project the context back to the original dimension size using the projection layer\n",
    "        out = self.proj(context)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Attributes:\n",
    "        n_head: The number of attention heads.\n",
    "        d_model: The dimension of the input.\n",
    "        dtype: The data type of the computation. Default is jnp.float32.\n",
    "    \"\"\"\n",
    "\n",
    "    n_head: int  # number of heads the attention is split into\n",
    "    d_model: int  # dimension of the input, aka n_embd or C which is the size of the embedding.\n",
    "    dtype: jnp.dtype = jnp.float32  # data type of the computation (default: float32)\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        # Check if d_model is divisible by n_head to ensure the input can be evenly distributed among all heads\n",
    "        assert self.d_model % self.n_head == 0\n",
    "\n",
    "        # Compute the size of each head by dividing the input dimension by the number of heads. head size, e.g. 128/4 = 32\n",
    "        self.d_head = self.d_model // self.n_head\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        query: Float[Array, \"*batch seq_len d_model\"],\n",
    "        key: Float[Array, \"*batch seq_len d_model\"],\n",
    "        value: Float[Array, \"*batch seq_len d_model\"],\n",
    "        mask: Optional[Float[Array, \"*batch seq_len d_model\"]] = None,\n",
    "    ) -> Tuple[\n",
    "        Float[Array, \"*batch seq_len d_model\"], Float[Array, \"*batch seq_len seq_len\"]\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Call method, used for calculating the forward pass for the Self Attention Module.\n",
    "\n",
    "        Args:\n",
    "            query (jnp.ndarray):\n",
    "                Shape: (batch d_model)\n",
    "                The query tensor.\n",
    "\n",
    "            key (jnp.ndarray):\n",
    "                Shape: (batch d_model)\n",
    "                The key tensor.\n",
    "\n",
    "            value (jnp.ndarray):\n",
    "                Shape: (batch d_model)\n",
    "                The value tensor.\n",
    "\n",
    "        Returns:\n",
    "            r (jnp.ndarray):\n",
    "                Shape: (batch d_model)\n",
    "                The projected context tensor back to the original dimension size.\n",
    "        \"\"\"\n",
    "\n",
    "        key = einops.rearrange(key, \"... i j -> ... j i\")\n",
    "\n",
    "        # Calculate the attention scores by taking the dot product of query and key\n",
    "        score = (query @ key) / jnp.sqrt(self.d_head)\n",
    "\n",
    "        # Masking to avoid performing attention on padding token indices.\n",
    "        if mask is not None:\n",
    "            assert (\n",
    "                mask.shape == score.shape\n",
    "            ), f\"Mask shape {mask.shape} must match score shape {score.shape}\"\n",
    "\n",
    "            # Set the score for all padding token indices to a large negative value\n",
    "            score = jnp.where(\n",
    "                mask == 0, -9e15, score\n",
    "            )  # -9e15 is a very large negative number\n",
    "\n",
    "        # then apply softmax to get probabilities.\n",
    "        attn = nn.softmax(score, axis=-1)\n",
    "\n",
    "        # Multiply the attention scores with the value to get the context\n",
    "        context = attn @ value\n",
    "\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    n_head: int  # number of heads the attention is split into\n",
    "    d_model: int  # dimension of the input, aka n_embd or C which is the size of the embedding.\n",
    "    # step: int  # step size for the GBT\n",
    "    # lambda_n: float = 1.0  # lambda_n for the LegT\n",
    "    # alpha: float = 2.0  # alpha for the GBT,\n",
    "    # measure: str = \"legs\"  # measure for type of the polynomial,\n",
    "    # basis: float = 1.0  # basis for the polynomial\n",
    "    # unroll: bool = False  # unroll the loop for the output\n",
    "    dtype: jnp.dtype = jnp.float32  # data type of the computation (default: float32)\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        # Check if d_model is divisible by n_head to ensure the input can be evenly distributed among all heads\n",
    "        assert self.d_model % self.n_head == 0\n",
    "\n",
    "        # Compute the size of each head by dividing the input dimension by the number of heads. head size, e.g. 128/4 = 32\n",
    "        self.d_head = self.d_model // self.n_head\n",
    "\n",
    "        # Create dense layers for key, query, and value with the dimension size of each head.\n",
    "\n",
    "        self.key = nn.Dense(\n",
    "            self.d_model,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),  # Weights with Xavier uniform init\n",
    "            bias_init=nn.initializers.zeros,  # Bias init with zeros\n",
    "            name=f\"key_layer\",\n",
    "        )\n",
    "        self.query = nn.Dense(\n",
    "            self.d_model,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),  # Weights with Xavier uniform init\n",
    "            bias_init=nn.initializers.zeros,  # Bias init with zeros\n",
    "            name=f\"query_layer\",\n",
    "        )\n",
    "        self.value = nn.Dense(\n",
    "            self.d_model,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),  # Weights with Xavier uniform init\n",
    "            bias_init=nn.initializers.zeros,  # Bias init with zeros\n",
    "            name=f\"value_layer\",\n",
    "        )\n",
    "\n",
    "        # Create a dense layer for projecting the output back to the original dimension size.\n",
    "        self.proj = nn.Dense(\n",
    "            self.d_model,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),  # Weights with Xavier uniform init\n",
    "            bias_init=nn.initializers.zeros,  # Bias init with zeros\n",
    "            name=f\"proj_layer\",\n",
    "        )\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(\n",
    "            n_head=self.n_head, d_model=self.d_model\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        query: Float[Array, \"*batch seq_len d_model\"],\n",
    "        key: Float[Array, \"*batch seq_len d_model\"],\n",
    "        value: Float[Array, \"*batch seq_len d_model\"],\n",
    "        mask: Optional[Float[Array, \"*batch seq_len d_model\"]] = None,\n",
    "    ) -> Tuple[\n",
    "        Float[Array, \"*batch seq_len d_model\"], Float[Array, \"*batch seq_len seq_len\"]\n",
    "    ]:\n",
    "        jax.debug.print(\"query shape: {x}\", x=query.shape)\n",
    "        jax.debug.print(\"key shape: {x}\", x=key.shape)\n",
    "        jax.debug.print(\"value shape: {x}\", x=value.shape)\n",
    "\n",
    "        q = self.query(query)\n",
    "        k = self.key(key)\n",
    "        v = self.value(value)\n",
    "\n",
    "        q = einops.rearrange(\n",
    "            q,\n",
    "            \"... seq_len (n_head d_head) -> ... seq_len n_head d_head\",\n",
    "            n_head=self.n_head,\n",
    "            d_head=self.d_head,\n",
    "        )\n",
    "        k = einops.rearrange(\n",
    "            k,\n",
    "            \"... seq_len (n_head d_head) -> ... seq_len n_head d_head\",\n",
    "            n_head=self.n_head,\n",
    "            d_head=self.d_head,\n",
    "        )\n",
    "        v = einops.rearrange(\n",
    "            v,\n",
    "            \"... seq_len (n_head d_head) -> ... seq_len n_head d_head\",\n",
    "            n_head=self.n_head,\n",
    "            d_head=self.d_head,\n",
    "        )\n",
    "\n",
    "        jax.debug.print(\"q shape: {x}\", x=q.shape)\n",
    "        jax.debug.print(\"k shape: {x}\", x=k.shape)\n",
    "        jax.debug.print(\"v shape: {x}\\n\", x=v.shape)\n",
    "\n",
    "        context, attn = jax.vmap(self.attention, in_axes=(1, 1, 1, None))(q, k, v, mask)\n",
    "\n",
    "        context = einops.rearrange(\n",
    "            context,\n",
    "            \"... n_head seq_len d_head -> ... seq_len (n_head d_head)\",\n",
    "            n_head=self.n_head,\n",
    "            d_head=self.d_head,\n",
    "        )\n",
    "\n",
    "        # jax.debug.print(\"context: {x}\\n\", x=context)\n",
    "        jax.debug.print(\"context shape: {x}\", x=context.shape)\n",
    "        # jax.debug.print(\"attn: {x}\\n\", x=attn)\n",
    "        jax.debug.print(\"attn shape: {x}\", x=attn.shape)\n",
    "\n",
    "        # Project the context back to the original dimension size using the projection layer\n",
    "        out = self.proj(context)\n",
    "\n",
    "        # jax.debug.print(\"out: {x}\\n\", x=out)\n",
    "        jax.debug.print(\"out shape: {x}\", x=out.shape)\n",
    "\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    d_model: int  # Input dimension is needed here since it is equal to the output dimension (residual connection)\n",
    "    n_head: int  # number of heads the attention is split into\n",
    "    ffn_expan: int  # expansion factor for the feedforward layer\n",
    "    _dropout: float  # dropout probability\n",
    "    # step: int  # step size for the GBT\n",
    "    # lambda_n: float = 1.0  # lambda_n for the LegT\n",
    "    # alpha: float = 2.0  # alpha for the GBT,\n",
    "    # measure: str = \"legs\"  # measure for type of the polynomial,\n",
    "    # basis: float = 1.0  # basis for the polynomial\n",
    "    # unroll: bool = False  # unroll the loop for the output\n",
    "\n",
    "    def setup(self):\n",
    "        # Attention layer\n",
    "        self.attention = MultiHeadAttention(n_head=self.n_head, d_model=self.d_model)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.ffn = [\n",
    "            nn.Dense(self.ffn_expan * self.d_model),\n",
    "            nn.Dropout(rate=self._dropout),\n",
    "            nn.relu,\n",
    "            nn.Dense(self.d_model),\n",
    "        ]\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "        self.dropout = nn.Dropout(rate=self._dropout)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        query: Float[Array, \"*batch seq_len d_model\"],\n",
    "        key: Float[Array, \"*batch seq_len d_model\"],\n",
    "        value: Float[Array, \"*batch seq_len d_model\"],\n",
    "        mask: Optional[Float[Array, \"*batch seq_len d_model\"]] = None,\n",
    "        train: bool = True,\n",
    "    ) -> Float[Array, \"*batch seq_len d_model\"]:\n",
    "\n",
    "        # Attention part\n",
    "        proj_context, attn = jax.vmap(self.attention, in_axes=(0, 0, 0, 0))(\n",
    "            self.norm1(query), self.norm1(key), self.norm1(value), mask\n",
    "        )\n",
    "        x = query + self.dropout(proj_context, deterministic=not train)\n",
    "\n",
    "        # MLP part\n",
    "        linear_out = self.norm2(x)\n",
    "        for layer in self.ffn:\n",
    "            if not isinstance(layer, nn.Dropout):\n",
    "                linear_out = layer(linear_out)\n",
    "            else:\n",
    "                linear_out = layer(linear_out, deterministic=not train)\n",
    "\n",
    "        x = x + self.dropout(linear_out, deterministic=not train)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: (128, 512)\n",
      "key shape: (128, 512)\n",
      "value shape: (128, 512)\n",
      "q shape: (128, 8, 64)\n",
      "k shape: (128, 8, 64)\n",
      "v shape: (128, 8, 64)\n",
      "\n",
      "context shape: (128, 512)\n",
      "attn shape: (8, 128, 128)\n",
      "out shape: (128, 512)\n",
      "query shape: (128, 512)\n",
      "key shape: (128, 512)\n",
      "value shape: (128, 512)\n",
      "q shape: (128, 8, 64)\n",
      "k shape: (128, 8, 64)\n",
      "v shape: (128, 8, 64)\n",
      "\n",
      "context shape: (128, 512)\n",
      "attn shape: (8, 128, 128)\n",
      "out shape: (128, 512)\n",
      "Transformer block test successful!\n"
     ]
    }
   ],
   "source": [
    "def test_transformer_block():\n",
    "    # Parameters\n",
    "    batch_size = 16\n",
    "    seq_len = 128\n",
    "    d_model = 512\n",
    "    n_head = 8\n",
    "    ffn_expan = 4\n",
    "    dropout = 0.1\n",
    "\n",
    "    # Create transformer block\n",
    "    transformer_block = TransformerBlock(\n",
    "        d_model=d_model, n_head=n_head, ffn_expan=ffn_expan, _dropout=dropout\n",
    "    )\n",
    "\n",
    "    # Initialize parameters\n",
    "    variables = transformer_block.init(\n",
    "        {\"params\": subkeys[1], \"dropout\": subkeys[2]},\n",
    "        jnp.ones((batch_size, seq_len, d_model)),\n",
    "        jnp.ones((batch_size, seq_len, d_model)),\n",
    "        jnp.ones((batch_size, seq_len, d_model)),\n",
    "    )\n",
    "    params = variables[\"params\"]\n",
    "\n",
    "    # Test forward pass\n",
    "    y = transformer_block.apply(\n",
    "        {\"params\": params},\n",
    "        jnp.ones((batch_size, seq_len, d_model)),\n",
    "        jnp.ones((batch_size, seq_len, d_model)),\n",
    "        jnp.ones((batch_size, seq_len, d_model)),\n",
    "        rngs={\"dropout\": subkeys[4]},\n",
    "    )\n",
    "\n",
    "    assert y.shape == (\n",
    "        batch_size,\n",
    "        seq_len,\n",
    "        d_model,\n",
    "    ), f\"Unexpected output shape: {y.shape}\"\n",
    "    print(\"Transformer block test successful!\")\n",
    "\n",
    "\n",
    "test_transformer_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    d_model: int  # Input dimension is needed here since it is equal to the output dimension (residual connection)\n",
    "    n_head: int  # number of heads the attention is split into\n",
    "    ffn_expan: int  # expansion factor for the feedforward layer\n",
    "    dropout: float  # dropout probability\n",
    "\n",
    "    def setup(self):\n",
    "        # Attention layer\n",
    "        self.attention = MultiHeadAttention(n_head=self.n_head, d_model=self.d_model)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            d_model=self.d_model,\n",
    "            n_head=self.n_head,\n",
    "            ffn_expan=self.ffn_expan,\n",
    "            _dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "        self.dropout = nn.Dropout(rate=self.dropout)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: Float[Array, \"*batch d_model\"],\n",
    "        key: Float[Array, \"*batch d_model\"],\n",
    "        value: Float[Array, \"*batch d_model\"],\n",
    "        mask: Optional[Float[Array, \"*batch d_model\"]] = None,\n",
    "        trg_mask: Optional[Float[Array, \"*batch d_model\"]] = None,\n",
    "        train: bool = True,\n",
    "    ) -> Float[Array, \"*batch d_model\"]:\n",
    "        # Masked Attention part\n",
    "        mask_proj_context, mask_attn = jax.vmap(self.attention, in_axes=(0, 0, 0, 0))(\n",
    "            self.norm1(x), self.norm1(key), self.norm1(value), trg_mask\n",
    "        )\n",
    "        query = x + self.dropout(mask_proj_context, deterministic=not train)\n",
    "\n",
    "        # Encoder Attention part\n",
    "        out = self.transformer_block(\n",
    "            self.norm2(query), self.norm2(key), self.norm2(value), mask=mask\n",
    "        )\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4mer-pkg-jZnBSgjq-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a81e05d1d7f7eae781698b7c1b81c0d771335201ebad1d81045cb177cef974b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
