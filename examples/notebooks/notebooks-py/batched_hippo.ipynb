{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiPPO Matrices\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Loading In Necessary Packages](#load-packages)\n",
    "    * [Instantiate The HiPPO Matrix](#instantiate-the-hippo-matrix)\n",
    "        * [Translated Legendre (LegT)](#translated-legendre-legt)\n",
    "            * [LegT](#legt)\n",
    "            * [LMU](#lmu)\n",
    "        * [Translated Laguerre (LagT)](#translated-laguerre-lagt)\n",
    "        * [Scaled Legendre (LegS)](#scaled-legendre-legs)\n",
    "        * [Fourier Basis](#fourier-basis)\n",
    "            * [Fourier Recurrent Unit (FRU)](#fourier-recurrent-unit-fru)\n",
    "            * [Truncated Fourier (FouT)](#truncated-fourier-fout)\n",
    "            * [Fourier With Decay (FourD)](#fourier-with-decay-fourd)\n",
    "    * [Utilities For Gu HiPPO Operator](#utilities-for-gu-hippo-operator)\n",
    "    * [Gu's HiPPO LegT Operator](#gus-hippo-legt-operator)\n",
    "    * [Gu's Scale invariant HiPPO LegS Operator](#gus-scale-invariant-hippo-legs-operator)\n",
    "    * [Implementation Of General HiPPO Operator](#implementation-of-general-hippo-operator)\n",
    "    * [Output](#output)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]\n",
      "The Device: gpu\n"
     ]
    }
   ],
   "source": [
    "## import packages\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import linen as jnn\n",
    "\n",
    "from jax.nn.initializers import lecun_normal, uniform\n",
    "from jax.numpy.linalg import eig, inv, matrix_power\n",
    "from jax.scipy.signal import convolve\n",
    "\n",
    "# import modules \n",
    "from src.models.hippo.gu_transition import GuTransMatrix\n",
    "from src.data.process import moving_window, rolling_window\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "from scipy import linalg as la\n",
    "from scipy import signal\n",
    "from scipy import special as ss\n",
    "\n",
    "import math\n",
    "\n",
    "print(jax.devices())\n",
    "print(f\"The Device: {jax.lib.xla_bridge.get_backend().platform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS enabled: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from typing import Any\n",
    "from functools import partial\n",
    "\n",
    "print(f\"MPS enabled: {torch.backends.mps.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(linewidth=150)\n",
    "np.set_printoptions(linewidth=150)\n",
    "jnp.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 5\n",
    "rng, key2, key3, key4, key5 = jax.random.split(key, num=num_copies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate The HiPPO Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransMatrix:\n",
    "    def __init__(\n",
    "        self,\n",
    "        N: int,\n",
    "        measure: str = \"legs\",\n",
    "        lambda_n: float = 1.0,\n",
    "        alpha: float = 0.0,\n",
    "        beta: float = 1.0,\n",
    "        dtype: Any = jnp.float32,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiates the HiPPO matrix of a given order using a particular measure.\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            v (str): choose between this repo's implementation or hazy research's implementation.\n",
    "            measure (str):\n",
    "                choose between\n",
    "                    - HiPPO w/ Translated Legendre (LegT) - legt\n",
    "                    - HiPPO w/ Translated Laguerre (LagT) - lagt\n",
    "                    - HiPPO w/ Scaled Legendre (LegS) - legs\n",
    "                    - HiPPO w/ Fourier basis - fourier\n",
    "                        - FRU: Fourier Recurrent Unit\n",
    "                        - FouT: Translated Fourier\n",
    "            lambda_n (int): The amount of tilt applied to the HiPPO-LegS basis, determines between LegS and LMU.\n",
    "            fourier_type (str): chooses between the following:\n",
    "                - FRU: Fourier Recurrent Unit - fru\n",
    "                - FouT: Translated Fourier - fout\n",
    "                - FourD: Fourier Decay - fourd\n",
    "            alpha (float): The order of the Laguerre basis.\n",
    "            beta (float): The scale of the Laguerre basis.\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The HiPPO matrix multiplied by -1.\n",
    "            B (jnp.ndarray): The other corresponding state space matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        A = None\n",
    "        B = None\n",
    "        if measure in [\"legt\", \"lmu\"]:\n",
    "            A, B = self.build_LegT(N=N, lambda_n=lambda_n, dtype=dtype)\n",
    "\n",
    "        elif measure == \"lagt\":\n",
    "            A, B = self.build_LagT(alpha=alpha, beta=beta, N=N, dtype=dtype)\n",
    "\n",
    "        elif measure == \"legs\":\n",
    "            A, B = self.build_LegS(N=N, dtype=dtype)\n",
    "\n",
    "        elif measure in [\"fout\", \"fru\", \"foud\"]:\n",
    "            A, B = self.build_Fourier(N=N, fourier_type=measure, dtype=dtype)\n",
    "\n",
    "        elif measure == \"random\":\n",
    "            A = jnp.random.randn(N, N) / N\n",
    "            B = jnp.random.randn(N, 1)\n",
    "\n",
    "        elif measure == \"diagonal\":\n",
    "            A = -jnp.diag(jnp.exp(jnp.random.randn(N)))\n",
    "            B = jnp.random.randn(N, 1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid HiPPO type\")\n",
    "\n",
    "        self.A = (A.copy()).astype(dtype)\n",
    "        self.B = (B.copy()).astype(dtype)\n",
    "\n",
    "    # Translated Legendre (LegT) - vectorized\n",
    "    @staticmethod\n",
    "    def build_LegT(N, lambda_n=1, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        The, vectorized implementation of the, measure derived from the translated Legendre basis.\n",
    "\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            legt_type (str): Choice between the two different tilts of basis.\n",
    "                - legt: translated Legendre - 'legt'\n",
    "                - lmu: Legendre Memory Unit - 'lmu'\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        q = jnp.arange(N, dtype=dtype)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "        case = jnp.power(-1.0, (n - k))\n",
    "        A = None\n",
    "        B = None\n",
    "\n",
    "        if lambda_n == 1:\n",
    "            A_base = jnp.sqrt(2 * n + 1) * jnp.sqrt(2 * k + 1)\n",
    "            pre_D = jnp.sqrt(jnp.diag(2 * q + 1))\n",
    "            B = D = jnp.diag(pre_D)[:, None]\n",
    "            A = jnp.where(\n",
    "                k <= n, A_base, A_base * case\n",
    "            )  # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "        elif lambda_n == 2:  # (jnp.sqrt(2*n+1) * jnp.power(-1, n)):\n",
    "            A_base = 2 * n + 1\n",
    "            B = jnp.diag((2 * q + 1) * jnp.power(-1, n))[:, None]\n",
    "            A = jnp.where(\n",
    "                k <= n, A_base * case, A_base\n",
    "            )  # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "        return -A.astype(dtype), B.astype(dtype)\n",
    "\n",
    "    # Translated Laguerre (LagT) - non-vectorized\n",
    "    @staticmethod\n",
    "    def build_LagT(alpha, beta, N, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        The, vectorized implementation of the, measure derived from the translated Laguerre basis.\n",
    "\n",
    "        Args:\n",
    "            alpha (float): The order of the Laguerre basis.\n",
    "            beta (float): The scale of the Laguerre basis.\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        L = jnp.exp(\n",
    "            0.5\n",
    "            * (ss.gammaln(jnp.arange(N) + alpha + 1) - ss.gammaln(jnp.arange(N) + 1))\n",
    "        )\n",
    "        inv_L = 1.0 / L[:, None]\n",
    "        pre_A = (jnp.eye(N) * ((1 + beta) / 2)) + jnp.tril(jnp.ones((N, N)), -1)\n",
    "        pre_B = ss.binom(alpha + jnp.arange(N), jnp.arange(N))[:, None]\n",
    "\n",
    "        A = -inv_L * pre_A * L[None, :]\n",
    "        B = (\n",
    "            jnp.exp(-0.5 * ss.gammaln(1 - alpha))\n",
    "            * jnp.power(beta, (1 - alpha) / 2)\n",
    "            * inv_L\n",
    "            * pre_B\n",
    "        )\n",
    "\n",
    "        return A.astype(dtype), B.astype(dtype)\n",
    "\n",
    "    # Scaled Legendre (LegS) vectorized\n",
    "    @staticmethod\n",
    "    def build_LegS(N, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        The, vectorized implementation of the, measure derived from the Scaled Legendre basis.\n",
    "\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        q = jnp.arange(N, dtype=dtype)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "        pre_D = jnp.sqrt(jnp.diag(2 * q + 1))\n",
    "        B = D = jnp.diag(pre_D)[:, None]\n",
    "\n",
    "        A_base = jnp.sqrt(2 * n + 1) * jnp.sqrt(2 * k + 1)\n",
    "\n",
    "        A = jnp.where(n > k, A_base, jnp.where(n == k, n + 1, 0.0))\n",
    "\n",
    "        return -A.astype(dtype), B.astype(dtype)\n",
    "\n",
    "    # Fourier Basis OPs and functions - vectorized\n",
    "    @staticmethod\n",
    "    def build_Fourier(N, fourier_type=\"fru\", dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        Vectorized measure implementations derived from fourier basis.\n",
    "\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            fourier_type (str): The type of Fourier measure.\n",
    "                - FRU: Fourier Recurrent Unit - fru\n",
    "                - FouT: truncated Fourier - fout\n",
    "                - fouD: decayed Fourier - foud\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        A = jnp.diag(\n",
    "            jnp.stack([jnp.zeros(N // 2), jnp.zeros(N // 2)], axis=-1).reshape(-1)[1:],\n",
    "            1,\n",
    "        )\n",
    "        B = jnp.zeros(A.shape[1], dtype=dtype)\n",
    "\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        q = jnp.arange(A.shape[1], dtype=dtype)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "\n",
    "        n_odd = n % 2 == 0\n",
    "        k_odd = k % 2 == 0\n",
    "\n",
    "        case_1 = (n == k) & (n == 0)\n",
    "        case_2_3 = ((k == 0) & (n_odd)) | ((n == 0) & (k_odd))\n",
    "        case_4 = (n_odd) & (k_odd)\n",
    "        case_5 = (n - k == 1) & (k_odd)\n",
    "        case_6 = (k - n == 1) & (n_odd)\n",
    "\n",
    "        if fourier_type == \"fru\":  # Fourier Recurrent Unit (FRU) - vectorized\n",
    "            A = jnp.where(\n",
    "                case_1,\n",
    "                -1.0,\n",
    "                jnp.where(\n",
    "                    case_2_3,\n",
    "                    -jnp.sqrt(2),\n",
    "                    jnp.where(\n",
    "                        case_4,\n",
    "                        -2,\n",
    "                        jnp.where(\n",
    "                            case_5,\n",
    "                            jnp.pi * (n // 2),\n",
    "                            jnp.where(case_6, -jnp.pi * (k // 2), 0.0),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        elif fourier_type == \"fout\":  # truncated Fourier (FouT) - vectorized\n",
    "            A = jnp.where(\n",
    "                case_1,\n",
    "                -1.0,\n",
    "                jnp.where(\n",
    "                    case_2_3,\n",
    "                    -jnp.sqrt(2),\n",
    "                    jnp.where(\n",
    "                        case_4,\n",
    "                        -2,\n",
    "                        jnp.where(\n",
    "                            case_5,\n",
    "                            jnp.pi * (n // 2),\n",
    "                            jnp.where(case_6, -jnp.pi * (k // 2), 0.0),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            A = 2 * A\n",
    "            B = 2 * B\n",
    "\n",
    "        elif fourier_type == \"foud\":\n",
    "            A = jnp.where(\n",
    "                case_1,\n",
    "                -1.0,\n",
    "                jnp.where(\n",
    "                    case_2_3,\n",
    "                    -jnp.sqrt(2),\n",
    "                    jnp.where(\n",
    "                        case_4,\n",
    "                        -2,\n",
    "                        jnp.where(\n",
    "                            case_5,\n",
    "                            2 * jnp.pi * (n // 2),\n",
    "                            jnp.where(case_6, 2 * -jnp.pi * (k // 2), 0.0),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            A = 0.5 * A\n",
    "            B = 0.5 * B\n",
    "\n",
    "        B = B[:, None]\n",
    "\n",
    "        return A.astype(dtype), B.astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Legendre (LegT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LegT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LegT():\n",
    "    legt_matrices = TransMatrix(N=8, measure=\"legt\", lambda_n=1.0)\n",
    "    A, B = legt_matrices.A, legt_matrices.B\n",
    "    gu_legt_matrices = GuTransMatrix(N=8, measure=\"legt\", lambda_n=1.0)\n",
    "    gu_A, gu_B = gu_legt_matrices.A, gu_legt_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -1.          1.7320508  -2.2360678   2.6457512  -3.          3.3166246  -3.6055512   3.8729832]\n",
      " [ -1.7320508  -3.          3.872983   -4.5825753   5.196152   -5.744562    6.244998   -6.708204 ]\n",
      " [ -2.2360678  -3.872983   -4.999999    5.916079   -6.7082033   7.4161973  -8.062257    8.660253 ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -6.9999995   7.937254   -8.774963    9.5393915 -10.24695  ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -9.          9.949874  -10.816654   11.61895  ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874  -10.999999   11.958261  -12.845232 ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261  -13.         13.964239 ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232  -13.964239  -14.999999 ]]\n",
      "Gu's A:\n",
      " [[ -1.          1.7320508  -2.2360678   2.6457512  -3.          3.3166246  -3.6055512   3.8729832]\n",
      " [ -1.7320508  -3.          3.872983   -4.5825753   5.196152   -5.744562    6.244998   -6.708204 ]\n",
      " [ -2.2360678  -3.872983   -4.999999    5.916079   -6.7082033   7.4161973  -8.062257    8.660253 ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -6.9999995   7.937254   -8.774963    9.5393915 -10.24695  ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -9.          9.949874  -10.816654   11.61895  ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874  -10.999999   11.958261  -12.845232 ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261  -13.         13.964239 ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232  -13.964239  -14.999999 ]]\n",
      "B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n",
      "Gu's B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n"
     ]
    }
   ],
   "source": [
    "test_LegT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LMU():\n",
    "    lmu_matrices = TransMatrix(\n",
    "        N=8, measure=\"legt\", lambda_n=2.0\n",
    "    )  # change lambda so resulting matrix is in the form of LMU\n",
    "    A, B = lmu_matrices.A, lmu_matrices.B\n",
    "    gu_lmu_matrices = GuTransMatrix(\n",
    "        N=8, measure=\"legt\", lambda_n=2.0\n",
    "    )  # change lambda so resulting matrix is in the form of LMU\n",
    "    gu_A, gu_B = gu_lmu_matrices.A, gu_lmu_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.]\n",
      " [ -5.   5.  -5.  -5.  -5.  -5.  -5.  -5.]\n",
      " [  7.  -7.   7.  -7.  -7.  -7.  -7.  -7.]\n",
      " [ -9.   9.  -9.   9.  -9.  -9.  -9.  -9.]\n",
      " [ 11. -11.  11. -11.  11. -11. -11. -11.]\n",
      " [-13.  13. -13.  13. -13.  13. -13. -13.]\n",
      " [ 15. -15.  15. -15.  15. -15.  15. -15.]]\n",
      "Gu's A:\n",
      " [[ -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.]\n",
      " [ -5.   5.  -5.  -5.  -5.  -5.  -5.  -5.]\n",
      " [  7.  -7.   7.  -7.  -7.  -7.  -7.  -7.]\n",
      " [ -9.   9.  -9.   9.  -9.  -9.  -9.  -9.]\n",
      " [ 11. -11.  11. -11.  11. -11. -11. -11.]\n",
      " [-13.  13. -13.  13. -13.  13. -13. -13.]\n",
      " [ 15. -15.  15. -15.  15. -15.  15. -15.]]\n",
      "B:\n",
      " [[  1.]\n",
      " [ -3.]\n",
      " [  5.]\n",
      " [ -7.]\n",
      " [  9.]\n",
      " [-11.]\n",
      " [ 13.]\n",
      " [-15.]]\n",
      "Gu's B:\n",
      " [[  1.]\n",
      " [ -3.]\n",
      " [  5.]\n",
      " [ -7.]\n",
      " [  9.]\n",
      " [-11.]\n",
      " [ 13.]\n",
      " [-15.]]\n"
     ]
    }
   ],
   "source": [
    "test_LMU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Laguerre (LagT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LagT():\n",
    "    lagt_matrices = TransMatrix(\n",
    "        N=8,\n",
    "        measure=\"lagt\",\n",
    "        alpha=0.0,  # change resulting tilt through alpha and beta\n",
    "        beta=1.0,\n",
    "    )  # change resulting tilt through alpha and beta\n",
    "    A, B = lagt_matrices.A, lagt_matrices.B\n",
    "    gu_lagt_matrices = GuTransMatrix(\n",
    "        N=8,\n",
    "        measure=\"lagt\",\n",
    "        alpha=0.0,  # change resulting tilt through alpha and beta\n",
    "        beta=1.0,\n",
    "    )  # change resulting tilt through alpha and beta\n",
    "    gu_A, gu_B = gu_lagt_matrices.A, gu_lagt_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-1.         -0.         -0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.         -1.         -0.        ]\n",
      " [-0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -1.        ]]\n",
      "Gu's A:\n",
      " [[-1.         -0.         -0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.         -1.         -0.        ]\n",
      " [-0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -1.        ]]\n",
      "B:\n",
      " [[1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Gu's B:\n",
      " [[1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n"
     ]
    }
   ],
   "source": [
    "test_LagT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Legendre (LegS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LegS():\n",
    "    legs_matrices = TransMatrix(N=8, measure=\"legs\")\n",
    "    A, B = legs_matrices.A, legs_matrices.B\n",
    "    gu_legs_matrices = GuTransMatrix(N=8, measure=\"legs\")\n",
    "    gu_A, gu_B = gu_legs_matrices.A, gu_legs_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -1.         -0.         -0.         -0.         -0.         -0.         -0.         -0.       ]\n",
      " [ -1.7320508  -2.         -0.         -0.         -0.         -0.         -0.         -0.       ]\n",
      " [ -2.2360678  -3.872983   -3.         -0.         -0.         -0.         -0.         -0.       ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -4.         -0.         -0.         -0.         -0.       ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -5.         -0.         -0.         -0.       ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874   -6.         -0.         -0.       ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261   -7.         -0.       ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232  -13.964239   -8.       ]]\n",
      "Gu's A:\n",
      " [[ -1.          0.          0.          0.          0.          0.          0.          0.       ]\n",
      " [ -1.7320508  -1.9999999   0.          0.          0.          0.          0.          0.       ]\n",
      " [ -2.2360678  -3.872983   -3.          0.          0.          0.          0.          0.       ]\n",
      " [ -2.6457512  -4.582576   -5.91608    -4.          0.          0.          0.          0.       ]\n",
      " [ -3.         -5.196152   -6.7082047  -7.9372544  -5.          0.          0.          0.       ]\n",
      " [ -3.3166246  -5.744562   -7.4161987  -8.774965   -9.949874   -6.          0.          0.       ]\n",
      " [ -3.6055512  -6.244998   -8.062259   -9.539392  -10.816654  -11.958261   -7.          0.       ]\n",
      " [ -3.8729832  -6.708204   -8.6602545 -10.246951  -11.61895   -12.845232  -13.964239   -7.9999995]]\n",
      "B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n",
      "Gu's B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n"
     ]
    }
   ],
   "source": [
    "test_LegS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Recurrent Unit (FRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FRU():\n",
    "    fru_matrices = TransMatrix(N=8, measure=\"fru\")\n",
    "    A, B = fru_matrices.A, fru_matrices.B\n",
    "    gu_fru_matrices = GuTransMatrix(N=8, measure=\"fru\")\n",
    "    gu_A, gu_B = gu_fru_matrices.A, gu_fru_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-1.        -0.        -1.4142135  0.        -1.4142135  0.        -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.         0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -2.        -3.1415927 -2.         0.        -2.         0.       ]\n",
      " [ 0.         0.         3.1415927  0.         0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -2.         0.        -2.        -6.2831855 -2.         0.       ]\n",
      " [ 0.         0.         0.         0.         6.2831855  0.         0.         0.       ]\n",
      " [-1.4142135  0.        -2.         0.        -2.         0.        -2.        -9.424778 ]\n",
      " [ 0.         0.         0.         0.         0.         0.         9.424778   0.       ]]\n",
      "Gu's A:\n",
      " [[-1.         0.        -1.4142135  0.        -1.4142135  0.        -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.         0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999 -3.1415927 -1.9999999  0.        -1.9999999  0.       ]\n",
      " [ 0.         0.         3.1415927  0.         0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999  0.        -1.9999999 -6.2831855 -1.9999999  0.       ]\n",
      " [ 0.         0.         0.         0.         6.2831855  0.         0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999  0.        -1.9999999  0.        -1.9999999 -9.424778 ]\n",
      " [ 0.         0.         0.         0.         0.         0.         9.424778   0.       ]]\n",
      "B:\n",
      " [[1.       ]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]]\n",
      "Gu's B:\n",
      " [[1.       ]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]]\n"
     ]
    }
   ],
   "source": [
    "test_FRU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated Fourier (FouT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FouT():\n",
    "    fout_matrices = TransMatrix(N=8, measure=\"fout\")\n",
    "    A, B = fout_matrices.A, fout_matrices.B\n",
    "    gu_fout_matrices = GuTransMatrix(N=8, measure=\"fout\")\n",
    "    gu_A, gu_B = gu_fout_matrices.A, gu_fout_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -2.         -0.         -2.828427    0.         -2.828427    0.         -2.828427    0.       ]\n",
      " [  0.          0.          0.          0.          0.          0.          0.          0.       ]\n",
      " [ -2.828427    0.         -4.         -6.2831855  -4.          0.         -4.          0.       ]\n",
      " [  0.          0.          6.2831855   0.          0.          0.          0.          0.       ]\n",
      " [ -2.828427    0.         -4.          0.         -4.        -12.566371   -4.          0.       ]\n",
      " [  0.          0.          0.          0.         12.566371    0.          0.          0.       ]\n",
      " [ -2.828427    0.         -4.          0.         -4.          0.         -4.        -18.849556 ]\n",
      " [  0.          0.          0.          0.          0.          0.         18.849556    0.       ]]\n",
      "Gu's A:\n",
      " [[ -2.          0.         -2.828427    0.         -2.828427    0.         -2.828427    0.       ]\n",
      " [  0.          0.          0.          0.          0.          0.          0.          0.       ]\n",
      " [ -2.828427    0.         -3.9999998  -6.2831855  -3.9999998   0.         -3.9999998   0.       ]\n",
      " [  0.          0.          6.2831855   0.          0.          0.          0.          0.       ]\n",
      " [ -2.828427    0.         -3.9999998   0.         -3.9999998 -12.566371   -3.9999998   0.       ]\n",
      " [  0.          0.          0.          0.         12.566371    0.          0.          0.       ]\n",
      " [ -2.828427    0.         -3.9999998   0.         -3.9999998   0.         -3.9999998 -18.849556 ]\n",
      " [  0.          0.          0.          0.          0.          0.         18.849556    0.       ]]\n",
      "B:\n",
      " [[2.      ]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]]\n",
      "Gu's B:\n",
      " [[2.      ]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]]\n"
     ]
    }
   ],
   "source": [
    "test_FouT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier With Decay (FourD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FouD():\n",
    "    the_measure = \"foud\"\n",
    "    foud_matrices = TransMatrix(N=8, measure=\"foud\")\n",
    "    A, B = foud_matrices.A, foud_matrices.B\n",
    "    gu_foud_matrices = GuTransMatrix(N=8, measure=\"foud\")\n",
    "    gu_A, gu_B = gu_foud_matrices.A, gu_foud_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-0.5        -0.         -0.70710677  0.         -0.70710677  0.         -0.70710677  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.70710677  0.         -1.         -3.1415927  -1.          0.         -1.          0.        ]\n",
      " [ 0.          0.          3.1415927   0.          0.          0.          0.          0.        ]\n",
      " [-0.70710677  0.         -1.          0.         -1.         -6.2831855  -1.          0.        ]\n",
      " [ 0.          0.          0.          0.          6.2831855   0.          0.          0.        ]\n",
      " [-0.70710677  0.         -1.          0.         -1.          0.         -1.         -9.424778  ]\n",
      " [ 0.          0.          0.          0.          0.          0.          9.424778    0.        ]]\n",
      "Gu's A:\n",
      " [[-0.5         0.         -0.70710677  0.         -0.70710677  0.         -0.70710677  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.70710677  0.         -0.99999994 -3.1415927  -0.99999994  0.         -0.99999994  0.        ]\n",
      " [ 0.          0.          3.1415927   0.          0.          0.          0.          0.        ]\n",
      " [-0.70710677  0.         -0.99999994  0.         -0.99999994 -6.2831855  -0.99999994  0.        ]\n",
      " [ 0.          0.          0.          0.          6.2831855   0.          0.          0.        ]\n",
      " [-0.70710677  0.         -0.99999994  0.         -0.99999994  0.         -0.99999994 -9.424778  ]\n",
      " [ 0.          0.          0.          0.          0.          0.          9.424778    0.        ]]\n",
      "B:\n",
      " [[0.5       ]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]]\n",
      "Gu's B:\n",
      " [[0.5       ]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "test_FouD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities For Gu HiPPO Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(method, c=0.0):\n",
    "    if method in [\"legt\", \"lmu\"]:\n",
    "        fn = lambda x: np.heaviside(x, 0.0) * np.heaviside(1.0 - x, 0.0)\n",
    "    elif method == \"legs\":\n",
    "        fn = lambda x: np.heaviside(x, 1.0) * np.exp(-x)\n",
    "    elif method == \"lagt\":\n",
    "        fn = lambda x: np.heaviside(x, 1.0) * np.exp(-x)\n",
    "    elif method in [\"fourier\", \"fru\", \"fout\", \"foud\"]:\n",
    "        fn = lambda x: np.heaviside(x, 1.0) * np.heaviside(1.0 - x, 1.0)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    fn_tilted = lambda x: np.exp(c * x) * fn(x)\n",
    "    return fn_tilted\n",
    "\n",
    "\n",
    "def basis(method, N, vals, c=0.0, truncate_measure=True):\n",
    "    \"\"\"\n",
    "    vals: list of times (forward in time)\n",
    "    returns: shape (T, N) where T is length of vals\n",
    "    \"\"\"\n",
    "    if method in [\"legt\", \"lmu\"]:\n",
    "        eval_matrix = ss.eval_legendre(np.arange(N)[:, None], 2 * vals - 1).T\n",
    "        eval_matrix *= (2 * np.arange(N) + 1) ** 0.5 * (-1) ** np.arange(N)\n",
    "    elif method == \"legs\":\n",
    "        _vals = np.exp(-vals)\n",
    "        eval_matrix = ss.eval_legendre(np.arange(N)[:, None], 1 - 2 * _vals).T  # (L, N)\n",
    "        eval_matrix *= (2 * np.arange(N) + 1) ** 0.5 * (-1) ** np.arange(N)\n",
    "    elif method == \"lagt\":\n",
    "        vals = vals[::-1]\n",
    "        eval_matrix = ss.eval_genlaguerre(np.arange(N)[:, None], 0, vals)\n",
    "        eval_matrix = eval_matrix * np.exp(-vals / 2)\n",
    "        eval_matrix = eval_matrix.T\n",
    "    elif method in [\"fourier\", \"fru\", \"fout\", \"foud\"]:\n",
    "        cos = 2**0.5 * np.cos(\n",
    "            2 * np.pi * np.arange(N // 2)[:, None] * (vals)\n",
    "        )  # (N/2, T/dt)\n",
    "        sin = 2**0.5 * np.sin(\n",
    "            2 * np.pi * np.arange(N // 2)[:, None] * (vals)\n",
    "        )  # (N/2, T/dt)\n",
    "        cos[0] /= 2**0.5\n",
    "        eval_matrix = np.stack([cos.T, sin.T], axis=-1).reshape(-1, N)  # (T/dt, N)\n",
    "    #     print(\"eval_matrix shape\", eval_matrix.shape)\n",
    "\n",
    "    if truncate_measure:\n",
    "        eval_matrix[measure(method)(vals) == 0.0] = 0.0\n",
    "\n",
    "    p = torch.tensor(eval_matrix)\n",
    "    p *= np.exp(-c * vals)[:, None]  # [::-1, None]\n",
    "    return p\n",
    "\n",
    "\n",
    "def shift_up(a, s=None, drop=True, dim=0):\n",
    "    assert dim == 0\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(a[0, ...])\n",
    "    s = s.unsqueeze(dim)\n",
    "    if drop:\n",
    "        a = a[:-1, ...]\n",
    "    return torch.cat((s, a), dim=dim)\n",
    "\n",
    "\n",
    "def interleave(a, b, uneven=False, dim=0):\n",
    "    \"\"\"Interleave two tensors of same shape\"\"\"\n",
    "    # assert(a.shape == b.shape)\n",
    "    assert dim == 0  # TODO temporary to make handling uneven case easier\n",
    "    if dim < 0:\n",
    "        dim = N + dim\n",
    "    if uneven:\n",
    "        a_ = a[-1:, ...]\n",
    "        a = a[:-1, ...]\n",
    "    c = torch.stack((a, b), dim + 1)\n",
    "    out_shape = list(a.shape)\n",
    "    out_shape[dim] *= 2\n",
    "    c = c.view(out_shape)\n",
    "    if uneven:\n",
    "        c = torch.cat((c, a_), dim=dim)\n",
    "    return c\n",
    "\n",
    "\n",
    "def batch_mult(A, u, has_batch=None):\n",
    "    \"\"\"Matrix mult A @ u with special case to save memory if u has additional batch dim\n",
    "\n",
    "    The batch dimension is assumed to be the second dimension\n",
    "    A : (L, ..., N, N)\n",
    "    u : (L, [B], ..., N)\n",
    "    has_batch: True, False, or None. If None, determined automatically\n",
    "\n",
    "    Output:\n",
    "    x : (L, [B], ..., N)\n",
    "      A @ u broadcasted appropriately\n",
    "    \"\"\"\n",
    "\n",
    "    if has_batch is None:\n",
    "        has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    if has_batch:\n",
    "        u = u.permute([0] + list(range(2, len(u.shape))) + [1])\n",
    "    else:\n",
    "        u = u.unsqueeze(-1)\n",
    "    v = A @ u\n",
    "    if has_batch:\n",
    "        v = v.permute([0] + [len(u.shape) - 1] + list(range(1, len(u.shape) - 1)))\n",
    "    else:\n",
    "        v = v[..., 0]\n",
    "    return v\n",
    "\n",
    "\n",
    "### Main unrolling functions\n",
    "\n",
    "\n",
    "def unroll(A, u):\n",
    "    \"\"\"\n",
    "    A : (..., N, N) # TODO I think this can't take batch dimension?\n",
    "    u : (L, ..., N)\n",
    "    output : x (..., N) # TODO a lot of these shapes are wrong\n",
    "    x[i, ...] = A^{i} @ u[0, ...] + ... + A @ u[i-1, ...] + u[i, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    m = u.new_zeros(u.shape[1:])\n",
    "    outputs = []\n",
    "    for u_ in torch.unbind(u, dim=0):\n",
    "        m = F.linear(m, A) + u_\n",
    "        outputs.append(m)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive(A, u):\n",
    "    \"\"\"Bottom-up divide-and-conquer version of unroll.\"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_(A, u):\n",
    "        if u.shape[0] == 1:\n",
    "            return u\n",
    "\n",
    "        u_evens = u[0::2, ...]\n",
    "        u_odds = u[1::2, ...]\n",
    "\n",
    "        # u2 = F.linear(u_evens, A) + u_odds\n",
    "        u2 = (A @ u_evens.unsqueeze(-1)).squeeze(-1) + u_odds\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_odds = parallel_unroll_recursive_(A2, u2)\n",
    "        # x_evens = F.linear(shift_up(x_odds), A) + u_evens\n",
    "        x_evens = (A @ shift_up(x_odds).unsqueeze(-1)).squeeze(-1) + u_evens\n",
    "\n",
    "        x = interleave(x_evens, x_odds, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n) / math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N - u.shape[0],) + u.shape[1:])), dim=0)\n",
    "\n",
    "    return parallel_unroll_recursive_(A, u)[:n, ...]\n",
    "\n",
    "\n",
    "def parallel_unroll_recursive_br(A, u):\n",
    "    \"\"\"Same as parallel_unroll_recursive but uses bit reversal for locality.\"\"\"\n",
    "\n",
    "    # Main recursive function\n",
    "    def parallel_unroll_recursive_br_(A, u):\n",
    "        n = u.shape[0]\n",
    "        if n == 1:\n",
    "            return u\n",
    "\n",
    "        m = n // 2\n",
    "        u_0 = u[:m, ...]\n",
    "        u_1 = u[m:, ...]\n",
    "\n",
    "        u2 = F.linear(u_0, A) + u_1\n",
    "        A2 = A @ A\n",
    "\n",
    "        x_1 = parallel_unroll_recursive_br_(A2, u2)\n",
    "        x_0 = F.linear(shift_up(x_1), A) + u_0\n",
    "\n",
    "        # x = torch.cat((x_0, x_1), dim=0) # is there a way to do this with cat?\n",
    "        x = interleave(x_0, x_1, dim=0)\n",
    "        return x\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n) / math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N - u.shape[0],) + u.shape[1:])), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    x = parallel_unroll_recursive_br_(A, u)\n",
    "    return x[:n, ...]\n",
    "\n",
    "\n",
    "def parallel_unroll_iterative(A, u):\n",
    "    \"\"\"Bottom-up divide-and-conquer version of unroll, implemented iteratively\"\"\"\n",
    "\n",
    "    # Pad u to power of 2\n",
    "    n = u.shape[0]\n",
    "    m = int(math.ceil(math.log(n) / math.log(2)))\n",
    "    N = 1 << m\n",
    "    u = torch.cat((u, u.new_zeros((N - u.shape[0],) + u.shape[1:])), dim=0)\n",
    "\n",
    "    # Apply bit reversal\n",
    "    br = bitreversal_po2(N)\n",
    "    u = u[br, ...]\n",
    "\n",
    "    # Main recursive loop, flattened\n",
    "    us = []  # stores the u_0 terms in the recursive version\n",
    "    N_ = N\n",
    "    As = []  # stores the A matrices\n",
    "    for l in range(m):\n",
    "        N_ = N_ // 2\n",
    "        As.append(A)\n",
    "        u_0 = u[:N_, ...]\n",
    "        us.append(u_0)\n",
    "        u = F.linear(u_0, A) + u[N_:, ...]\n",
    "        A = A @ A\n",
    "    x_0 = []\n",
    "    x = u  # x_1\n",
    "    for l in range(m - 1, -1, -1):\n",
    "        x_0 = F.linear(shift_up(x), As[l]) + us[l]\n",
    "        x = interleave(x_0, x, dim=0)\n",
    "\n",
    "    return x[:n, ...]\n",
    "\n",
    "\n",
    "def variable_unroll_sequential(A, u, s=None, variable=True):\n",
    "    \"\"\"Unroll with variable (in time/length) transitions A.\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] @ s + A[i..1] @ u[0] + ... + A[i] @ u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        # s = F.linear(s, A_) + u_\n",
    "        s = batch_mult(A_.unsqueeze(0), s.unsqueeze(0), has_batch)[0]\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def variable_unroll(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    \"\"\"Bottom-up divide-and-conquer version of variable_unroll.\"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        return variable_unroll_sequential(A, u, s, variable)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1 = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1 = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = A_1 @ A_0_\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll(A_10, u_10, s, variable, recurse_limit)\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "    x = interleave(\n",
    "        x_0, x_1, uneven, dim=0\n",
    "    )  # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "\n",
    "def variable_unroll_general_sequential(A, u, s, op, variable=True):\n",
    "    \"\"\"Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N, N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (..., N)\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for (A_, u_) in zip(torch.unbind(A, dim=0), torch.unbind(u, dim=0)):\n",
    "        s = op(A_, s)\n",
    "        s = s + u_\n",
    "        outputs.append(s)\n",
    "\n",
    "    output = torch.stack(outputs, dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def variable_unroll_matrix_sequential(A, u, s=None, variable=True):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    return variable_unroll_general_sequential(A, u, s, op, variable=True)\n",
    "\n",
    "\n",
    "def variable_unroll_toeplitz_sequential(A, u, s=None, variable=True, pad=False):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if not variable:\n",
    "        A = A.expand((u.shape[0],) + A.shape)\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]\n",
    "    # op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        ret = variable_unroll_general_sequential(\n",
    "            A, u, s, triangular_toeplitz_multiply_padded, variable=True\n",
    "        )\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    return variable_unroll_general_sequential(\n",
    "        A, u, s, triangular_toeplitz_multiply, variable=True\n",
    "    )\n",
    "\n",
    "\n",
    "### General parallel scan functions with generic binary composition operators\n",
    "\n",
    "\n",
    "def variable_unroll_general(\n",
    "    A, u, s, op, compose_op=None, sequential_op=None, variable=True, recurse_limit=16\n",
    "):\n",
    "    \"\"\"Bottom-up divide-and-conquer version of variable_unroll.\n",
    "\n",
    "    compose is an optional function that defines how to compose A without multiplying by a leaf u\n",
    "    \"\"\"\n",
    "\n",
    "    if u.shape[0] <= recurse_limit:\n",
    "        if sequential_op is None:\n",
    "            sequential_op = op\n",
    "        return variable_unroll_general_sequential(A, u, s, sequential_op, variable)\n",
    "\n",
    "    if compose_op is None:\n",
    "        compose_op = op\n",
    "\n",
    "    uneven = u.shape[0] % 2 == 1\n",
    "    # has_batch = len(u.shape) >= len(A.shape)\n",
    "\n",
    "    u_0 = u[0::2, ...]\n",
    "    u_1 = u[1::2, ...]\n",
    "\n",
    "    if variable:\n",
    "        A_0 = A[0::2, ...]\n",
    "        A_1 = A[1::2, ...]\n",
    "    else:\n",
    "        A_0 = A\n",
    "        A_1 = A\n",
    "\n",
    "    u_0_ = u_0\n",
    "    A_0_ = A_0\n",
    "    if uneven:\n",
    "        u_0_ = u_0[:-1, ...]\n",
    "        if variable:\n",
    "            A_0_ = A_0[:-1, ...]\n",
    "\n",
    "    u_10 = op(A_1, u_0_)  # batch_mult(A_1, u_0_, has_batch)\n",
    "    u_10 = u_10 + u_1\n",
    "    A_10 = compose_op(A_1, A_0_)\n",
    "\n",
    "    # Recursive call\n",
    "    x_1 = variable_unroll_general(\n",
    "        A_10,\n",
    "        u_10,\n",
    "        s,\n",
    "        op,\n",
    "        compose_op,\n",
    "        sequential_op,\n",
    "        variable=variable,\n",
    "        recurse_limit=recurse_limit,\n",
    "    )\n",
    "\n",
    "    x_0 = shift_up(x_1, s, drop=not uneven)\n",
    "    x_0 = op(A_0, x_0)  # batch_mult(A_0, x_0, has_batch)\n",
    "    x_0 = x_0 + u_0\n",
    "\n",
    "    x = interleave(\n",
    "        x_0, x_1, uneven, dim=0\n",
    "    )  # For some reason this interleave is slower than in the (non-multi) unroll_recursive\n",
    "    return x\n",
    "\n",
    "\n",
    "def variable_unroll_matrix(A, u, s=None, variable=True, recurse_limit=16):\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "    has_batch = len(u.shape) >= len(A.shape)\n",
    "    op = lambda x, y: batch_mult(x, y, has_batch)\n",
    "    sequential_op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[\n",
    "        0\n",
    "    ]\n",
    "    matmul = lambda x, y: x @ y\n",
    "    return variable_unroll_general(\n",
    "        A,\n",
    "        u,\n",
    "        s,\n",
    "        op,\n",
    "        compose_op=matmul,\n",
    "        sequential_op=sequential_op,\n",
    "        variable=variable,\n",
    "        recurse_limit=recurse_limit,\n",
    "    )\n",
    "\n",
    "\n",
    "def variable_unroll_toeplitz(A, u, s=None, variable=True, recurse_limit=8, pad=False):\n",
    "    \"\"\"Unroll with variable (in time/length) transitions A with general associative operation\n",
    "\n",
    "    A : ([L], ..., N) dimension L should exist iff variable is True\n",
    "    u : (L, [B], ..., N) updates\n",
    "    s : ([B], ..., N) start state\n",
    "    output : x (L, [B], ..., N) same shape as u\n",
    "    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n",
    "    \"\"\"\n",
    "    # Add the batch dimension to A if necessary\n",
    "    A_batch_dims = len(A.shape) - int(variable)\n",
    "    u_batch_dims = len(u.shape) - 1\n",
    "    if u_batch_dims > A_batch_dims:\n",
    "        # assert u_batch_dims == A_batch_dims + 1\n",
    "        if variable:\n",
    "            while len(A.shape) < len(u.shape):\n",
    "                A = A.unsqueeze(1)\n",
    "        # else:\n",
    "        #     A = A.unsqueeze(0)\n",
    "\n",
    "    if s is None:\n",
    "        s = torch.zeros_like(u[0])\n",
    "\n",
    "    if pad:\n",
    "        n = A.shape[-1]\n",
    "        A = F.pad(A, (0, n))\n",
    "        u = F.pad(u, (0, n))\n",
    "        s = F.pad(s, (0, n))\n",
    "        op = triangular_toeplitz_multiply_padded\n",
    "        ret = variable_unroll_general(\n",
    "            A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit\n",
    "        )\n",
    "        ret = ret[..., :n]\n",
    "        return ret\n",
    "\n",
    "    op = triangular_toeplitz_multiply\n",
    "    ret = variable_unroll_general(\n",
    "        A, u, s, op, compose_op=op, variable=variable, recurse_limit=recurse_limit\n",
    "    )\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gu's HiPPO LegT Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LSI(nn.Module):\n",
    "    \"\"\"Vanilla HiPPO-LegS model (scale invariant instead of time invariant)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        N,\n",
    "        method=\"legs\",\n",
    "        max_length=1024,\n",
    "        discretization=0.5,\n",
    "        lambda_n=1.0,\n",
    "        alpha=0.0,\n",
    "        beta=1.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        max_length: maximum sequence length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        matrices = GuTransMatrix(\n",
    "            N=N, measure=method, lambda_n=lambda_n, alpha=alpha, beta=beta\n",
    "        )\n",
    "        A = matrices.A\n",
    "        B = matrices.B\n",
    "        # A, B = transition(method, N)\n",
    "        B = B.squeeze(-1)\n",
    "        A_stacked = np.empty((max_length, N, N), dtype=A.dtype)\n",
    "        B_stacked = np.empty((max_length, N), dtype=B.dtype)\n",
    "        for t in range(1, max_length + 1):\n",
    "            At = A / t\n",
    "            Bt = B / t\n",
    "            if discretization == 0.0:  # forward\n",
    "                A_stacked[t - 1] = np.eye(N) + At\n",
    "                B_stacked[t - 1] = Bt\n",
    "            elif discretization == 1.0:  # backward\n",
    "                A_stacked[t - 1] = la.solve_triangular(\n",
    "                    np.eye(N) - At, np.eye(N), lower=True\n",
    "                )\n",
    "                B_stacked[t - 1] = la.solve_triangular(np.eye(N) - At, Bt, lower=True)\n",
    "            elif discretization == 0.5:  # bilinear\n",
    "                # A_stacked[t - 1] = la.solve_triangular(\n",
    "                #     np.eye(N) - At / 2, np.eye(N) + At / 2, lower=True\n",
    "                # )\n",
    "                # B_stacked[t - 1] = la.solve_triangular(\n",
    "                #     np.eye(N) - At / 2, Bt, lower=True\n",
    "                # )\n",
    "                alpha = 0.5\n",
    "                A_stacked[t - 1] = np.linalg.lstsq(\n",
    "                    np.eye(N) - (At * alpha), np.eye(N) + (At * alpha), rcond=None\n",
    "                )[\n",
    "                    0\n",
    "                ]  # TODO: Referencing this: https://stackoverflow.com/questions/64527098/numpy-linalg-linalgerror-singular-matrix-error-when-trying-to-solve\n",
    "                B_stacked[t - 1] = np.linalg.lstsq(\n",
    "                    np.eye(N) - (At * alpha), Bt, rcond=None\n",
    "                )[0]\n",
    "            else:  # ZOH\n",
    "                A_stacked[t - 1] = la.expm(A * (math.log(t + 1) - math.log(t)))\n",
    "                B_stacked[t - 1] = la.solve_triangular(\n",
    "                    A, A_stacked[t - 1] @ B - B, lower=True\n",
    "                )\n",
    "        self.A_stacked = torch.Tensor(A_stacked.copy())  # (max_length, N, N)\n",
    "        self.B_stacked = torch.Tensor(B_stacked.copy())  # (max_length, N)\n",
    "\n",
    "        vals = np.linspace(0.0, 1.0, max_length)\n",
    "        self.eval_matrix = torch.from_numpy(\n",
    "            np.asarray(\n",
    "                ((B[:, None] * ss.eval_legendre(np.arange(N)[:, None], 2 * vals - 1)).T)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, fast=True):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "\n",
    "        L = inputs.shape[0]\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = torch.transpose(inputs, 0, -2)\n",
    "        u = u * self.B_stacked[:L]\n",
    "        print(f\"Gu - u * self.B_stacked[:L]: {u}\")\n",
    "        u = torch.transpose(u, 0, -2)  # (length, ..., N)\n",
    "\n",
    "        if fast:\n",
    "            result = variable_unroll_matrix(self.A_stacked[:L], u)\n",
    "            return result\n",
    "\n",
    "        c = torch.zeros(u.shape[1:]).to(inputs)\n",
    "        cs = []\n",
    "        for t, f in enumerate(inputs):\n",
    "            c = F.linear(c, self.A_stacked[t]) + self.B_stacked[t] * f\n",
    "            cs.append(c)\n",
    "        return torch.stack(cs, dim=0)\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        a = self.eval_matrix.to(c) @ c.unsqueeze(-1)\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gu's Scale invariant HiPPO LegS Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HiPPO_LTI(nn.Module):\n",
    "    \"\"\"Linear time invariant x' = Ax + Bu\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        N,\n",
    "        method=\"legt\",\n",
    "        dt=1.0,\n",
    "        T=1.0,\n",
    "        discretization=0.5,\n",
    "        lambda_n=1.0,\n",
    "        alpha=0.0,\n",
    "        beta=1.0,\n",
    "        c=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        N: the order of the HiPPO projection\n",
    "        dt: discretization step size - should be roughly inverse to the length of the sequence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.method = method\n",
    "        self.N = N\n",
    "        self.dt = dt\n",
    "        self.T = T\n",
    "        self.c = c\n",
    "\n",
    "        matrices = GuTransMatrix(\n",
    "            N=N, measure=method, lambda_n=lambda_n, alpha=alpha, beta=beta\n",
    "        )\n",
    "        A = matrices.A\n",
    "        B = matrices.B\n",
    "        # A, B = transition(method, N)\n",
    "        A = A + np.eye(N) * c\n",
    "        self.A = A\n",
    "        self.B = B.squeeze(-1)\n",
    "        self.measure_fn = measure(method)\n",
    "\n",
    "        C = np.ones((1, N))\n",
    "        D = np.zeros((1,))\n",
    "        if type(discretization) in [float, int]:\n",
    "            dA, dB, _, _, _ = signal.cont2discrete(\n",
    "                (A, B, C, D), dt=dt, method=\"gbt\", alpha=discretization\n",
    "            )\n",
    "        else:\n",
    "            dA, dB, _, _, _ = signal.cont2discrete((A, B, C, D), dt=dt, method=\"zoh\")\n",
    "\n",
    "        dB = dB.squeeze(-1)\n",
    "\n",
    "        self.register_buffer(\"dA\", torch.Tensor(dA))  # (N, N)\n",
    "        self.register_buffer(\"dB\", torch.Tensor(dB))  # (N,)\n",
    "\n",
    "        self.vals = np.arange(0.0, T, dt)\n",
    "        self.eval_matrix = basis(self.method, self.N, self.vals, c=self.c)  # (T/dt, N)\n",
    "        self.measure = measure(self.method)(self.vals)\n",
    "\n",
    "    def forward(self, inputs, fast=True):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = inputs * self.dB  # (length, ..., N)\n",
    "\n",
    "        if fast:\n",
    "            dA = repeat(self.dA, \"m n -> l m n\", l=u.size(0))\n",
    "            return variable_unroll_matrix(dA, u)\n",
    "\n",
    "        c = torch.zeros(u.shape[1:]).to(inputs)\n",
    "        cs = []\n",
    "        for f in inputs:\n",
    "            c = F.linear(c, self.dA) + self.dB * f\n",
    "            cs.append(c)\n",
    "        return torch.stack(cs, dim=0)\n",
    "\n",
    "    def reconstruct(\n",
    "        self, c, evals=None\n",
    "    ):  # TODO take in a times array for reconstruction\n",
    "        \"\"\"\n",
    "        c: (..., N,) HiPPO coefficients (same as x(t) in S4 notation)\n",
    "        output: (..., L,)\n",
    "        \"\"\"\n",
    "        if evals is not None:\n",
    "            eval_matrix = basis(self.method, self.N, evals)\n",
    "        else:\n",
    "            eval_matrix = self.eval_matrix\n",
    "\n",
    "        m = self.measure[self.measure != 0.0]\n",
    "\n",
    "        c = c.unsqueeze(-1)\n",
    "        y = eval_matrix.to(c) @ c\n",
    "        return y.squeeze(-1).flip(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Of General HiPPO Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nb_HiPPO(jnn.Module):\n",
    "    \"\"\"\n",
    "    class that constructs HiPPO model using the defined measure.\n",
    "\n",
    "    Args:\n",
    "        N (int): order of the HiPPO projection, aka the number of coefficients to describe the matrix\n",
    "        max_length (int): maximum sequence length to be input\n",
    "        measure (str): the measure used to define which way to instantiate the HiPPO matrix\n",
    "        step (float): step size used for descretization\n",
    "        GBT_alpha (float): represents which descretization transformation to use based off the alpha value\n",
    "        seq_L (int): length of the sequence to be used for training\n",
    "        v (str): choice of vectorized or non-vectorized function instantiation\n",
    "            - 'v': vectorized\n",
    "            - 'nv': non-vectorized\n",
    "        lambda_n (float): value associated with the tilt of legt\n",
    "            - 1: tilt on legt\n",
    "            - \\sqrt(2n+1)(-1)^{N}: tilt associated with the legendre memory unit (LMU)\n",
    "        fourier_type (str): choice of fourier measures\n",
    "            - fru: fourier recurrent unit measure (FRU) - 'fru'\n",
    "            - fout: truncated Fourier (FouT) - 'fout'\n",
    "            - fourd: decaying fourier transform - 'fourd'\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "    \"\"\"\n",
    "\n",
    "    N: int\n",
    "    max_length: int\n",
    "    step: float\n",
    "    GBT_alpha: float\n",
    "    seq_L: int\n",
    "    A: jnp.ndarray\n",
    "    B: jnp.ndarray\n",
    "    measure: str\n",
    "\n",
    "    def setup(self):\n",
    "        A = self.A\n",
    "        B = self.B\n",
    "        self.C = jnp.ones((self.N,))\n",
    "        self.D = jnp.zeros((1,))\n",
    "\n",
    "        if self.measure == \"legt\":\n",
    "            L = self.seq_L\n",
    "            vals = jnp.arange(0.0, 1.0, L)\n",
    "            # n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 1 - 2 * vals\n",
    "            self.eval_matrix = jax.scipy.special.lpmn_values(\n",
    "                m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "            ).T  # ss.eval_legendre(n, x).T\n",
    "\n",
    "        elif self.measure == \"legs\":\n",
    "            L = self.max_length\n",
    "            vals = jnp.linspace(0.0, 1.0, L)\n",
    "            # n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 2 * vals - 1\n",
    "            self.eval_matrix = (\n",
    "                B[:, None]\n",
    "                * jax.scipy.special.lpmn_values(\n",
    "                    m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "                )\n",
    "            ).T  # ss.eval_legendre(n, x)).T\n",
    "\n",
    "        elif self.measure == \"lagt\":\n",
    "            raise NotImplementedError(\"Translated Laguerre measure not implemented yet\")\n",
    "\n",
    "        elif self.measure == \"fourier\":\n",
    "            raise NotImplementedError(\"Fourier measures are not implemented yet\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"invalid measure\")\n",
    "\n",
    "    def __call__(self, f, init_state=None, t_step=0, kernel=False):\n",
    "        # print(f\"u shape:\\n{f.shape}\")\n",
    "        # print(f\"u:\\n{f}\")\n",
    "        if not kernel:\n",
    "            if init_state is None:\n",
    "                init_state = jnp.zeros((self.N, 1))\n",
    "\n",
    "            # Ab, Bb, Cb, Db = self.collect_SSM_vars(\n",
    "            #     self.A, self.B, self.C, self.D, f, t_step=t_step, alpha=self.GBT_alpha\n",
    "            # )\n",
    "            c_k, y_k, GBT_A, GBT_B = self.loop_SSM(\n",
    "                A=self.A,\n",
    "                B=self.B,\n",
    "                C=self.C,\n",
    "                D=self.D,\n",
    "                c_0=init_state,\n",
    "                f=f,\n",
    "                alpha=self.GBT_alpha,\n",
    "            )\n",
    "            # c_k, y_k = self.scan_SSM(Ab=Ab, Bb=Bb, Cb=Cb, Db=Db, c_0=init_state, f=f)\n",
    "\n",
    "        else:\n",
    "            Ab, Bb, Cb, Db = self.discretize(\n",
    "                self.A, self.B, self.C, self.D, step=self.step, alpha=self.GBT_alpha\n",
    "            )\n",
    "            c_k, y_k = self.causal_convolution(\n",
    "                f, self.K_conv(Ab, Bb, Cb, Db, L=self.max_length)\n",
    "            )\n",
    "\n",
    "        return c_k, y_k, GBT_A, GBT_B\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        \"\"\"\n",
    "        Uses coeffecients to reconstruct the signal\n",
    "\n",
    "        Args:\n",
    "            c (jnp.ndarray): coefficients of the HiPPO projection\n",
    "\n",
    "        Returns:\n",
    "            reconstructed signal\n",
    "        \"\"\"\n",
    "        return (self.eval_matrix @ jnp.expand_dims(c, -1)).squeeze(-1)\n",
    "\n",
    "    def discretize(self, A, B, C, D, step, alpha=0.5):\n",
    "        \"\"\"\n",
    "        function used for discretizing the HiPPO matrix\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            step (float): step size used for discretization\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "                - forward Euler corresponds to α = 0,\n",
    "                - backward Euler corresponds to α = 1,\n",
    "                - bilinear corresponds to α = 0.5,\n",
    "                - Zero-order Hold corresponds to α > 1\n",
    "        \"\"\"\n",
    "        I = jnp.eye(A.shape[0])\n",
    "        step_size = 1 / step\n",
    "        part1 = I - (step_size * alpha * A)\n",
    "        part2 = I + (step_size * (1 - alpha) * A)\n",
    "\n",
    "        GBT_A = jnp.linalg.lstsq(part1, part2, rcond=None)[0]\n",
    "\n",
    "        base_GBT_B = jnp.linalg.lstsq(part1, B, rcond=None)[0]\n",
    "        GBT_B = step_size * base_GBT_B\n",
    "\n",
    "        if alpha > 1:  # Zero-order Hold\n",
    "            GBT_A = jax.scipy.linalg.expm(step_size * A)\n",
    "            GBT_B = (jnp.linalg.inv(A) @ (jax.scipy.linalg.expm(step_size * A) - I)) @ B\n",
    "\n",
    "        return (\n",
    "            GBT_A.astype(jnp.float32),\n",
    "            GBT_B.astype(jnp.float32),\n",
    "            C.astype(jnp.float32),\n",
    "            D.astype(jnp.float32),\n",
    "        )\n",
    "\n",
    "    def collect_SSM_vars(self, A, B, C, D, f, t_step=0, alpha=0.5):\n",
    "        \"\"\"\n",
    "        turns the continuos HiPPO matrix components into discrete ones\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            f (jnp.ndarray): input signal\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "\n",
    "        Returns:\n",
    "            Ab (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Bb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Cb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Db (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "        \"\"\"\n",
    "        N = A.shape[0]\n",
    "\n",
    "        if t_step == 0:\n",
    "            L = f.shape[0]  # seq_L, 1\n",
    "            assert (\n",
    "                L == self.seq_L\n",
    "            ), f\"sequence length must match, currently {L} != {self.seq_L}\"\n",
    "            assert N == self.N, f\"Order number must match, currently {N} != {self.N}\"\n",
    "        else:\n",
    "            L = t_step\n",
    "            assert t_step >= 1, f\"time step must be greater than 0, currently {t_step}\"\n",
    "            assert N == self.N, f\"Order number must match, currently {N} != {self.N}\"\n",
    "\n",
    "        Ab, Bb, Cb, Db = self.discretize(A, B, C, D, step=L, alpha=alpha)\n",
    "\n",
    "        return (\n",
    "            Ab.astype(jnp.float32),\n",
    "            Bb.astype(jnp.float32),\n",
    "            Cb.astype(jnp.float32),\n",
    "            Db.astype(jnp.float32),\n",
    "        )\n",
    "\n",
    "    def scan_SSM(self, Ad, Bd, Cd, Dd, c_0, f):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            f (jnp.ndarray): the input sequence\n",
    "            c_0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "\n",
    "        def step(c_k_1, f_k):\n",
    "            \"\"\"\n",
    "            Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "            Args:\n",
    "                c_k_1: previous hidden state\n",
    "                f_k: output from function f at, descritized, time step, k.\n",
    "                t:\n",
    "\n",
    "            Returns:\n",
    "                c_k: current hidden state\n",
    "                y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "            \"\"\"\n",
    "            part1 = Ad @ c_k_1\n",
    "            part2 = jnp.expand_dims((Bd @ f_k), -1)\n",
    "\n",
    "            c_k = part1 + part2\n",
    "            y_k = Cd @ c_k  # + (Db.T @ f_k)\n",
    "\n",
    "            return c_k, y_k\n",
    "\n",
    "        return jax.lax.scan(step, c_0, f)\n",
    "\n",
    "    def loop_SSM(self, A, B, C, D, c_0, f, alpha=0.5):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            f (jnp.ndarray): the input sequence\n",
    "            c_0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "        GBT_A_lst = []\n",
    "        GBT_B_lst = []\n",
    "        c_k_list = []\n",
    "        y_k_list = []\n",
    "\n",
    "        c_k = c_0.copy()\n",
    "        print(f\"no batch f shape: {f.shape}\")\n",
    "        print(f\"no batch c_k shape: {c_k.shape}\")\n",
    "        for i in range(1, f.shape[0] + 1):\n",
    "            Ad_i, Bd_i, Cd_i, Dd_i = self.collect_SSM_vars(\n",
    "                A=A, B=B, C=C, D=D, f=f, t_step=i, alpha=alpha\n",
    "            )\n",
    "            c_k, y_k = self.loop_step(\n",
    "                Ad=Ad_i, Bd=Bd_i, Cd=Cd_i, Dd=Dd_i, c_k_i=c_k, f_k=f[i - 1][0]\n",
    "            )\n",
    "            c_k_list.append(c_k.copy())\n",
    "            y_k_list.append(y_k.copy())\n",
    "            GBT_A_lst.append(Ad_i.copy())\n",
    "            GBT_B_lst.append(Bd_i.copy())\n",
    "\n",
    "        return c_k_list, y_k_list, GBT_A_lst, GBT_B_lst\n",
    "\n",
    "    def loop_step(self, Ad, Bd, Cd, Dd, c_k_i, f_k):\n",
    "        \"\"\"\n",
    "        Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "        Args:\n",
    "            c_k_i: previous hidden state\n",
    "            f_k: output from function f at, descritized, time step, k.\n",
    "\n",
    "        Returns:\n",
    "            c_k: current hidden state\n",
    "            y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "        \"\"\"\n",
    "        # print(f\"c_k_i:\\n{c_k_i}\")\n",
    "        # print(f\"f_k:\\n{f_k}\")\n",
    "\n",
    "        part1 = Ad @ c_k_i\n",
    "        part2 = Bd * f_k\n",
    "        c_k = part1 + part2\n",
    "        y_k = Cd @ c_k  # + (Db.T @ f_k)\n",
    "\n",
    "        return c_k.astype(jnp.float32), y_k.astype(jnp.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class b_HiPPO(jnn.Module):\n",
    "    \"\"\"\n",
    "    class that constructs HiPPO model using the defined measure.\n",
    "\n",
    "    Args:\n",
    "        N (int): order of the HiPPO projection, aka the number of coefficients to describe the matrix\n",
    "        max_length (int): maximum sequence length to be input\n",
    "        measure (str): the measure used to define which way to instantiate the HiPPO matrix\n",
    "        step (float): step size used for descretization\n",
    "        GBT_alpha (float): represents which descretization transformation to use based off the alpha value\n",
    "        seq_L (int): length of the sequence to be used for training\n",
    "        v (str): choice of vectorized or non-vectorized function instantiation\n",
    "            - 'v': vectorized\n",
    "            - 'nv': non-vectorized\n",
    "        lambda_n (float): value associated with the tilt of legt\n",
    "            - 1: tilt on legt\n",
    "            - \\sqrt(2n+1)(-1)^{N}: tilt associated with the legendre memory unit (LMU)\n",
    "        fourier_type (str): choice of fourier measures\n",
    "            - fru: fourier recurrent unit measure (FRU) - 'fru'\n",
    "            - fout: truncated Fourier (FouT) - 'fout'\n",
    "            - fourd: decaying fourier transform - 'fourd'\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "    \"\"\"\n",
    "\n",
    "    N: int\n",
    "    max_length: int\n",
    "    step: float\n",
    "    GBT_alpha: float\n",
    "    seq_L: int\n",
    "    A: jnp.ndarray\n",
    "    B: jnp.ndarray\n",
    "    measure: str\n",
    "\n",
    "    def setup(self):\n",
    "        A = self.A\n",
    "        B = self.B\n",
    "        self.C = jnp.ones((self.N,))\n",
    "        self.D = jnp.zeros((1,))\n",
    "\n",
    "        if self.measure == \"legt\":\n",
    "            L = self.seq_L\n",
    "            vals = jnp.arange(0.0, 1.0, L)\n",
    "            # n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 1 - 2 * vals\n",
    "            self.eval_matrix = jax.scipy.special.lpmn_values(\n",
    "                m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "            ).T  # ss.eval_legendre(n, x).T\n",
    "\n",
    "        elif self.measure == \"legs\":\n",
    "            L = self.max_length\n",
    "            vals = jnp.linspace(0.0, 1.0, L)\n",
    "            # n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 2 * vals - 1\n",
    "            self.eval_matrix = (\n",
    "                B[:, None]\n",
    "                * jax.scipy.special.lpmn_values(\n",
    "                    m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "                )\n",
    "            ).T  # ss.eval_legendre(n, x)).T\n",
    "\n",
    "        elif self.measure == \"lagt\":\n",
    "            raise NotImplementedError(\"Translated Laguerre measure not implemented yet\")\n",
    "\n",
    "        elif self.measure == \"fourier\":\n",
    "            raise NotImplementedError(\"Fourier measures are not implemented yet\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"invalid measure\")\n",
    "\n",
    "    def __call__(self, f, init_state=None, t_step=0, kernel=False):\n",
    "        # print(f\"u shape:\\n{f.shape}\")\n",
    "        # print(f\"u:\\n{f}\")\n",
    "        if not kernel:\n",
    "            if init_state is None:\n",
    "                init_state = jnp.zeros((f.shape[0], self.N, 1))\n",
    "\n",
    "            # Ab, Bb, Cb, Db = self.collect_SSM_vars(\n",
    "            #     self.A, self.B, self.C, self.D, f, t_step=t_step, alpha=self.GBT_alpha\n",
    "            # )\n",
    "            c_k, y_k, GBT_A, GBT_B = self.loop_SSM(\n",
    "                A=self.A,\n",
    "                B=self.B,\n",
    "                C=self.C,\n",
    "                D=self.D,\n",
    "                c_0=init_state,\n",
    "                f=f,\n",
    "                alpha=self.GBT_alpha,\n",
    "            )\n",
    "            # c_k, y_k = self.scan_SSM(Ab=Ab, Bb=Bb, Cb=Cb, Db=Db, c_0=init_state, f=f)\n",
    "\n",
    "        else:\n",
    "            Ab, Bb, Cb, Db = self.discretize(\n",
    "                self.A, self.B, self.C, self.D, step=self.step, alpha=self.GBT_alpha\n",
    "            )\n",
    "            c_k, y_k = self.causal_convolution(\n",
    "                f, self.K_conv(Ab, Bb, Cb, Db, L=self.max_length)\n",
    "            )\n",
    "\n",
    "        return c_k, y_k, GBT_A, GBT_B\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        \"\"\"\n",
    "        Uses coeffecients to reconstruct the signal\n",
    "\n",
    "        Args:\n",
    "            c (jnp.ndarray): coefficients of the HiPPO projection\n",
    "\n",
    "        Returns:\n",
    "            reconstructed signal\n",
    "        \"\"\"\n",
    "        return (self.eval_matrix @ jnp.expand_dims(c, -1)).squeeze(-1)\n",
    "\n",
    "    def discretize(self, A, B, C, D, step, alpha=0.5):\n",
    "        \"\"\"\n",
    "        function used for discretizing the HiPPO matrix\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            step (float): step size used for discretization\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "                - forward Euler corresponds to α = 0,\n",
    "                - backward Euler corresponds to α = 1,\n",
    "                - bilinear corresponds to α = 0.5,\n",
    "                - Zero-order Hold corresponds to α > 1\n",
    "        \"\"\"\n",
    "        I = jnp.eye(A.shape[0])\n",
    "        step_size = 1 / step\n",
    "        part1 = I - (step_size * alpha * A)\n",
    "        part2 = I + (step_size * (1 - alpha) * A)\n",
    "\n",
    "        GBT_A = jnp.linalg.lstsq(part1, part2, rcond=None)[0]\n",
    "\n",
    "        base_GBT_B = jnp.linalg.lstsq(part1, B, rcond=None)[0]\n",
    "        GBT_B = step_size * base_GBT_B\n",
    "\n",
    "        if alpha > 1:  # Zero-order Hold\n",
    "            GBT_A = jax.scipy.linalg.expm(step_size * A)\n",
    "            GBT_B = (jnp.linalg.inv(A) @ (jax.scipy.linalg.expm(step_size * A) - I)) @ B\n",
    "\n",
    "        return (\n",
    "            GBT_A.astype(jnp.float32),\n",
    "            GBT_B.astype(jnp.float32),\n",
    "            C.astype(jnp.float32),\n",
    "            D.astype(jnp.float32),\n",
    "        )\n",
    "\n",
    "    def collect_SSM_vars(self, A, B, C, D, f, t_step=0, alpha=0.5):\n",
    "        \"\"\"\n",
    "        turns the continuos HiPPO matrix components into discrete ones\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            f (jnp.ndarray): input signal\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "\n",
    "        Returns:\n",
    "            Ab (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Bb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Cb (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "            Db (jnp.ndarray): discrete form of the HiPPO matrix\n",
    "        \"\"\"\n",
    "        N = A.shape[0]\n",
    "\n",
    "        if t_step == 0:\n",
    "            L = f.shape[1]  # seq_L, 1\n",
    "            assert (\n",
    "                L == self.seq_L\n",
    "            ), f\"sequence length must match, currently {L} != {self.seq_L}\"\n",
    "            assert N == self.N, f\"Order number must match, currently {N} != {self.N}\"\n",
    "        else:\n",
    "            L = t_step\n",
    "            assert t_step >= 1, f\"time step must be greater than 0, currently {t_step}\"\n",
    "            assert N == self.N, f\"Order number must match, currently {N} != {self.N}\"\n",
    "\n",
    "        Ab, Bb, Cb, Db = self.discretize(A, B, C, D, step=L, alpha=alpha)\n",
    "\n",
    "        return (\n",
    "            Ab.astype(jnp.float32),\n",
    "            Bb.astype(jnp.float32),\n",
    "            Cb.astype(jnp.float32),\n",
    "            Db.astype(jnp.float32),\n",
    "        )\n",
    "\n",
    "    def scan_SSM(self, Ad, Bd, Cd, Dd, c_0, f):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            f (jnp.ndarray): the input sequence\n",
    "            c_0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "\n",
    "        def step(c_k_1, f_k):\n",
    "            \"\"\"\n",
    "            Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "            Args:\n",
    "                c_k_1: previous hidden state\n",
    "                f_k: output from function f at, descritized, time step, k.\n",
    "                t:\n",
    "\n",
    "            Returns:\n",
    "                c_k: current hidden state\n",
    "                y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "            \"\"\"\n",
    "            part1 = Ad @ c_k_1\n",
    "            part2 = jnp.expand_dims((Bd @ f_k), -1)\n",
    "\n",
    "            c_k = part1 + part2\n",
    "            y_k = Cd @ c_k  # + (Db.T @ f_k)\n",
    "\n",
    "            return c_k, y_k\n",
    "\n",
    "        return jax.lax.scan(step, c_0, f)\n",
    "\n",
    "    def loop_SSM(self, A, B, C, D, c_0, f, alpha=0.5):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            f (jnp.ndarray): the input sequence\n",
    "            c_0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "        GBT_A_lst = []\n",
    "        GBT_B_lst = []\n",
    "        c_k_list = []\n",
    "        y_k_list = []\n",
    "\n",
    "        c_k = c_0.copy()\n",
    "        print(f\"f shape:{f.shape}\")\n",
    "        print(f\"c_k shape:{c_k.shape}\")\n",
    "        # jax.debug.print(f\"f:\\n{f}\")\n",
    "        for i in range(1, f.shape[1] + 1):\n",
    "            Ad_i, Bd_i, Cd_i, Dd_i = self.collect_SSM_vars(\n",
    "                A=A, B=B, C=C, D=D, f=f, t_step=i, alpha=alpha\n",
    "            )\n",
    "            # jax.debug.print(f\"f[:,i-1,:] shape: {f[:,i-1,:].shape}\")\n",
    "            # jax.debug.print(f\"f[:,i-1,:]: {f[:,i-1,:]}\")\n",
    "            # print(f\"f[i - 1][0] shape: {f[i - 1].shape}\")\n",
    "            # print(f\"f[i - 1][0]: {f[i - 1]}\")\n",
    "            # print(f\"c_k shape: {c_k.shape}\")\n",
    "            # c_k, y_k = self.loop_step(\n",
    "            #     Ad=Ad_i, Bd=Bd_i, Cd=Cd_i, Dd=Dd_i, c_k_i=c_k, f_k=f[i-1,:][0]\n",
    "            # )\n",
    "            c_k, y_k = jax.vmap(self.loop_step, in_axes=(None, None, None, None, 0, 0))(\n",
    "                Ad_i, Bd_i, Cd_i, Dd_i, c_k, f[:,i-1,:]\n",
    "            )\n",
    "            c_k_list.append(c_k.copy())\n",
    "            y_k_list.append(y_k.copy())\n",
    "            GBT_A_lst.append(Ad_i.copy())\n",
    "            GBT_B_lst.append(Bd_i.copy())\n",
    "\n",
    "        return c_k_list, y_k_list, GBT_A_lst, GBT_B_lst\n",
    "\n",
    "    def loop_step(self, Ad, Bd, Cd, Dd, c_k_i, f_k):\n",
    "        \"\"\"\n",
    "        Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "        Args:\n",
    "            c_k_i: previous hidden state\n",
    "            f_k: output from function f at, descritized, time step, k.\n",
    "\n",
    "        Returns:\n",
    "            c_k: current hidden state\n",
    "            y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "        \"\"\"\n",
    "        # jax.debug.print(f\"c_k_i:\\n{c_k_i}\")\n",
    "        # jax.debug.print(f\"f_k:\\n{f_k}\")\n",
    "        \n",
    "        part1 = Ad @ c_k_i\n",
    "        part2 = Bd * f_k\n",
    "        c_k = part1 + part2\n",
    "        y_k = Cd @ c_k  # + (Db.T @ f_k)\n",
    "\n",
    "        return c_k.astype(jnp.float32), y_k.astype(jnp.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO(jnn.Module):\n",
    "    \"\"\"\n",
    "    class that constructs HiPPO model using the defined measure.\n",
    "\n",
    "    Args:\n",
    "        N (int): order of the HiPPO projection, aka the number of coefficients to describe the matrix\n",
    "        max_length (int): maximum sequence length to be input\n",
    "        measure (str): the measure used to define which way to instantiate the HiPPO matrix\n",
    "        step (float): step size used for descretization\n",
    "        GBT_alpha (float): represents which descretization transformation to use based off the alpha value\n",
    "        seq_L (int): length of the sequence to be used for training\n",
    "        v (str): choice of vectorized or non-vectorized function instantiation\n",
    "            - 'v': vectorized\n",
    "            - 'nv': non-vectorized\n",
    "        lambda_n (float): value associated with the tilt of legt\n",
    "            - 1: tilt on legt\n",
    "            - \\sqrt(2n+1)(-1)^{N}: tilt associated with the legendre memory unit (LMU)\n",
    "        fourier_type (str): choice of fourier measures\n",
    "            - fru: fourier recurrent unit measure (FRU) - 'fru'\n",
    "            - fout: truncated Fourier (FouT) - 'fout'\n",
    "            - fourd: decaying fourier transform - 'fourd'\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "    \"\"\"\n",
    "\n",
    "    max_length: int\n",
    "    step_size: float = 1.0  # < 1.0 if you want to use LTI discretization\n",
    "    N: int = 100\n",
    "    lambda_n: float = 1.0\n",
    "    alpha: float = 0.0\n",
    "    beta: float = 1.0\n",
    "    GBT_alpha: float = 0.5\n",
    "    measure: str = \"legs\"\n",
    "    dtype: Any = jnp.float32\n",
    "    verbose: bool = False\n",
    "\n",
    "    def setup(self):\n",
    "        matrices = TransMatrix(\n",
    "            N=self.N,\n",
    "            measure=self.measure,\n",
    "            lambda_n=self.lambda_n,\n",
    "            alpha=self.alpha,\n",
    "            beta=self.beta,\n",
    "            dtype=self.dtype,\n",
    "        )\n",
    "\n",
    "        self.A = matrices.A\n",
    "        self.B = matrices.B\n",
    "\n",
    "        self.C = jnp.ones((self.N,))\n",
    "        self.D = jnp.zeros((1,))\n",
    "\n",
    "        if self.step_size == 1.0:\n",
    "            self.GBT_A_list, self.GBT_B_list = self.make_GBT_list(\n",
    "                matrices.A, matrices.B, dtype=self.dtype\n",
    "            )\n",
    "\n",
    "        self.eval_matrix = self.create_eval_matrix(matrices.A, matrices.B)\n",
    "\n",
    "    def __call__(self, f, init_state=None, kernel=False):\n",
    "        if not kernel:\n",
    "            if init_state is None:\n",
    "                init_state = jnp.zeros((f.shape[0], self.N, 1))\n",
    "\n",
    "            # Ab, Bb, Cb, Db = self.collect_SSM_vars(\n",
    "            #     self.A, self.B, self.C, self.D, f, t_step=t_step, alpha=self.GBT_alpha\n",
    "            # )\n",
    "            if self.step_size == 1.0:\n",
    "                c_k, y_k = self.lsi_recurrence(\n",
    "                    A=self.GBT_A_list,\n",
    "                    B=self.GBT_B_list,\n",
    "                    C=self.C,\n",
    "                    D=self.D,\n",
    "                    c_0=init_state,\n",
    "                    f=f,\n",
    "                    alpha=self.GBT_alpha,\n",
    "                    dtype=self.dtype,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                jax.debug.print(f\"c_0 shape:\\n{init_state.shape}\")\n",
    "                jax.debug.print(f\"c_0:\\n{init_state}\")\n",
    "                \n",
    "                jax.debug.print(f\"f shape:\\n{f.shape}\")\n",
    "                jax.debug.print(f\"f:\\n{f}\")\n",
    "                c_k, y_k = self.lti_recurrence(\n",
    "                    A=self.A,\n",
    "                    B=self.B,\n",
    "                    C=self.C,\n",
    "                    D=self.D,\n",
    "                    c_0=init_state,\n",
    "                    f=f,\n",
    "                    alpha=self.GBT_alpha,\n",
    "                    dtype=self.dtype,\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            Ab, Bb, Cb, Db = self.discretize(\n",
    "                self.A,\n",
    "                self.B,\n",
    "                self.C,\n",
    "                self.D,\n",
    "                step=self.step_size,\n",
    "                alpha=self.GBT_alpha,\n",
    "            )\n",
    "            c_k, y_k = self.causal_convolution(\n",
    "                f, self.K_conv(Ab, Bb, Cb, Db, L=self.max_length)\n",
    "            )\n",
    "\n",
    "        return c_k, y_k\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        \"\"\"\n",
    "        Uses coeffecients to reconstruct the signal\n",
    "\n",
    "        Args:\n",
    "            c (jnp.ndarray): coefficients of the HiPPO projection\n",
    "\n",
    "        Returns:\n",
    "            reconstructed signal\n",
    "        \"\"\"\n",
    "        return (self.eval_matrix @ jnp.expand_dims(c, -1)).squeeze(-1)\n",
    "\n",
    "    def make_GBT_list(self, A, B, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        Creates the discretized GBT matrices for the given step size\n",
    "        \"\"\"\n",
    "        GBT_a_list = []\n",
    "        GBT_b_list = []\n",
    "        for i in range(1, self.max_length + 1):\n",
    "            # TODO: make this scale invariant optional\n",
    "            GBT_A, GBT_B = self.discretize(\n",
    "                A, B, step=i, alpha=self.GBT_alpha, dtype=dtype\n",
    "            )\n",
    "            GBT_a_list.append(GBT_A)\n",
    "            GBT_b_list.append(GBT_B)\n",
    "\n",
    "        return GBT_a_list, GBT_b_list\n",
    "\n",
    "    def create_eval_matrix(self, A, B):\n",
    "        \"\"\"\n",
    "        Creates the evaluation matrix used for reconstructing the signal\n",
    "        \"\"\"\n",
    "        eval_matrix = None\n",
    "        if self.measure == \"legt\":\n",
    "            L = self.max_length\n",
    "            vals = jnp.arange(0.0, 1.0, L)\n",
    "            # n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 1 - 2 * vals\n",
    "            eval_matrix = jax.scipy.special.lpmn_values(\n",
    "                m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "            ).T  # ss.eval_legendre(n, x).T\n",
    "\n",
    "        elif self.measure == \"legs\":\n",
    "            L = self.max_length\n",
    "            vals = jnp.linspace(0.0, 1.0, L)\n",
    "            # n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 2 * vals - 1\n",
    "            eval_matrix = (\n",
    "                B[:, None]\n",
    "                * jax.scipy.special.lpmn_values(\n",
    "                    m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "                )\n",
    "            ).T  # ss.eval_legendre(n, x)).T\n",
    "\n",
    "        elif self.measure == \"lagt\":\n",
    "            raise NotImplementedError(\"Translated Laguerre measure not implemented yet\")\n",
    "\n",
    "        elif self.measure == \"fourier\":\n",
    "            raise NotImplementedError(\"Fourier measures are not implemented yet\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"invalid measure\")\n",
    "\n",
    "        return eval_matrix\n",
    "\n",
    "    def discretize(self, A, B, step, alpha=0.5, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        function used for discretizing the HiPPO matrix\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            step (float): step size used for discretization\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "                - forward Euler corresponds to α = 0,\n",
    "                - backward Euler corresponds to α = 1,\n",
    "                - bilinear corresponds to α = 0.5,\n",
    "                - Zero-order Hold corresponds to α > 1\n",
    "        \"\"\"\n",
    "        I = jnp.eye(A.shape[0])\n",
    "        step_size = 1 / step\n",
    "        part1 = I - (step_size * alpha * A)\n",
    "        part2 = I + (step_size * (1 - alpha) * A)\n",
    "\n",
    "        GBT_A = jnp.linalg.lstsq(part1, part2, rcond=None)[0]\n",
    "\n",
    "        base_GBT_B = jnp.linalg.lstsq(part1, B, rcond=None)[0]\n",
    "        GBT_B = step_size * base_GBT_B\n",
    "\n",
    "        if alpha > 1:  # Zero-order Hold\n",
    "            GBT_A = jax.scipy.linalg.expm(step_size * A)\n",
    "            GBT_B = (jnp.linalg.inv(A) @ (jax.scipy.linalg.expm(step_size * A) - I)) @ B\n",
    "\n",
    "        return GBT_A.astype(dtype), GBT_B.astype(dtype)\n",
    "\n",
    "    def lsi_recurrence(self, A, B, C, D, c_0, f, alpha=0.5, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            f (jnp.ndarray): the input sequence\n",
    "            c_0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "\n",
    "        c_k_list = []\n",
    "        y_k_list = []\n",
    "\n",
    "        c_k = c_0.copy()\n",
    "        for i in range(f.shape[1]):\n",
    "            c_k, y_k = jax.vmap(self.lsi_step, in_axes=(None, None, None, None, 0, 0))(\n",
    "                A[i], B[i], C, D, c_k, f[:, i, :]\n",
    "            )\n",
    "            c_k_list.append((c_k.copy()).astype(dtype))\n",
    "            y_k_list.append((y_k.copy()).astype(dtype))\n",
    "\n",
    "        if self.verbose:\n",
    "            return c_k_list, y_k_list\n",
    "        else:\n",
    "            return c_k_list[-1], y_k_list[-1]\n",
    "\n",
    "    def lti_recurrence(self, A, B, C, D, c_0, f, alpha=0.5, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            f (jnp.ndarray): the input sequence\n",
    "            c_0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "        Ad, Bd = self.discretize(\n",
    "            A=A, B=B, step=1 / self.step_size, alpha=alpha, dtype=dtype\n",
    "        )\n",
    "\n",
    "        def lti_step(c_k_i, f_k):\n",
    "            \"\"\"\n",
    "            Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "            Args:\n",
    "                c_k_i: previous hidden state\n",
    "                f_k: output from function f at, descritized, time step, k.\n",
    "\n",
    "            Returns:\n",
    "                c_k: current hidden state\n",
    "                y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "            \"\"\"\n",
    "\n",
    "            part1 = Ad @ c_k_i\n",
    "            part2 = Bd * f_k\n",
    "            c_k = part1 + part2\n",
    "\n",
    "            part3 = C @ c_k\n",
    "            part4 = D.T @ f_k\n",
    "            y_k = part3 + part4\n",
    "            \n",
    "            # jax.debug.print(f\"Ad shape:\\n{Ad.shape}\")\n",
    "            # jax.debug.print(f\"Ad:\\n{Ad}\")\n",
    "            \n",
    "            jax.debug.print(f\"c_k_i shape:\\n{c_k_i.shape}\")\n",
    "            jax.debug.print(f\"c_k_i:\\n{c_k_i}\")\n",
    "            \n",
    "            # jax.debug.print(f\"part1 shape:\\n{part1.shape}\")\n",
    "            # jax.debug.print(f\"part1:\\n{part1}\\n\")\n",
    "            \n",
    "            # jax.debug.print(f\"Bd shape:\\n{Bd.shape}\")\n",
    "            # jax.debug.print(f\"Bd:\\n{Bd}\")\n",
    "            \n",
    "            jax.debug.print(f\"f_k shape:\\n{f_k.shape}\")\n",
    "            jax.debug.print(f\"f_k:\\n{f_k}\")\n",
    "            \n",
    "            # jax.debug.print(f\"part2 shape:\\n{part2.shape}\")\n",
    "            # jax.debug.print(f\"part2:\\n{part2}\\n\")\n",
    "            \n",
    "            jax.debug.print(f\"c_k in step shape:\\n{c_k.shape}\")\n",
    "            jax.debug.print(f\"c_k in step:\\n{c_k}\")\n",
    "\n",
    "            return c_k, y_k\n",
    "\n",
    "        partial_scan_step = partial(jax.lax.scan, lti_step)\n",
    "        result, stacked_result = jax.vmap(partial_scan_step, in_axes=(0, 0))(c_0, f)\n",
    "\n",
    "        if self.verbose:\n",
    "            jax.debug.print(f\"stacked_result[0] shape:\\n{stacked_result[0].shape}\")\n",
    "            jax.debug.print(f\"stacked_result:\\n{stacked_result}\")\n",
    "            return stacked_result\n",
    "        else:\n",
    "            jax.debug.print(f\"result shape:\\n{result.shape}\")\n",
    "            jax.debug.print(f\"result:\\n{result}\")\n",
    "            return result\n",
    "\n",
    "    def lsi_step(self, Ad, Bd, Cd, Dd, c_k_i, f_k):\n",
    "        \"\"\"\n",
    "        Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "        Args:\n",
    "            c_k_i: previous hidden state\n",
    "            f_k: output from function f at, descritized, time step, k.\n",
    "\n",
    "        Returns:\n",
    "            c_k: current hidden state\n",
    "            y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "        \"\"\"\n",
    "\n",
    "        part1 = Ad @ c_k_i\n",
    "        part2 = Bd * f_k\n",
    "        c_k = part1 + part2\n",
    "\n",
    "        part3 = Cd @ c_k\n",
    "        part4 = Dd.T @ f_k\n",
    "        y_k = part3 + part4\n",
    "\n",
    "        return c_k, y_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vmap_compare(gu_c, c_k):\n",
    "    # jax.debug.print(f\"c_k shape: {c_k.shape}\")\n",
    "    # jax.debug.print(f\"gu_c shape: {gu_c.shape}\")\n",
    "    for i in range(c_k.shape[0]):\n",
    "        # jax.debug.print(f\"c_k[i,:,:] shape: {c_k[i,:,:].shape}\")\n",
    "        # jax.debug.print(f\"gu_c[i,:,:] shape: {gu_c[i,:,:].shape}\")\n",
    "        # jax.debug.print(f\"c_k[{i},:,:]:\\n{c_k[i,:,:]}\\n\")\n",
    "        # jax.debug.print(f\"gu_c[{i},:,:]:\\n{gu_c[i,:,:]}\\n\")\n",
    "        \n",
    "        jax.debug.print(f\"HiPPO LegS Test: {jnp.allclose(c_k[i,:,:], gu_c[i,:,:], rtol=1e-03, atol=1e-03)}\")\n",
    "        #print(f\"c_k:\\n{c_k}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hippo_legs_operator(hippo_legs, nb_hippo_legs, gu_hippo_legs, nb_gu_hippo_legs, random_input, legs_key, nb_legs_key):\n",
    "    i = 0\n",
    "    print(f\"inputted data shape: {random_input.shape}\")\n",
    "    x_tensor = torch.tensor(random_input, dtype=torch.float32)\n",
    "    #nb_x_tensor = torch.squeeze(torch.tensor(random_input, dtype=torch.float32), dim=0)\n",
    "    x_jnp = jnp.asarray(x_tensor, dtype=jnp.float64)  # convert torch array to jax array\n",
    "    #nb_x_jnp = jnp.squeeze(x_jnp, axis=0)\n",
    "    # print(f\"Data jnp shape, no batch: {nb_x_jnp.shape}\")\n",
    "    # print(f\"Data tensor shape, no batch: {nb_x_tensor.shape}\")\n",
    "    print(f\"Data jnp shape: {x_jnp.shape}\")\n",
    "    print(f\"Data tensor shape: {x_tensor.shape}\")\n",
    "    \n",
    "    #NOT BATCHED\n",
    "    # nb_params = nb_hippo_legs.init(nb_legs_key, f=nb_x_jnp, t_step=(nb_x_jnp.shape[0]))\n",
    "    # nb_c_k_list, nb_y_k_list, nb_GBT_A_list, nb_GBT_B_list = nb_hippo_legs.apply(\n",
    "    #     nb_params, f=nb_x_jnp, t_step=(nb_x_jnp.shape[1])\n",
    "    # )\n",
    "    # nb_c_k = jnp.stack(nb_c_k_list, axis=0)\n",
    "    # print(f\"nb_c_k shape: {nb_c_k.shape}\")\n",
    "    \n",
    "    # BATCHED\n",
    "    params = hippo_legs.init(legs_key, f=x_jnp)\n",
    "    c_k_list, y_k_list = hippo_legs.apply(params, f=x_jnp)\n",
    "    c_k = jnp.stack(c_k_list, axis=0)\n",
    "    c_k = jnp.moveaxis(c_k, 0, 1)\n",
    "    print(f\"c_k shape: {c_k.shape}\")\n",
    "    \n",
    "    # Gu's HiPPO LegS\n",
    "    GU_c_k = gu_hippo_legs(x_tensor)\n",
    "    gu_c = jnp.asarray(GU_c_k, dtype=jnp.float64)  # convert torch array to jax array\n",
    "    gu_c = jnp.moveaxis(gu_c, -1, -2)\n",
    "    print(f\"gu_c shape: {gu_c.shape}\")\n",
    "    \n",
    "    # NOT BATCHED Gu's HiPPO LegS\n",
    "    # nb_GU_c_k = nb_gu_hippo_legs(nb_x_tensor, fast=True)\n",
    "    # nb_gu_c = jnp.asarray(nb_GU_c_k, dtype=jnp.float64)  # convert torch array to jax array\n",
    "    # nb_gu_c = jnp.moveaxis(nb_gu_c, -1, -2)\n",
    "    # print(f\"nb_gu_c shape: {nb_gu_c.shape}\")\n",
    "    \n",
    "    \n",
    "    # print(f\"c_k shape before vmap: {c_k.shape}\")\n",
    "    # print(f\"gu_c shape before vmap: {gu_c.shape}\")\n",
    "    \n",
    "    # print(f\"c_k before vmap:\\n{c_k}\")\n",
    "    # print(f\"nb_c_k before vmap:\\n{nb_c_k}\")\n",
    "    # print(f\"gu_c before vmap:\\n{gu_c}\")\n",
    "    for i in range(c_k.shape[0]):\n",
    "        for j in range(c_k.shape[1]):\n",
    "            # jax.debug.print(f\"c_k @ b{i} t{j} - before vmap:\\n{c_k[i,j,:,:]}\\n\")\n",
    "                # jax.debug.print(f\"nb_c_k @ t{j} - before vmap:\\n{nb_c_k[j,:,:]}\\n\")\n",
    "            # jax.debug.print(f\"gu_c @ b{i} t{j} - before vmap:\\n{gu_c[i,j,:,:]}\\n\")\n",
    "                # jax.debug.print(f\"nb_gu_c @ t{j} - before vmap:\\n{nb_gu_c[j,:,:]}\\n\")\n",
    "            jax.debug.print(f\"batch {i} on trajectory {j} compare : {jnp.allclose(c_k[i,j,:,:], gu_c[i,j,:,:], rtol=1e-03, atol=1e-03)}\")\n",
    "                # jax.debug.print(f\"no batch on trajectory {j} compare : {jnp.allclose(nb_c_k[j,:,:], gu_c[i,j,:,:], rtol=1e-03, atol=1e-03)}\")\n",
    "                # jax.debug.print(f\"no batch on trajectory {j} compare : {jnp.allclose(nb_c_k[j,:,:], nb_gu_c[j,:,:], rtol=1e-03, atol=1e-03)}\")\n",
    "                # jax.debug.print(f\"no batch on trajectory {j} compare : {jnp.allclose(c_k[i,j,:,:], nb_gu_c[j,:,:], rtol=1e-03, atol=1e-03)}\")\n",
    "                # jax.debug.print(f\"no batch on trajectory {j} compare : {jnp.allclose(nb_gu_c[j,:,:], gu_c[i,j,:,:], rtol=1e-03, atol=1e-03)}\\n\")\n",
    "        \n",
    "    \n",
    "    #jax.vmap(vmap_compare, in_axes=(1, 1))(gu_c, c_k)\n",
    "    # print(f\"GU_c_k shape: {GU_c_k.shape}\")\n",
    "    # for i, c_k in enumerate(c_k_list):\n",
    "    #     g_c_k = GU_c_k[i,:,:,:]\n",
    "    #     # g_c_k = GU_c_k[i,:,:]\n",
    "    #     gu = torch.unsqueeze(g_c_k, -1)\n",
    "    #     gu_c = jnp.asarray(gu, dtype=jnp.float64)  # convert torch array to jax array\n",
    "    #     print(f\"HiPPO LegS Test: {jnp.allclose(c_k, gu_c, rtol=1e-04, atol=1e-06)}\")\n",
    "    #     #print(f\"c_k:\\n{c_k}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_16_input(key_generator, batch_size=16, data_size=784, input_size=28):\n",
    "    # x = jax.random.randint(key_generator, (batch_size, data_size), 0, 255)\n",
    "    x = jax.random.uniform(key_generator, (batch_size, data_size))\n",
    "    return jax.vmap(moving_window, in_axes=(0, None))(x, input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_GBT(hippo, gu_hippo, A, B, random_input):\n",
    "    L = random_input.shape[1]\n",
    "    for i in range(1, L+1):\n",
    "        GBT_A, GBT_B = hippo.discretize(A, B, step=i, alpha=0.5, dtype=jnp.float32)\n",
    "        gu_GBT_A, gu_GBT_B = (\n",
    "            jnp.asarray(gu_hippo.A_stacked[i-1], dtype=jnp.float32),\n",
    "            jnp.expand_dims(jnp.asarray(gu_hippo.B_stacked[i-1], dtype=jnp.float32), axis=1),\n",
    "        )\n",
    "        # print(f\"gu_GBT_A shape:{gu_GBT_A.shape}\\n\")\n",
    "        # print(f\"GBT_A shape: {GBT_A.shape}\\n\")\n",
    "        # print(f\"gu_GBT_B shape: {gu_GBT_B.shape}\\n\")\n",
    "        # print(f\"GBT_B shape: {GBT_B.shape}\")\n",
    "        \n",
    "        # print(f\"gu_GBT_A:\\n{gu_GBT_A}\\n\")\n",
    "        # print(f\"GBT_A:\\n{GBT_A}\\n\")\n",
    "        # print(f\"gu_GBT_B:\\n{gu_GBT_B}\\n\")\n",
    "        # print(f\"GBT_B:\\n{GBT_B}\")\n",
    "        \n",
    "        # print(f\"GBT_A: {jnp.allclose(GBT_A, gu_GBT_A, rtol=1e-04, atol=1e-04)}\")\n",
    "        # print(f\"GBT_B: {jnp.allclose(GBT_B, gu_GBT_B, rtol=1e-04, atol=1e-04)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # N = 256\n",
    "    # L = 128\n",
    "    \n",
    "    batch_size = 2\n",
    "    data_size = 16\n",
    "    input_size = 1\n",
    "    \n",
    "    N = 32\n",
    "    L = data_size\n",
    "    \n",
    "    x_jnp = random_16_input(\n",
    "        key_generator=key3, \n",
    "        batch_size=batch_size, \n",
    "        data_size=data_size, \n",
    "        input_size=input_size\n",
    "    )\n",
    "    x_np = np.asarray(x_jnp)\n",
    "    \n",
    "    # N = 16\n",
    "    # L = 8\n",
    "    \n",
    "    # x_np = np.array(\n",
    "    #     [\n",
    "    #         [0.3527],\n",
    "    #         [0.6617],\n",
    "    #         [0.2434],\n",
    "    #         [0.6674],\n",
    "    #         [1.2293],\n",
    "    #         [0.0964],\n",
    "    #         [-2.2756],\n",
    "    #         [0.5618],\n",
    "    #     ],\n",
    "    #     dtype=np.float32,\n",
    "    # )\n",
    "\n",
    "    # x = torch.randn(L, 1)\n",
    "    x = torch.tensor(x_np, dtype=torch.float32)\n",
    "\n",
    "    print(f\"X shape: {x.shape}\")\n",
    "    # print(f\"X:\\n{x}\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegT model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"\\nTesting HiPPO LegT model\")\n",
    "    hippo_legt = HiPPO_LTI(N, dt=1.0 / L)\n",
    "\n",
    "    c_k = hippo_legt(x)\n",
    "\n",
    "    # print(f\"Gu's Coeffiecients for LegT:\\n{c_k}\")\n",
    "    # print(f\"Gu's Coeffiecient shapes for LegT:\\n{c_k.shape}\")\n",
    "\n",
    "    # z = hippo_legt.reconstruct(c_k)\n",
    "    # print(f\"Gu's Reconstruction for LegT:\\n{z}\")\n",
    "    # print(f\"Gu's Reconstruction shape for LegT:\\n{z.shape}\")\n",
    "\n",
    "    # mse = loss(z[-1, 0, :L], x.squeeze(-1))\n",
    "    # print(f\"h-MSE shape:\\n{mse}\")\n",
    "    # print(f\"end of test for HiPPO LegT model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegS model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"\\nTesting HiPPO LegS model\")\n",
    "    gu_hippo_legs = HiPPO_LSI(N, max_length=L)  # The Gu's\n",
    "    \n",
    "    print(f\"gu_hippo_legs A_stacked: {gu_hippo_legs.A_stacked}\")\n",
    "    c_k = gu_hippo_legs(x, fast=True)\n",
    "\n",
    "    print(f\"Gu's Coeffiecients  for LegS:\\n{c_k}\")\n",
    "    print(f\"Gu's Coeffiecient shapes for LegS:\\n{c_k.shape}\")\n",
    "\n",
    "    # z = hippo_legs.reconstruct(c_k)\n",
    "\n",
    "    # print(f\"Gu's Reconstruction for LegS:\\n{z}\")\n",
    "    # print(f\"Gu's Reconstruction shape for LegS:\\n{z.shape}\")\n",
    "\n",
    "    # print(y-z)\n",
    "    print(f\"end of test for HiPPO LegS model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test Generic HiPPO model --------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    the_measure = \"legs\"\n",
    "    print(f\"\\nTesting BRYANS HiPPO-{the_measure} model\")\n",
    "    legs_matrices = TransMatrix(N=N, measure=the_measure)\n",
    "    A = legs_matrices.A\n",
    "    B = legs_matrices.B\n",
    "    nb_hippo_LegS_B = nb_HiPPO(\n",
    "        N=N,\n",
    "        max_length=L,\n",
    "        step=1.0 / L,\n",
    "        GBT_alpha=0.5,\n",
    "        seq_L=L,\n",
    "        A=A,\n",
    "        B=B,\n",
    "        measure=the_measure,\n",
    "    )  # Bryan's\n",
    "    \n",
    "    # hippo_LegS_B = HiPPO(\n",
    "    #     N=N,\n",
    "    #     max_length=L,\n",
    "    #     step=1.0 / L,\n",
    "    #     GBT_alpha=0.5,\n",
    "    #     seq_L=L,\n",
    "    #     A=A,\n",
    "    #     B=B,\n",
    "    #     measure=the_measure,\n",
    "    # )  # Bryan's\n",
    "    \n",
    "    hippo_LegS_B = HiPPO(\n",
    "        max_length=L,\n",
    "        step_size=1.0/L,\n",
    "        N=N,\n",
    "        lambda_n=1.0,\n",
    "        alpha=0.0,\n",
    "        beta=1.0,\n",
    "        GBT_alpha=0.5,\n",
    "        measure=\"legs\",\n",
    "        dtype = jnp.float32,\n",
    "        verbose = True,\n",
    "    )  # Bryan's\n",
    "    \n",
    "    test_GBT(\n",
    "        hippo=hippo_LegS_B, \n",
    "        gu_hippo=gu_hippo_legs, \n",
    "        A=A, \n",
    "        B=B, \n",
    "        random_input=x_np\n",
    "    )    \n",
    "    # GBT_a_list = []\n",
    "    # GBT_b_list = []\n",
    "    # for i in range(L):\n",
    "    #     # TODO: make this scale invariant optional\n",
    "    #     GBT_A, GBT_B = hippo_LegS_B.discretion(A, B, step=L, alpha=0.5, dtype=jnp.float32)\n",
    "    #     GBT_a_list.append(GBT_A)\n",
    "    #     GBT_b_list.append(GBT_B)\n",
    "    # print(f\"GBT_a_list:\\n{GBT_a_list}\\n\")\n",
    "    # print(f\"GBT_b_list:\\n{GBT_b_list}\")\n",
    "    \n",
    "    print(f\"Bryan's Coeffiecients for HiPPO-{the_measure}\")\n",
    "    nb_gu_hippo_legs = HiPPO_LSI(N, max_length=L)  # The Gu's\n",
    "    test_hippo_legs_operator(hippo_legs=hippo_LegS_B, \n",
    "                             nb_hippo_legs=nb_hippo_LegS_B,\n",
    "                             gu_hippo_legs=gu_hippo_legs, \n",
    "                             nb_gu_hippo_legs=nb_gu_hippo_legs,\n",
    "                             random_input=x_np, \n",
    "                             legs_key=key2,\n",
    "                             nb_legs_key=key4)\n",
    "    \n",
    "    # y_legs = hippo_LegS_B.apply(\n",
    "    #     {\"params\": params}, c_k, method=hippo_LegS_B.reconstruct\n",
    "    # )\n",
    "\n",
    "    # print(f\"Bryan's Reconstruction for HiPPO-{the_measure}:\\n{y_legs}\")\n",
    "    # print(f\"Bryan's Reconstruction shape for HiPPO-{the_measure}:\\n{y_legs.shape}\")\n",
    "\n",
    "    print(f\"end of test for HiPPO-{the_measure} model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([2, 16, 1])\n",
      "\n",
      "Testing HiPPO LegT model\n",
      "\n",
      "Testing HiPPO LegS model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1118008/1568596272.py:64: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  self.eval_matrix = torch.from_numpy(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gu_hippo_legs A_stacked: tensor([[[ 3.3333e-01, -2.2230e-15,  3.7705e-15,  ...,  1.1191e-15,  1.7628e-15,  2.2591e-16],\n",
      "         [-5.7735e-01,  2.9802e-08,  1.0686e-15,  ..., -2.4980e-16, -7.3552e-16, -6.9389e-17],\n",
      "         [-1.4907e-01, -7.7460e-01, -2.0000e-01,  ..., -3.6316e-17,  4.0979e-16,  4.7384e-17],\n",
      "         ...,\n",
      "         [ 5.2325e-08, -1.9174e-07, -2.5004e-06,  ..., -8.7500e-01, -1.7738e-16,  4.7271e-17],\n",
      "         [-6.5608e-08,  2.0566e-07,  2.0668e-06,  ..., -2.2724e-01, -8.7879e-01, -3.1420e-16],\n",
      "         [ 6.0608e-08, -1.2849e-07, -1.8105e-06,  ...,  1.9018e-01, -2.2101e-01, -8.8235e-01]],\n",
      "\n",
      "        [[ 6.0000e-01, -3.5127e-15,  3.7322e-15,  ...,  1.9720e-15,  1.3421e-15,  8.8373e-17],\n",
      "         [-4.6188e-01,  3.3333e-01,  2.6368e-16,  ..., -5.5511e-16,  1.8041e-16,  2.5153e-16],\n",
      "         [-2.5555e-01, -7.3771e-01,  1.4286e-01,  ...,  1.8802e-15,  1.4118e-15,  3.1912e-16],\n",
      "         ...,\n",
      "         [ 2.0668e-08, -1.9147e-07,  7.6689e-08,  ..., -7.6471e-01, -5.0134e-16,  1.2924e-16],\n",
      "         [-3.2209e-08,  1.9052e-07, -8.1039e-08,  ..., -4.0331e-01, -7.7143e-01, -3.2943e-16],\n",
      "         [ 3.2505e-08, -1.0543e-07, -2.0006e-08,  ...,  2.9601e-01, -3.9360e-01, -7.7778e-01]],\n",
      "\n",
      "        [[ 7.1429e-01, -1.8542e-15,  7.2268e-16,  ...,  1.1697e-15,  7.5636e-16,  1.0982e-16],\n",
      "         [-3.7115e-01,  5.0000e-01, -1.0270e-15,  ...,  4.5103e-16,  3.4001e-16,  3.2092e-16],\n",
      "         [-2.6620e-01, -6.4550e-01,  3.3333e-01,  ..., -1.6867e-15, -5.0003e-16, -2.2208e-16],\n",
      "         ...,\n",
      "         [ 4.4030e-08, -1.3145e-07,  1.2056e-07,  ..., -6.6667e-01,  1.5266e-16,  1.3856e-16],\n",
      "         [-4.4793e-08,  9.7626e-08, -1.0966e-07,  ..., -5.4047e-01, -6.7568e-01, -1.8941e-16],\n",
      "         [ 2.3566e-08, -1.4568e-08, -3.3478e-08,  ...,  3.4690e-01, -5.2909e-01, -6.8421e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 9.3103e-01,  1.2296e-15, -6.4598e-16,  ..., -3.0784e-16,  1.7807e-16,  2.9290e-18],\n",
      "         [-1.1149e-01,  8.6667e-01,  6.8001e-16,  ...,  5.5511e-17, -5.5511e-17, -1.0842e-18],\n",
      "         [-1.2536e-01, -2.3321e-01,  8.0645e-01,  ...,  5.8536e-17, -3.0021e-16, -2.6025e-17],\n",
      "         ...,\n",
      "         [ 7.1434e-09, -6.1724e-08, -1.2370e-09,  ..., -3.4483e-02,  3.0206e-16,  2.3107e-17],\n",
      "         [-1.3861e-09,  4.3375e-08, -2.5738e-08,  ..., -9.8175e-01, -5.0847e-02, -1.8702e-18],\n",
      "         [ 2.2513e-09,  1.3873e-08, -1.0669e-08,  ...,  3.3257e-02, -9.8066e-01, -6.6667e-02]],\n",
      "\n",
      "        [[ 9.3548e-01, -2.1261e-16, -4.7086e-16,  ..., -4.7739e-16,  6.3820e-18, -4.7741e-18],\n",
      "         [-1.0476e-01,  8.7500e-01, -3.3307e-16,  ..., -6.7307e-16, -2.7409e-16, -1.3444e-17],\n",
      "         [-1.1885e-01, -2.2006e-01,  8.1818e-01,  ..., -7.6363e-17, -1.6592e-16, -5.2952e-18],\n",
      "         ...,\n",
      "         [ 1.2843e-08, -5.0748e-08, -4.8908e-09,  ...,  1.3704e-16,  1.9711e-16,  9.0599e-18],\n",
      "         [-2.1802e-10,  3.3529e-08,  1.1977e-08,  ..., -9.8347e-01, -1.6393e-02,  6.9321e-18],\n",
      "         [-1.5571e-08,  1.5271e-09, -9.3653e-08,  ..., -5.0075e-08, -9.8348e-01, -3.2258e-02]],\n",
      "\n",
      "        [[ 9.3939e-01,  1.2600e-15, -3.0211e-16,  ...,  7.7618e-17,  3.8933e-17,  0.0000e+00],\n",
      "         [-9.8798e-02,  8.8235e-01,  7.9103e-16,  ..., -1.5266e-16, -4.8919e-16,  0.0000e+00],\n",
      "         [-1.1297e-01, -2.0829e-01,  8.2857e-01,  ...,  1.0285e-16, -1.8650e-16,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 3.5383e-09, -5.3209e-08,  7.0965e-09,  ...,  3.2258e-02,  1.6870e-16,  0.0000e+00],\n",
      "         [-2.7888e-09,  3.5455e-08, -1.0577e-08,  ..., -9.8297e-01,  1.5873e-02,  0.0000e+00],\n",
      "         [ 3.3018e-09,  1.1219e-08, -4.9395e-08,  ..., -3.1217e-02, -9.8400e-01,  0.0000e+00]]])\n",
      "Gu - u * self.B_stacked[:L]: tensor([[[[ 3.2127e-01,  2.7823e-01,  7.1838e-02,  ..., -2.5215e-08,  3.1617e-08, -2.9207e-08],\n",
      "          [ 1.7809e-01,  2.0564e-01,  1.1378e-01,  ..., -9.2019e-09,  1.4341e-08, -1.4472e-08]],\n",
      "\n",
      "         [[ 1.3184e-01,  1.1418e-01,  2.9481e-02,  ..., -1.0348e-08,  1.2975e-08, -1.1986e-08],\n",
      "          [ 3.1642e-01,  3.6537e-01,  2.0215e-01,  ..., -1.6349e-08,  2.5479e-08, -2.5713e-08]],\n",
      "\n",
      "         [[ 6.7515e-02,  5.8469e-02,  1.5097e-02,  ..., -5.2990e-09,  6.6443e-09, -6.1379e-09],\n",
      "          [ 2.4757e-01,  2.8587e-01,  1.5817e-01,  ..., -1.2792e-08,  1.9935e-08, -2.0118e-08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2723e-02,  1.9679e-02,  5.0811e-03,  ..., -1.7835e-09,  2.2362e-09, -2.0658e-09],\n",
      "          [ 1.8529e-01,  2.1396e-01,  1.1838e-01,  ..., -9.5739e-09,  1.4920e-08, -1.5057e-08]],\n",
      "\n",
      "         [[ 4.5488e-01,  3.9394e-01,  1.0171e-01,  ..., -3.5702e-08,  4.4766e-08, -4.1354e-08],\n",
      "          [ 3.0540e-01,  3.5265e-01,  1.9511e-01,  ..., -1.5780e-08,  2.4592e-08, -2.4817e-08]],\n",
      "\n",
      "         [[ 2.0649e-01,  1.7883e-01,  4.6173e-02,  ..., -1.6207e-08,  2.0321e-08, -1.8773e-08],\n",
      "          [ 1.8833e-01,  2.1747e-01,  1.2032e-01,  ..., -9.7310e-09,  1.5165e-08, -1.5304e-08]]]])\n",
      "Gu's Coeffiecients  for LegS:\n",
      "tensor([[[[ 3.2127e-01,  2.7823e-01,  7.1838e-02,  ..., -2.5215e-08,  3.1617e-08, -2.9207e-08]],\n",
      "\n",
      "         [[ 1.3184e-01,  1.1418e-01,  2.9481e-02,  ..., -1.0348e-08,  1.2975e-08, -1.1986e-08]],\n",
      "\n",
      "         [[ 6.7515e-02,  5.8469e-02,  1.5097e-02,  ..., -5.2990e-09,  6.6443e-09, -6.1379e-09]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2723e-02,  1.9679e-02,  5.0811e-03,  ..., -1.7835e-09,  2.2362e-09, -2.0658e-09]],\n",
      "\n",
      "         [[ 4.5488e-01,  3.9394e-01,  1.0171e-01,  ..., -3.5702e-08,  4.4766e-08, -4.1354e-08]],\n",
      "\n",
      "         [[ 2.0649e-01,  1.7883e-01,  4.6173e-02,  ..., -1.6207e-08,  2.0321e-08, -1.8773e-08]]],\n",
      "\n",
      "\n",
      "        [[[ 3.7085e-01,  1.5000e-01, -1.6331e-01,  ..., -4.5009e-08,  4.7104e-08, -3.9437e-08]],\n",
      "\n",
      "         [[ 3.9553e-01,  3.4254e-01,  8.8443e-02,  ..., -3.1044e-08,  3.8925e-08, -3.5958e-08]],\n",
      "\n",
      "         [[ 2.8808e-01,  2.7417e-01,  9.9936e-02,  ..., -2.0317e-08,  2.6820e-08, -2.5364e-08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9893e-01,  2.1002e-01,  9.8780e-02,  ..., -1.2106e-08,  1.7238e-08, -1.6823e-08]],\n",
      "\n",
      "         [[ 5.7833e-01,  2.7386e-01, -1.9721e-01,  ..., -6.6478e-08,  7.0980e-08, -6.0164e-08]],\n",
      "\n",
      "         [[ 3.1223e-01,  1.8170e-01, -5.7776e-02,  ..., -3.2746e-08,  3.6223e-08, -3.1350e-08]]]])\n",
      "Gu's Coeffiecient shapes for LegS:\n",
      "torch.Size([2, 16, 1, 32])\n",
      "end of test for HiPPO LegS model\n",
      "\n",
      "Testing BRYANS HiPPO-legs model\n",
      "Bryan's Coeffiecients for HiPPO-legs\n",
      "inputted data shape: (2, 16, 1)\n",
      "Data jnp shape: (2, 16, 1)\n",
      "Data tensor shape: torch.Size([2, 16, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1118008/2261351061.py:6: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  x_jnp = jnp.asarray(x_tensor, dtype=jnp.float64)  # convert torch array to jax array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_0 shape:\n",
      "(2, 32, 1)\n",
      "c_0:\n",
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n",
      "f shape:\n",
      "(2, 16, 1)\n",
      "f:\n",
      "[[[0.48190224]\n",
      "  [0.19776285]\n",
      "  [0.10127199]\n",
      "  [0.00995553]\n",
      "  [0.29279542]\n",
      "  [0.99540186]\n",
      "  [0.58847713]\n",
      "  [0.0353756 ]\n",
      "  [0.7706082 ]\n",
      "  [0.08752763]\n",
      "  [0.27107787]\n",
      "  [0.41544926]\n",
      "  [0.97598505]\n",
      "  [0.0340848 ]\n",
      "  [0.6823162 ]\n",
      "  [0.30973995]]\n",
      "\n",
      " [[0.44523168]\n",
      "  [0.7910527 ]\n",
      "  [0.61892366]\n",
      "  [0.7225524 ]\n",
      "  [0.12798858]\n",
      "  [0.57040155]\n",
      "  [0.22614598]\n",
      "  [0.6732223 ]\n",
      "  [0.87575054]\n",
      "  [0.4019662 ]\n",
      "  [0.5075493 ]\n",
      "  [0.8612207 ]\n",
      "  [0.67529714]\n",
      "  [0.46322787]\n",
      "  [0.76350224]\n",
      "  [0.470829  ]]]\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "stacked_result[0] shape:\n",
      "(16, 1)\n",
      "stacked_result:\n",
      "[[[ 0.35130933]\n",
      "  [-0.03243109]\n",
      "  [ 0.21948597]\n",
      "  [-0.09571345]\n",
      "  [ 0.3737436 ]\n",
      "  [ 0.5213812 ]\n",
      "  [ 0.33552307]\n",
      "  [ 0.07129531]\n",
      "  [ 0.7304812 ]\n",
      "  [-0.19659598]\n",
      "  [ 0.6441436 ]\n",
      "  [-0.00400206]\n",
      "  [ 1.0533978 ]\n",
      "  [-0.43659788]\n",
      "  [ 1.2166165 ]\n",
      "  [-0.42877778]]\n",
      "\n",
      " [[ 0.3245763 ]\n",
      "  [ 0.41351894]\n",
      "  [ 0.362837  ]\n",
      "  [ 0.5144456 ]\n",
      "  [ 0.02164113]\n",
      "  [ 0.66242135]\n",
      "  [-0.07086375]\n",
      "  [ 0.8551806 ]\n",
      "  [ 0.26871267]\n",
      "  [ 0.5780011 ]\n",
      "  [ 0.26858482]\n",
      "  [ 0.860192  ]\n",
      "  [ 0.26910442]\n",
      "  [ 0.6760838 ]\n",
      "  [ 0.44914493]\n",
      "  [ 0.56193   ]]]\n",
      "c_0 shape:\n",
      "(2, 32, 1)\n",
      "c_0:\n",
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n",
      "f shape:\n",
      "(2, 16, 1)\n",
      "f:\n",
      "[[[0.48190224]\n",
      "  [0.19776285]\n",
      "  [0.10127199]\n",
      "  [0.00995553]\n",
      "  [0.29279542]\n",
      "  [0.99540186]\n",
      "  [0.58847713]\n",
      "  [0.0353756 ]\n",
      "  [0.7706082 ]\n",
      "  [0.08752763]\n",
      "  [0.27107787]\n",
      "  [0.41544926]\n",
      "  [0.97598505]\n",
      "  [0.0340848 ]\n",
      "  [0.6823162 ]\n",
      "  [0.30973995]]\n",
      "\n",
      " [[0.44523168]\n",
      "  [0.7910527 ]\n",
      "  [0.61892366]\n",
      "  [0.7225524 ]\n",
      "  [0.12798858]\n",
      "  [0.57040155]\n",
      "  [0.22614598]\n",
      "  [0.6732223 ]\n",
      "  [0.87575054]\n",
      "  [0.4019662 ]\n",
      "  [0.5075493 ]\n",
      "  [0.8612207 ]\n",
      "  [0.67529714]\n",
      "  [0.46322787]\n",
      "  [0.76350224]\n",
      "  [0.470829  ]]]\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k_i shape:\n",
      "(32, 1)\n",
      "c_k_i:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "f_k shape:\n",
      "(1,)\n",
      "f_k:\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "c_k in step shape:\n",
      "(32, 1)\n",
      "c_k in step:\n",
      "Traced<ShapedArray(float32[32,1])>with<DynamicJaxprTrace(level=2/0)>\n",
      "stacked_result[0] shape:\n",
      "(16, 1)\n",
      "stacked_result:\n",
      "[[[ 0.35130933]\n",
      "  [-0.03243109]\n",
      "  [ 0.21948597]\n",
      "  [-0.09571345]\n",
      "  [ 0.3737436 ]\n",
      "  [ 0.5213812 ]\n",
      "  [ 0.33552307]\n",
      "  [ 0.07129531]\n",
      "  [ 0.7304812 ]\n",
      "  [-0.19659598]\n",
      "  [ 0.6441436 ]\n",
      "  [-0.00400206]\n",
      "  [ 1.0533978 ]\n",
      "  [-0.43659788]\n",
      "  [ 1.2166165 ]\n",
      "  [-0.42877778]]\n",
      "\n",
      " [[ 0.3245763 ]\n",
      "  [ 0.41351894]\n",
      "  [ 0.362837  ]\n",
      "  [ 0.5144456 ]\n",
      "  [ 0.02164113]\n",
      "  [ 0.66242135]\n",
      "  [-0.07086375]\n",
      "  [ 0.8551806 ]\n",
      "  [ 0.26871267]\n",
      "  [ 0.5780011 ]\n",
      "  [ 0.26858482]\n",
      "  [ 0.860192  ]\n",
      "  [ 0.26910442]\n",
      "  [ 0.6760838 ]\n",
      "  [ 0.44914493]\n",
      "  [ 0.56193   ]]]\n",
      "c_k shape: (1, 16)\n",
      "Gu - u * self.B_stacked[:L]: tensor([[[[ 3.2127e-01,  2.7823e-01,  7.1838e-02,  ..., -2.5215e-08,  3.1617e-08, -2.9207e-08],\n",
      "          [ 1.7809e-01,  2.0564e-01,  1.1378e-01,  ..., -9.2019e-09,  1.4341e-08, -1.4472e-08]],\n",
      "\n",
      "         [[ 1.3184e-01,  1.1418e-01,  2.9481e-02,  ..., -1.0348e-08,  1.2975e-08, -1.1986e-08],\n",
      "          [ 3.1642e-01,  3.6537e-01,  2.0215e-01,  ..., -1.6349e-08,  2.5479e-08, -2.5713e-08]],\n",
      "\n",
      "         [[ 6.7515e-02,  5.8469e-02,  1.5097e-02,  ..., -5.2990e-09,  6.6443e-09, -6.1379e-09],\n",
      "          [ 2.4757e-01,  2.8587e-01,  1.5817e-01,  ..., -1.2792e-08,  1.9935e-08, -2.0118e-08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2723e-02,  1.9679e-02,  5.0811e-03,  ..., -1.7835e-09,  2.2362e-09, -2.0658e-09],\n",
      "          [ 1.8529e-01,  2.1396e-01,  1.1838e-01,  ..., -9.5739e-09,  1.4920e-08, -1.5057e-08]],\n",
      "\n",
      "         [[ 4.5488e-01,  3.9394e-01,  1.0171e-01,  ..., -3.5702e-08,  4.4766e-08, -4.1354e-08],\n",
      "          [ 3.0540e-01,  3.5265e-01,  1.9511e-01,  ..., -1.5780e-08,  2.4592e-08, -2.4817e-08]],\n",
      "\n",
      "         [[ 2.0649e-01,  1.7883e-01,  4.6173e-02,  ..., -1.6207e-08,  2.0321e-08, -1.8773e-08],\n",
      "          [ 1.8833e-01,  2.1747e-01,  1.2032e-01,  ..., -9.7310e-09,  1.5165e-08, -1.5304e-08]]]])\n",
      "gu_c shape: (2, 16, 32, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1118008/2261351061.py:30: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  gu_c = jnp.asarray(GU_c_k, dtype=jnp.float64)  # convert torch array to jax array\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Too many indices for array: 4 non-None/Ellipsis indices for dim 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test()\n",
      "Cell \u001b[0;32mIn[33], line 147\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBryan\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms Coeffiecients for HiPPO-\u001b[39m\u001b[39m{\u001b[39;00mthe_measure\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m nb_gu_hippo_legs \u001b[39m=\u001b[39m HiPPO_LSI(N, max_length\u001b[39m=\u001b[39mL)  \u001b[39m# The Gu's\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m test_hippo_legs_operator(hippo_legs\u001b[39m=\u001b[39;49mhippo_LegS_B, \n\u001b[1;32m    148\u001b[0m                          nb_hippo_legs\u001b[39m=\u001b[39;49mnb_hippo_LegS_B,\n\u001b[1;32m    149\u001b[0m                          gu_hippo_legs\u001b[39m=\u001b[39;49mgu_hippo_legs, \n\u001b[1;32m    150\u001b[0m                          nb_gu_hippo_legs\u001b[39m=\u001b[39;49mnb_gu_hippo_legs,\n\u001b[1;32m    151\u001b[0m                          random_input\u001b[39m=\u001b[39;49mx_np, \n\u001b[1;32m    152\u001b[0m                          legs_key\u001b[39m=\u001b[39;49mkey2,\n\u001b[1;32m    153\u001b[0m                          nb_legs_key\u001b[39m=\u001b[39;49mkey4)\n\u001b[1;32m    155\u001b[0m \u001b[39m# y_legs = hippo_LegS_B.apply(\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39m#     {\"params\": params}, c_k, method=hippo_LegS_B.reconstruct\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[39m# print(f\"Bryan's Reconstruction for HiPPO-{the_measure}:\\n{y_legs}\")\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m# print(f\"Bryan's Reconstruction shape for HiPPO-{the_measure}:\\n{y_legs.shape}\")\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend of test for HiPPO-\u001b[39m\u001b[39m{\u001b[39;00mthe_measure\u001b[39m}\u001b[39;00m\u001b[39m model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 53\u001b[0m, in \u001b[0;36mtest_hippo_legs_operator\u001b[0;34m(hippo_legs, nb_hippo_legs, gu_hippo_legs, nb_gu_hippo_legs, random_input, legs_key, nb_legs_key)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(c_k\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m     48\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(c_k\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m     49\u001b[0m         \u001b[39m# jax.debug.print(f\"c_k @ b{i} t{j} - before vmap:\\n{c_k[i,j,:,:]}\\n\")\u001b[39;00m\n\u001b[1;32m     50\u001b[0m             \u001b[39m# jax.debug.print(f\"nb_c_k @ t{j} - before vmap:\\n{nb_c_k[j,:,:]}\\n\")\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         \u001b[39m# jax.debug.print(f\"gu_c @ b{i} t{j} - before vmap:\\n{gu_c[i,j,:,:]}\\n\")\u001b[39;00m\n\u001b[1;32m     52\u001b[0m             \u001b[39m# jax.debug.print(f\"nb_gu_c @ t{j} - before vmap:\\n{nb_gu_c[j,:,:]}\\n\")\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m         jax\u001b[39m.\u001b[39mdebug\u001b[39m.\u001b[39mprint(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m on trajectory \u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m}\u001b[39;00m\u001b[39m compare : \u001b[39m\u001b[39m{\u001b[39;00mjnp\u001b[39m.\u001b[39mallclose(c_k[i,j,:,:], gu_c[i,j,:,:], rtol\u001b[39m=\u001b[39m\u001b[39m1e-03\u001b[39m, atol\u001b[39m=\u001b[39m\u001b[39m1e-03\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/s4mer-pkg-jZnBSgjq-py3.8/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:3816\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3813\u001b[0m       \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39mdynamic_index_in_dim(arr, idx, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   3815\u001b[0m treedef, static_idx, dynamic_idx \u001b[39m=\u001b[39m _split_index_for_jit(idx, arr\u001b[39m.\u001b[39mshape)\n\u001b[0;32m-> 3816\u001b[0m \u001b[39mreturn\u001b[39;00m _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[1;32m   3817\u001b[0m                unique_indices, mode, fill_value)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/s4mer-pkg-jZnBSgjq-py3.8/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:3825\u001b[0m, in \u001b[0;36m_gather\u001b[0;34m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3822\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_gather\u001b[39m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[1;32m   3823\u001b[0m             unique_indices, mode, fill_value):\n\u001b[1;32m   3824\u001b[0m   idx \u001b[39m=\u001b[39m _merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)\n\u001b[0;32m-> 3825\u001b[0m   indexer \u001b[39m=\u001b[39m _index_to_gather(shape(arr), idx)  \u001b[39m# shared with _scatter_update\u001b[39;00m\n\u001b[1;32m   3826\u001b[0m   y \u001b[39m=\u001b[39m arr\n\u001b[1;32m   3828\u001b[0m   \u001b[39mif\u001b[39;00m fill_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/s4mer-pkg-jZnBSgjq-py3.8/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:3926\u001b[0m, in \u001b[0;36m_index_to_gather\u001b[0;34m(x_shape, idx, normalize_indices)\u001b[0m\n\u001b[1;32m   3924\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_index_to_gather\u001b[39m(x_shape, idx, normalize_indices\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   3925\u001b[0m   \u001b[39m# Remove ellipses and add trailing slice(None)s.\u001b[39;00m\n\u001b[0;32m-> 3926\u001b[0m   idx \u001b[39m=\u001b[39m _canonicalize_tuple_index(\u001b[39mlen\u001b[39;49m(x_shape), idx)\n\u001b[1;32m   3928\u001b[0m   \u001b[39m# Check for advanced indexing:\u001b[39;00m\n\u001b[1;32m   3929\u001b[0m   \u001b[39m# https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing\u001b[39;00m\n\u001b[1;32m   3930\u001b[0m \n\u001b[1;32m   3931\u001b[0m   \u001b[39m# Do the advanced indexing axes appear contiguously? If not, NumPy semantics\u001b[39;00m\n\u001b[1;32m   3932\u001b[0m   \u001b[39m# move the advanced axes to the front.\u001b[39;00m\n\u001b[1;32m   3933\u001b[0m   advanced_axes_are_contiguous \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/s4mer-pkg-jZnBSgjq-py3.8/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:4259\u001b[0m, in \u001b[0;36m_canonicalize_tuple_index\u001b[0;34m(arr_ndim, idx, array_name)\u001b[0m\n\u001b[1;32m   4257\u001b[0m len_without_none \u001b[39m=\u001b[39m _sum(\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m idx \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mEllipsis\u001b[39m)\n\u001b[1;32m   4258\u001b[0m \u001b[39mif\u001b[39;00m len_without_none \u001b[39m>\u001b[39m arr_ndim:\n\u001b[0;32m-> 4259\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\n\u001b[1;32m   4260\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mToo many indices for \u001b[39m\u001b[39m{\u001b[39;00marray_name\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mlen_without_none\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4261\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnon-None/Ellipsis indices for dim \u001b[39m\u001b[39m{\u001b[39;00marr_ndim\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   4262\u001b[0m ellipses \u001b[39m=\u001b[39m (i \u001b[39mfor\u001b[39;00m i, elt \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(idx) \u001b[39mif\u001b[39;00m elt \u001b[39mis\u001b[39;00m \u001b[39mEllipsis\u001b[39m)\n\u001b[1;32m   4263\u001b[0m ellipsis_index \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(ellipses, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Too many indices for array: 4 non-None/Ellipsis indices for dim 2."
     ]
    }
   ],
   "source": [
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('s4mer-pkg-jZnBSgjq-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a81e05d1d7f7eae781698b7c1b81c0d771335201ebad1d81045cb177cef974b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
