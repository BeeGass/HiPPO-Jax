{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiPPO Matrices\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Loading In Necessary Packages](#load-packages)\n",
    "    * [Instantiate The HiPPO Matrix](#instantiate-the-hippo-matrix)\n",
    "        * [Translated Legendre (LegT)](#translated-legendre-legt)\n",
    "            * [LegT](#legt)\n",
    "            * [LMU](#lmu)\n",
    "        * [Translated Laguerre (LagT)](#translated-laguerre-lagt)\n",
    "        * [Scaled Legendre (LegS)](#scaled-legendre-legs)\n",
    "        * [Fourier Basis](#fourier-basis)\n",
    "            * [Fourier Recurrent Unit (FRU)](#fourier-recurrent-unit-fru)\n",
    "            * [Truncated Fourier (FouT)](#truncated-fourier-fout)\n",
    "            * [Fourier With Decay (FourD)](#fourier-with-decay-fourd)\n",
    "    * [Gu's HiPPO LegT Operator](#gus-hippo-legt-operator)\n",
    "    * [Gu's Scale invariant HiPPO LegS Operator](#gus-scale-invariant-hippo-legs-operator)\n",
    "    * [Implementation Of General HiPPO Operator](#implementation-of-general-hippo-operator)\n",
    "    * [Output](#output)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]\n",
      "The Device: gpu\n"
     ]
    }
   ],
   "source": [
    "## import packages\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import linen as jnn\n",
    "\n",
    "from jax.nn.initializers import lecun_normal, uniform\n",
    "from jax.numpy.linalg import eig, inv, matrix_power\n",
    "from jax.scipy.signal import convolve\n",
    "\n",
    "# import modules \n",
    "from src.models.hippo.gu_transition import GuTransMatrix\n",
    "from src.models.hippo.unroll import (\n",
    "    measure,\n",
    "    basis,\n",
    "    variable_unroll_matrix,\n",
    "    variable_unroll_matrix_sequential,\n",
    ")\n",
    "from src.data.process import moving_window, rolling_window\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "from scipy import linalg as la\n",
    "from scipy import signal\n",
    "from scipy import special as ss\n",
    "\n",
    "import math\n",
    "\n",
    "print(jax.devices())\n",
    "print(f\"The Device: {jax.lib.xla_bridge.get_backend().platform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS enabled: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from typing import Any\n",
    "from functools import partial\n",
    "\n",
    "print(f\"MPS enabled: {torch.backends.mps.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(linewidth=150)\n",
    "np.set_printoptions(linewidth=150)\n",
    "jnp.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 5\n",
    "rng, key2, key3, key4, key5 = jax.random.split(key, num=num_copies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate The HiPPO Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransMatrix:\n",
    "    def __init__(\n",
    "        self,\n",
    "        N: int,\n",
    "        measure: str = \"legs\",\n",
    "        lambda_n: float = 1.0,\n",
    "        alpha: float = 0.0,\n",
    "        beta: float = 1.0,\n",
    "        dtype: Any = jnp.float32,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiates the HiPPO matrix of a given order using a particular measure.\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            v (str): choose between this repo's implementation or hazy research's implementation.\n",
    "            measure (str):\n",
    "                choose between\n",
    "                    - HiPPO w/ Translated Legendre (LegT) - legt\n",
    "                    - HiPPO w/ Translated Laguerre (LagT) - lagt\n",
    "                    - HiPPO w/ Scaled Legendre (LegS) - legs\n",
    "                    - HiPPO w/ Fourier basis\n",
    "                        - FRU: Fourier Recurrent Unit - fru\n",
    "                        - FouT: Translated Fourier - fout\n",
    "                        - FourD: Fourier Decay - fourd\n",
    "            lambda_n (int): The amount of tilt applied to the HiPPO-LegS basis, determines between LegS and LMU.\n",
    "            alpha (float): The order of the Laguerre basis.\n",
    "            beta (float): The scale of the Laguerre basis.\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The HiPPO matrix multiplied by -1.\n",
    "            B (jnp.ndarray): The other corresponding state space matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        A = None\n",
    "        B = None\n",
    "        if measure in [\"legt\", \"lmu\"]:\n",
    "            A, B = self.build_LegT(N=N, lambda_n=lambda_n, dtype=dtype)\n",
    "\n",
    "        elif measure == \"lagt\":\n",
    "            A, B = self.build_LagT(alpha=alpha, beta=beta, N=N, dtype=dtype)\n",
    "\n",
    "        elif measure == \"legs\":\n",
    "            A, B = self.build_LegS(N=N, dtype=dtype)\n",
    "\n",
    "        elif measure in [\"fout\", \"fru\", \"foud\"]:\n",
    "            A, B = self.build_Fourier(N=N, fourier_type=measure, dtype=dtype)\n",
    "\n",
    "        elif measure == \"random\":\n",
    "            A = jnp.random.randn(N, N) / N\n",
    "            B = jnp.random.randn(N, 1)\n",
    "\n",
    "        elif measure == \"diagonal\":\n",
    "            A = -jnp.diag(jnp.exp(jnp.random.randn(N)))\n",
    "            B = jnp.random.randn(N, 1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid HiPPO type\")\n",
    "\n",
    "        self.A = (A.copy()).astype(dtype)\n",
    "        self.B = (B.copy()).astype(dtype)\n",
    "\n",
    "    # Translated Legendre (LegT) - vectorized\n",
    "    @staticmethod\n",
    "    def build_LegT(N, lambda_n=1, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        The, vectorized implementation of the, measure derived from the translated Legendre basis.\n",
    "\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            legt_type (str): Choice between the two different tilts of basis.\n",
    "                - legt: translated Legendre - 'legt'\n",
    "                - lmu: Legendre Memory Unit - 'lmu'\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        q = jnp.arange(N, dtype=dtype)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "        case = jnp.power(-1.0, (n - k))\n",
    "        A = None\n",
    "        B = None\n",
    "\n",
    "        if lambda_n == 1:\n",
    "            A_base = jnp.sqrt(2 * n + 1) * jnp.sqrt(2 * k + 1)\n",
    "            pre_D = jnp.sqrt(jnp.diag(2 * q + 1))\n",
    "            B = D = jnp.diag(pre_D)[:, None]\n",
    "            A = jnp.where(\n",
    "                k <= n, A_base, A_base * case\n",
    "            )  # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "        elif lambda_n == 2:  # (jnp.sqrt(2*n+1) * jnp.power(-1, n)):\n",
    "            A_base = 2 * n + 1\n",
    "            B = jnp.diag((2 * q + 1) * jnp.power(-1, n))[:, None]\n",
    "            A = jnp.where(\n",
    "                k <= n, A_base * case, A_base\n",
    "            )  # if n >= k, then case_2 * A_base is used, otherwise A_base\n",
    "\n",
    "        return -A.astype(dtype), B.astype(dtype)\n",
    "\n",
    "    # Translated Laguerre (LagT) - non-vectorized\n",
    "    @staticmethod\n",
    "    def build_LagT(alpha, beta, N, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        The, vectorized implementation of the, measure derived from the translated Laguerre basis.\n",
    "\n",
    "        Args:\n",
    "            alpha (float): The order of the Laguerre basis.\n",
    "            beta (float): The scale of the Laguerre basis.\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        L = jnp.exp(\n",
    "            0.5\n",
    "            * (ss.gammaln(jnp.arange(N) + alpha + 1) - ss.gammaln(jnp.arange(N) + 1))\n",
    "        )\n",
    "        inv_L = 1.0 / L[:, None]\n",
    "        pre_A = (jnp.eye(N) * ((1 + beta) / 2)) + jnp.tril(jnp.ones((N, N)), -1)\n",
    "        pre_B = ss.binom(alpha + jnp.arange(N), jnp.arange(N))[:, None]\n",
    "\n",
    "        A = -inv_L * pre_A * L[None, :]\n",
    "        B = (\n",
    "            jnp.exp(-0.5 * ss.gammaln(1 - alpha))\n",
    "            * jnp.power(beta, (1 - alpha) / 2)\n",
    "            * inv_L\n",
    "            * pre_B\n",
    "        )\n",
    "\n",
    "        return A.astype(dtype), B.astype(dtype)\n",
    "\n",
    "    # Scaled Legendre (LegS) vectorized\n",
    "    @staticmethod\n",
    "    def build_LegS(N, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        The, vectorized implementation of the, measure derived from the Scaled Legendre basis.\n",
    "\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        q = jnp.arange(N, dtype=dtype)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "        pre_D = jnp.sqrt(jnp.diag(2 * q + 1))\n",
    "        B = D = jnp.diag(pre_D)[:, None]\n",
    "\n",
    "        A_base = jnp.sqrt(2 * n + 1) * jnp.sqrt(2 * k + 1)\n",
    "\n",
    "        A = jnp.where(n > k, A_base, jnp.where(n == k, n + 1, 0.0))\n",
    "\n",
    "        return -A.astype(dtype), B.astype(dtype)\n",
    "\n",
    "    # Fourier Basis OPs and functions - vectorized\n",
    "    @staticmethod\n",
    "    def build_Fourier(N, fourier_type=\"fru\", dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        Vectorized measure implementations derived from fourier basis.\n",
    "\n",
    "        Args:\n",
    "            N (int): Order of coefficients to describe the orthogonal polynomial that is the HiPPO projection.\n",
    "            fourier_type (str): The type of Fourier measure.\n",
    "                - FRU: Fourier Recurrent Unit - fru\n",
    "                - FouT: truncated Fourier - fout\n",
    "                - fouD: decayed Fourier - foud\n",
    "\n",
    "        Returns:\n",
    "            A (jnp.ndarray): The A HiPPO matrix.\n",
    "            B (jnp.ndarray): The B HiPPO matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        A = jnp.diag(\n",
    "            jnp.stack([jnp.zeros(N // 2), jnp.zeros(N // 2)], axis=-1).reshape(-1)[1:],\n",
    "            1,\n",
    "        )\n",
    "        B = jnp.zeros(A.shape[1], dtype=dtype)\n",
    "\n",
    "        B = B.at[0::2].set(jnp.sqrt(2))\n",
    "        B = B.at[0].set(1)\n",
    "\n",
    "        q = jnp.arange(A.shape[1], dtype=dtype)\n",
    "        k, n = jnp.meshgrid(q, q)\n",
    "\n",
    "        n_odd = n % 2 == 0\n",
    "        k_odd = k % 2 == 0\n",
    "\n",
    "        case_1 = (n == k) & (n == 0)\n",
    "        case_2_3 = ((k == 0) & (n_odd)) | ((n == 0) & (k_odd))\n",
    "        case_4 = (n_odd) & (k_odd)\n",
    "        case_5 = (n - k == 1) & (k_odd)\n",
    "        case_6 = (k - n == 1) & (n_odd)\n",
    "\n",
    "        if fourier_type == \"fru\":  # Fourier Recurrent Unit (FRU) - vectorized\n",
    "            A = jnp.where(\n",
    "                case_1,\n",
    "                -1.0,\n",
    "                jnp.where(\n",
    "                    case_2_3,\n",
    "                    -jnp.sqrt(2),\n",
    "                    jnp.where(\n",
    "                        case_4,\n",
    "                        -2,\n",
    "                        jnp.where(\n",
    "                            case_5,\n",
    "                            jnp.pi * (n // 2),\n",
    "                            jnp.where(case_6, -jnp.pi * (k // 2), 0.0),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        elif fourier_type == \"fout\":  # truncated Fourier (FouT) - vectorized\n",
    "            A = jnp.where(\n",
    "                case_1,\n",
    "                -1.0,\n",
    "                jnp.where(\n",
    "                    case_2_3,\n",
    "                    -jnp.sqrt(2),\n",
    "                    jnp.where(\n",
    "                        case_4,\n",
    "                        -2,\n",
    "                        jnp.where(\n",
    "                            case_5,\n",
    "                            jnp.pi * (n // 2),\n",
    "                            jnp.where(case_6, -jnp.pi * (k // 2), 0.0),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            A = 2 * A\n",
    "            B = 2 * B\n",
    "\n",
    "        elif fourier_type == \"foud\":\n",
    "            A = jnp.where(\n",
    "                case_1,\n",
    "                -1.0,\n",
    "                jnp.where(\n",
    "                    case_2_3,\n",
    "                    -jnp.sqrt(2),\n",
    "                    jnp.where(\n",
    "                        case_4,\n",
    "                        -2,\n",
    "                        jnp.where(\n",
    "                            case_5,\n",
    "                            2 * jnp.pi * (n // 2),\n",
    "                            jnp.where(case_6, 2 * -jnp.pi * (k // 2), 0.0),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            A = 0.5 * A\n",
    "            B = 0.5 * B\n",
    "\n",
    "        B = B[:, None]\n",
    "\n",
    "        return A.astype(dtype), B.astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Legendre (LegT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LegT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LegT():\n",
    "    legt_matrices = TransMatrix(N=8, measure=\"legt\", lambda_n=1.0)\n",
    "    A, B = legt_matrices.A, legt_matrices.B\n",
    "    gu_legt_matrices = GuTransMatrix(N=8, measure=\"legt\", lambda_n=1.0)\n",
    "    gu_A, gu_B = gu_legt_matrices.A, gu_legt_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -1.          1.7320508  -2.2360678   2.6457512  -3.          3.3166246  -3.6055512   3.8729832]\n",
      " [ -1.7320508  -3.          3.872983   -4.5825753   5.196152   -5.744562    6.244998   -6.708204 ]\n",
      " [ -2.2360678  -3.872983   -4.999999    5.916079   -6.7082033   7.4161973  -8.062257    8.660253 ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -6.9999995   7.937254   -8.774963    9.5393915 -10.24695  ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -9.          9.949874  -10.816654   11.61895  ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874  -10.999999   11.958261  -12.845232 ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261  -13.         13.964239 ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232  -13.964239  -14.999999 ]]\n",
      "Gu's A:\n",
      " [[ -1.          1.7320508  -2.2360678   2.6457512  -3.          3.3166246  -3.6055512   3.8729832]\n",
      " [ -1.7320508  -3.          3.872983   -4.5825753   5.196152   -5.744562    6.244998   -6.708204 ]\n",
      " [ -2.2360678  -3.872983   -4.999999    5.916079   -6.7082033   7.4161973  -8.062257    8.660253 ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -6.9999995   7.937254   -8.774963    9.5393915 -10.24695  ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -9.          9.949874  -10.816654   11.61895  ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874  -10.999999   11.958261  -12.845232 ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261  -13.         13.964239 ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232  -13.964239  -14.999999 ]]\n",
      "B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n",
      "Gu's B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n"
     ]
    }
   ],
   "source": [
    "test_LegT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LMU():\n",
    "    lmu_matrices = TransMatrix(\n",
    "        N=8, measure=\"legt\", lambda_n=2.0\n",
    "    )  # change lambda so resulting matrix is in the form of LMU\n",
    "    A, B = lmu_matrices.A, lmu_matrices.B\n",
    "    gu_lmu_matrices = GuTransMatrix(\n",
    "        N=8, measure=\"legt\", lambda_n=2.0\n",
    "    )  # change lambda so resulting matrix is in the form of LMU\n",
    "    gu_A, gu_B = gu_lmu_matrices.A, gu_lmu_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.]\n",
      " [ -5.   5.  -5.  -5.  -5.  -5.  -5.  -5.]\n",
      " [  7.  -7.   7.  -7.  -7.  -7.  -7.  -7.]\n",
      " [ -9.   9.  -9.   9.  -9.  -9.  -9.  -9.]\n",
      " [ 11. -11.  11. -11.  11. -11. -11. -11.]\n",
      " [-13.  13. -13.  13. -13.  13. -13. -13.]\n",
      " [ 15. -15.  15. -15.  15. -15.  15. -15.]]\n",
      "Gu's A:\n",
      " [[ -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  3.  -3.  -3.  -3.  -3.  -3.  -3.  -3.]\n",
      " [ -5.   5.  -5.  -5.  -5.  -5.  -5.  -5.]\n",
      " [  7.  -7.   7.  -7.  -7.  -7.  -7.  -7.]\n",
      " [ -9.   9.  -9.   9.  -9.  -9.  -9.  -9.]\n",
      " [ 11. -11.  11. -11.  11. -11. -11. -11.]\n",
      " [-13.  13. -13.  13. -13.  13. -13. -13.]\n",
      " [ 15. -15.  15. -15.  15. -15.  15. -15.]]\n",
      "B:\n",
      " [[  1.]\n",
      " [ -3.]\n",
      " [  5.]\n",
      " [ -7.]\n",
      " [  9.]\n",
      " [-11.]\n",
      " [ 13.]\n",
      " [-15.]]\n",
      "Gu's B:\n",
      " [[  1.]\n",
      " [ -3.]\n",
      " [  5.]\n",
      " [ -7.]\n",
      " [  9.]\n",
      " [-11.]\n",
      " [ 13.]\n",
      " [-15.]]\n"
     ]
    }
   ],
   "source": [
    "test_LMU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translated Laguerre (LagT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LagT():\n",
    "    lagt_matrices = TransMatrix(\n",
    "        N=8,\n",
    "        measure=\"lagt\",\n",
    "        alpha=0.0,  # change resulting tilt through alpha and beta\n",
    "        beta=1.0,\n",
    "    )  # change resulting tilt through alpha and beta\n",
    "    A, B = lagt_matrices.A, lagt_matrices.B\n",
    "    gu_lagt_matrices = GuTransMatrix(\n",
    "        N=8,\n",
    "        measure=\"lagt\",\n",
    "        alpha=0.0,  # change resulting tilt through alpha and beta\n",
    "        beta=1.0,\n",
    "    )  # change resulting tilt through alpha and beta\n",
    "    gu_A, gu_B = gu_lagt_matrices.A, gu_lagt_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-1.         -0.         -0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.         -1.         -0.        ]\n",
      " [-0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -1.        ]]\n",
      "Gu's A:\n",
      " [[-1.         -0.         -0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -0.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.         -0.         -0.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.         -1.         -0.        ]\n",
      " [-0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -0.99999976 -1.        ]]\n",
      "B:\n",
      " [[1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Gu's B:\n",
      " [[1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n"
     ]
    }
   ],
   "source": [
    "test_LagT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Legendre (LegS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LegS():\n",
    "    legs_matrices = TransMatrix(N=8, measure=\"legs\")\n",
    "    A, B = legs_matrices.A, legs_matrices.B\n",
    "    gu_legs_matrices = GuTransMatrix(N=8, measure=\"legs\")\n",
    "    gu_A, gu_B = gu_legs_matrices.A, gu_legs_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -1.         -0.         -0.         -0.         -0.         -0.         -0.         -0.       ]\n",
      " [ -1.7320508  -2.         -0.         -0.         -0.         -0.         -0.         -0.       ]\n",
      " [ -2.2360678  -3.872983   -3.         -0.         -0.         -0.         -0.         -0.       ]\n",
      " [ -2.6457512  -4.5825753  -5.916079   -4.         -0.         -0.         -0.         -0.       ]\n",
      " [ -3.         -5.196152   -6.7082033  -7.937254   -5.         -0.         -0.         -0.       ]\n",
      " [ -3.3166246  -5.744562   -7.4161973  -8.774963   -9.949874   -6.         -0.         -0.       ]\n",
      " [ -3.6055512  -6.244998   -8.062257   -9.5393915 -10.816654  -11.958261   -7.         -0.       ]\n",
      " [ -3.8729832  -6.708204   -8.660253  -10.24695   -11.61895   -12.845232  -13.964239   -8.       ]]\n",
      "Gu's A:\n",
      " [[ -1.          0.          0.          0.          0.          0.          0.          0.       ]\n",
      " [ -1.7320508  -1.9999999   0.          0.          0.          0.          0.          0.       ]\n",
      " [ -2.2360678  -3.872983   -3.          0.          0.          0.          0.          0.       ]\n",
      " [ -2.6457512  -4.582576   -5.91608    -4.          0.          0.          0.          0.       ]\n",
      " [ -3.         -5.196152   -6.7082047  -7.9372544  -5.          0.          0.          0.       ]\n",
      " [ -3.3166246  -5.744562   -7.4161987  -8.774965   -9.949874   -6.          0.          0.       ]\n",
      " [ -3.6055512  -6.244998   -8.062259   -9.539392  -10.816654  -11.958261   -7.          0.       ]\n",
      " [ -3.8729832  -6.708204   -8.6602545 -10.246951  -11.61895   -12.845232  -13.964239   -7.9999995]]\n",
      "B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n",
      "Gu's B:\n",
      " [[1.       ]\n",
      " [1.7320508]\n",
      " [2.2360678]\n",
      " [2.6457512]\n",
      " [3.       ]\n",
      " [3.3166246]\n",
      " [3.6055512]\n",
      " [3.8729832]]\n"
     ]
    }
   ],
   "source": [
    "test_LegS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Recurrent Unit (FRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FRU():\n",
    "    fru_matrices = TransMatrix(N=8, measure=\"fru\")\n",
    "    A, B = fru_matrices.A, fru_matrices.B\n",
    "    gu_fru_matrices = GuTransMatrix(N=8, measure=\"fru\")\n",
    "    gu_A, gu_B = gu_fru_matrices.A, gu_fru_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-1.        -0.        -1.4142135  0.        -1.4142135  0.        -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.         0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -2.        -3.1415927 -2.         0.        -2.         0.       ]\n",
      " [ 0.         0.         3.1415927  0.         0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -2.         0.        -2.        -6.2831855 -2.         0.       ]\n",
      " [ 0.         0.         0.         0.         6.2831855  0.         0.         0.       ]\n",
      " [-1.4142135  0.        -2.         0.        -2.         0.        -2.        -9.424778 ]\n",
      " [ 0.         0.         0.         0.         0.         0.         9.424778   0.       ]]\n",
      "Gu's A:\n",
      " [[-1.         0.        -1.4142135  0.        -1.4142135  0.        -1.4142135  0.       ]\n",
      " [ 0.         0.         0.         0.         0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999 -3.1415927 -1.9999999  0.        -1.9999999  0.       ]\n",
      " [ 0.         0.         3.1415927  0.         0.         0.         0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999  0.        -1.9999999 -6.2831855 -1.9999999  0.       ]\n",
      " [ 0.         0.         0.         0.         6.2831855  0.         0.         0.       ]\n",
      " [-1.4142135  0.        -1.9999999  0.        -1.9999999  0.        -1.9999999 -9.424778 ]\n",
      " [ 0.         0.         0.         0.         0.         0.         9.424778   0.       ]]\n",
      "B:\n",
      " [[1.       ]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]]\n",
      "Gu's B:\n",
      " [[1.       ]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]\n",
      " [1.4142135]\n",
      " [0.       ]]\n"
     ]
    }
   ],
   "source": [
    "test_FRU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated Fourier (FouT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FouT():\n",
    "    fout_matrices = TransMatrix(N=8, measure=\"fout\")\n",
    "    A, B = fout_matrices.A, fout_matrices.B\n",
    "    gu_fout_matrices = GuTransMatrix(N=8, measure=\"fout\")\n",
    "    gu_A, gu_B = gu_fout_matrices.A, gu_fout_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ -2.         -0.         -2.828427    0.         -2.828427    0.         -2.828427    0.       ]\n",
      " [  0.          0.          0.          0.          0.          0.          0.          0.       ]\n",
      " [ -2.828427    0.         -4.         -6.2831855  -4.          0.         -4.          0.       ]\n",
      " [  0.          0.          6.2831855   0.          0.          0.          0.          0.       ]\n",
      " [ -2.828427    0.         -4.          0.         -4.        -12.566371   -4.          0.       ]\n",
      " [  0.          0.          0.          0.         12.566371    0.          0.          0.       ]\n",
      " [ -2.828427    0.         -4.          0.         -4.          0.         -4.        -18.849556 ]\n",
      " [  0.          0.          0.          0.          0.          0.         18.849556    0.       ]]\n",
      "Gu's A:\n",
      " [[ -2.          0.         -2.828427    0.         -2.828427    0.         -2.828427    0.       ]\n",
      " [  0.          0.          0.          0.          0.          0.          0.          0.       ]\n",
      " [ -2.828427    0.         -3.9999998  -6.2831855  -3.9999998   0.         -3.9999998   0.       ]\n",
      " [  0.          0.          6.2831855   0.          0.          0.          0.          0.       ]\n",
      " [ -2.828427    0.         -3.9999998   0.         -3.9999998 -12.566371   -3.9999998   0.       ]\n",
      " [  0.          0.          0.          0.         12.566371    0.          0.          0.       ]\n",
      " [ -2.828427    0.         -3.9999998   0.         -3.9999998   0.         -3.9999998 -18.849556 ]\n",
      " [  0.          0.          0.          0.          0.          0.         18.849556    0.       ]]\n",
      "B:\n",
      " [[2.      ]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]]\n",
      "Gu's B:\n",
      " [[2.      ]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]\n",
      " [2.828427]\n",
      " [0.      ]]\n"
     ]
    }
   ],
   "source": [
    "test_FouT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier With Decay (FourD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FouD():\n",
    "    the_measure = \"foud\"\n",
    "    foud_matrices = TransMatrix(N=8, measure=\"foud\")\n",
    "    A, B = foud_matrices.A, foud_matrices.B\n",
    "    gu_foud_matrices = GuTransMatrix(N=8, measure=\"foud\")\n",
    "    gu_A, gu_B = gu_foud_matrices.A, gu_foud_matrices.B\n",
    "    print(f\"A:\\n\", A)\n",
    "    print(f\"Gu's A:\\n\", gu_A)\n",
    "    print(f\"B:\\n\", B)\n",
    "    print(f\"Gu's B:\\n\", gu_B)\n",
    "    assert jnp.allclose(A, gu_A)\n",
    "    assert jnp.allclose(B, gu_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-0.5        -0.         -0.70710677  0.         -0.70710677  0.         -0.70710677  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.70710677  0.         -1.         -3.1415927  -1.          0.         -1.          0.        ]\n",
      " [ 0.          0.          3.1415927   0.          0.          0.          0.          0.        ]\n",
      " [-0.70710677  0.         -1.          0.         -1.         -6.2831855  -1.          0.        ]\n",
      " [ 0.          0.          0.          0.          6.2831855   0.          0.          0.        ]\n",
      " [-0.70710677  0.         -1.          0.         -1.          0.         -1.         -9.424778  ]\n",
      " [ 0.          0.          0.          0.          0.          0.          9.424778    0.        ]]\n",
      "Gu's A:\n",
      " [[-0.5         0.         -0.70710677  0.         -0.70710677  0.         -0.70710677  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.70710677  0.         -0.99999994 -3.1415927  -0.99999994  0.         -0.99999994  0.        ]\n",
      " [ 0.          0.          3.1415927   0.          0.          0.          0.          0.        ]\n",
      " [-0.70710677  0.         -0.99999994  0.         -0.99999994 -6.2831855  -0.99999994  0.        ]\n",
      " [ 0.          0.          0.          0.          6.2831855   0.          0.          0.        ]\n",
      " [-0.70710677  0.         -0.99999994  0.         -0.99999994  0.         -0.99999994 -9.424778  ]\n",
      " [ 0.          0.          0.          0.          0.          0.          9.424778    0.        ]]\n",
      "B:\n",
      " [[0.5       ]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]]\n",
      "Gu's B:\n",
      " [[0.5       ]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]\n",
      " [0.70710677]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "test_FouD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gu's HiPPO LegT Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LTI(nn.Module):\n",
    "    \"\"\"Linear time invariant x' = Ax + Bu\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        N,\n",
    "        method=\"legt\",\n",
    "        dt=1.0,\n",
    "        T=1.0,\n",
    "        discretization=0.5,\n",
    "        lambda_n=1.0,\n",
    "        alpha=0.0,\n",
    "        beta=1.0,\n",
    "        c=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        N: the order of the HiPPO projection\n",
    "        dt: discretization step size - should be roughly inverse to the length of the sequence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.method = method\n",
    "        self.N = N\n",
    "        self.dt = dt\n",
    "        self.T = T\n",
    "        self.c = c\n",
    "\n",
    "        matrices = GuTransMatrix(\n",
    "            N=N, measure=method, lambda_n=lambda_n, alpha=alpha, beta=beta\n",
    "        )\n",
    "        A = matrices.A\n",
    "        B = matrices.B\n",
    "        # A, B = transition(method, N)\n",
    "        A = A + np.eye(N) * c\n",
    "        self.A = A\n",
    "        self.B = B.squeeze(-1)\n",
    "        self.measure_fn = measure(method)\n",
    "\n",
    "        C = np.ones((1, N))\n",
    "        D = np.zeros((1,))\n",
    "        if type(discretization) in [float, int]:\n",
    "            dA, dB, _, _, _ = signal.cont2discrete(\n",
    "                (A, B, C, D), dt=dt, method=\"gbt\", alpha=discretization\n",
    "            )\n",
    "        else:\n",
    "            dA, dB, _, _, _ = signal.cont2discrete((A, B, C, D), dt=dt, method=\"zoh\")\n",
    "\n",
    "        dB = dB.squeeze(-1)\n",
    "\n",
    "        self.dA = torch.Tensor(dA.copy())  # (N, N)\n",
    "        self.dB = torch.Tensor(dB.copy())  # (N, )\n",
    "\n",
    "        self.vals = np.arange(0.0, T, dt)\n",
    "        self.eval_matrix = basis(self.method, self.N, self.vals, c=self.c)  # (T/dt, N)\n",
    "        self.measure = measure(self.method)(self.vals)\n",
    "\n",
    "    def forward(self, inputs, fast=True):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = inputs * self.dB  # (length, ..., N)\n",
    "\n",
    "        if fast:\n",
    "            dA = repeat(self.dA, \"m n -> l m n\", l=u.size(0))\n",
    "            return variable_unroll_matrix(dA, u)\n",
    "\n",
    "        c = torch.zeros(u.shape[1:]).to(inputs)\n",
    "        cs = []\n",
    "        for f in inputs:\n",
    "            \n",
    "            # print(f\"dA shape:\\n{self.dA.shape}\")\n",
    "            # print(f\"dA:\\n{self.dA}\")\n",
    "\n",
    "            # print(f\"c shape:\\n{c.shape}\")\n",
    "            # print(f\"c:\\n{c}\")\n",
    "\n",
    "            # print(f\"dB shape:\\n{self.dB.shape}\")\n",
    "            # print(f\"dB:\\n{self.dB}\")\n",
    "\n",
    "            # print(f\"f shape:\\n{f.shape}\")\n",
    "            # print(f\"f:\\n{f}\")\n",
    "            \n",
    "            part1 = F.linear(c, self.dA)\n",
    "            part2 = self.dB * f\n",
    "            \n",
    "            c = part1 + part2\n",
    "\n",
    "            # print(f\"part1 shape:\\n{part1.shape}\")\n",
    "            # print(f\"part1 :\\n{part1}\")\n",
    "\n",
    "            # print(f\"part2 shape:\\n{part2.shape}\")\n",
    "            # print(f\"part2:\\n{part2}\")\n",
    "            \n",
    "            cs.append(c)\n",
    "        return torch.stack(cs, dim=0)\n",
    "\n",
    "    def reconstruct(\n",
    "        self, c, evals=None\n",
    "    ):  # TODO take in a times array for reconstruction\n",
    "        \"\"\"\n",
    "        c: (..., N,) HiPPO coefficients (same as x(t) in S4 notation)\n",
    "        output: (..., L,)\n",
    "        \"\"\"\n",
    "        if evals is not None:\n",
    "            eval_matrix = basis(self.method, self.N, evals)\n",
    "        else:\n",
    "            eval_matrix = self.eval_matrix\n",
    "\n",
    "        m = self.measure[self.measure != 0.0]\n",
    "\n",
    "        c = c.unsqueeze(-1)\n",
    "        y = eval_matrix.to(c) @ c\n",
    "        return y.squeeze(-1).flip(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gu's Scale invariant HiPPO LegS Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO_LSI(nn.Module):\n",
    "    \"\"\"Vanilla HiPPO-LegS model (scale invariant instead of time invariant)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        N,\n",
    "        method=\"legs\",\n",
    "        max_length=1024,\n",
    "        discretization=0.5,\n",
    "        lambda_n=1.0,\n",
    "        alpha=0.0,\n",
    "        beta=1.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        max_length: maximum sequence length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        matrices = GuTransMatrix(\n",
    "            N=N, measure=method, lambda_n=lambda_n, alpha=alpha, beta=beta\n",
    "        )\n",
    "        A = matrices.A\n",
    "        B = matrices.B\n",
    "        # A, B = transition(method, N)\n",
    "        B = B.squeeze(-1)\n",
    "        A_stacked = np.empty((max_length, N, N), dtype=A.dtype)\n",
    "        B_stacked = np.empty((max_length, N), dtype=B.dtype)\n",
    "        for t in range(1, max_length + 1):\n",
    "            At = A / t\n",
    "            Bt = B / t\n",
    "            if discretization == 0.0:  # forward\n",
    "                A_stacked[t - 1] = np.eye(N) + At\n",
    "                B_stacked[t - 1] = Bt\n",
    "            elif discretization == 1.0:  # backward\n",
    "                A_stacked[t - 1] = la.solve_triangular(\n",
    "                    np.eye(N) - At, np.eye(N), lower=True\n",
    "                )\n",
    "                B_stacked[t - 1] = la.solve_triangular(np.eye(N) - At, Bt, lower=True)\n",
    "            elif discretization == 0.5:  # bilinear\n",
    "                # A_stacked[t - 1] = la.solve_triangular(\n",
    "                #     np.eye(N) - At / 2, np.eye(N) + At / 2, lower=True\n",
    "                # )\n",
    "                # B_stacked[t - 1] = la.solve_triangular(\n",
    "                #     np.eye(N) - At / 2, Bt, lower=True\n",
    "                # )\n",
    "                alpha = 0.5\n",
    "                A_stacked[t - 1] = np.linalg.lstsq(\n",
    "                    np.eye(N) - (At * alpha), np.eye(N) + (At * alpha), rcond=None\n",
    "                )[\n",
    "                    0\n",
    "                ]  # TODO: Referencing this: https://stackoverflow.com/questions/64527098/numpy-linalg-linalgerror-singular-matrix-error-when-trying-to-solve\n",
    "                B_stacked[t - 1] = np.linalg.lstsq(\n",
    "                    np.eye(N) - (At * alpha), Bt, rcond=None\n",
    "                )[0]\n",
    "            else:  # ZOH\n",
    "                A_stacked[t - 1] = la.expm(A * (math.log(t + 1) - math.log(t)))\n",
    "                B_stacked[t - 1] = la.solve_triangular(\n",
    "                    A, A_stacked[t - 1] @ B - B, lower=True\n",
    "                )\n",
    "        self.A_stacked = torch.Tensor(A_stacked.copy())  # (max_length, N, N)\n",
    "        self.B_stacked = torch.Tensor(B_stacked.copy())  # (max_length, N)\n",
    "\n",
    "        vals = np.linspace(0.0, 1.0, max_length)\n",
    "        self.eval_matrix = torch.from_numpy(\n",
    "            np.asarray(\n",
    "                ((B[:, None] * ss.eval_legendre(np.arange(N)[:, None], 2 * vals - 1)).T)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, fast=True):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "\n",
    "        L = inputs.shape[0]\n",
    "\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        u = torch.transpose(inputs, 0, -2)\n",
    "        u = u * self.B_stacked[:L]\n",
    "        # print(f\"Gu - u * self.B_stacked[:L]: {u}\")\n",
    "        u = torch.transpose(u, 0, -2)  # (length, ..., N)\n",
    "\n",
    "        if fast:\n",
    "            result = variable_unroll_matrix(self.A_stacked[:L], u)\n",
    "            return result\n",
    "\n",
    "        c = torch.zeros(u.shape[1:]).to(inputs)\n",
    "        cs = []\n",
    "        for t, f in enumerate(inputs):\n",
    "            # print(f\"\\n--------------step {t}----------------\")\n",
    "            # print(f\"self.A_stacked[{t}] shape:\\n{self.A_stacked[t].shape}\")\n",
    "            # print(f\"self.A_stacked[{t}]:\\n{self.A_stacked[t]}\")\n",
    "\n",
    "            # print(f\"c shape:\\n{c.shape}\")\n",
    "            # print(f\"c:\\n{c}\")\n",
    "\n",
    "            # print(f\"self.B_stacked[{t}] shape:\\n{self.B_stacked[t].shape}\")\n",
    "            # print(f\"self.B_stacked[{t}]:\\n{self.B_stacked[t]}\")\n",
    "\n",
    "            # print(f\"f shape:\\n{f.shape}\")\n",
    "            # print(f\"f:\\n{f}\")\n",
    "            \n",
    "            part1 = F.linear(c, self.A_stacked[t])\n",
    "            part2 = self.B_stacked[t] * f\n",
    "            \n",
    "            c = part1 + part2\n",
    "\n",
    "            # print(f\"part1 - {t} - shape:\\n{part1.shape}\")\n",
    "            # print(f\"part1 - {t} -:\\n{part1}\")\n",
    "\n",
    "            # print(f\"part2 - {t} - shape:\\n{part2.shape}\")\n",
    "            # print(f\"part2 - {t} -:\\n{part2}\")\n",
    "            \n",
    "            cs.append(c)\n",
    "        return torch.stack(cs, dim=0)\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        a = self.eval_matrix.to(c) @ c.unsqueeze(-1)\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Of General HiPPO Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPO(jnn.Module):\n",
    "    \"\"\"\n",
    "    class that constructs HiPPO model using the defined measure.\n",
    "\n",
    "    Args:\n",
    "        N (int): order of the HiPPO projection, aka the number of coefficients to describe the matrix\n",
    "        max_length (int): maximum sequence length to be input\n",
    "        measure (str): the measure used to define which way to instantiate the HiPPO matrix\n",
    "        step (float): step size used for descretization\n",
    "        GBT_alpha (float): represents which descretization transformation to use based off the alpha value\n",
    "        seq_L (int): length of the sequence to be used for training\n",
    "        v (str): choice of vectorized or non-vectorized function instantiation\n",
    "            - 'v': vectorized\n",
    "            - 'nv': non-vectorized\n",
    "        lambda_n (float): value associated with the tilt of legt\n",
    "            - 1: tilt on legt\n",
    "            - \\sqrt(2n+1)(-1)^{N}: tilt associated with the legendre memory unit (LMU)\n",
    "        fourier_type (str): choice of fourier measures\n",
    "            - fru: fourier recurrent unit measure (FRU) - 'fru'\n",
    "            - fout: truncated Fourier (FouT) - 'fout'\n",
    "            - fourd: decaying fourier transform - 'fourd'\n",
    "        alpha (float): The order of the Laguerre basis.\n",
    "        beta (float): The scale of the Laguerre basis.\n",
    "    \"\"\"\n",
    "\n",
    "    max_length: int\n",
    "    step_size: float = 1.0  # < 1.0 if you want to use LTI discretization\n",
    "    N: int = 100\n",
    "    lambda_n: float = 1.0\n",
    "    alpha: float = 0.0\n",
    "    beta: float = 1.0\n",
    "    GBT_alpha: float = 0.5\n",
    "    measure: str = \"legs\"\n",
    "    s_t: str = \"lti\"\n",
    "    dtype: Any = jnp.float32\n",
    "    verbose: bool = False\n",
    "\n",
    "    def setup(self):\n",
    "        matrices = TransMatrix(\n",
    "            N=self.N,\n",
    "            measure=self.measure,\n",
    "            lambda_n=self.lambda_n,\n",
    "            alpha=self.alpha,\n",
    "            beta=self.beta,\n",
    "            dtype=self.dtype,\n",
    "        )\n",
    "\n",
    "        self.A = matrices.A\n",
    "        self.B = matrices.B\n",
    "\n",
    "        self.C = jnp.ones((self.N, 1))\n",
    "        self.D = jnp.zeros((1,))\n",
    "\n",
    "        if self.step_size == 1.0:\n",
    "            self.GBT_A_list, self.GBT_B_list = self.make_GBT_list(\n",
    "                matrices.A, matrices.B, dtype=self.dtype\n",
    "            )\n",
    "\n",
    "        self.eval_matrix = self.create_eval_matrix(matrices.A, matrices.B)\n",
    "\n",
    "    def __call__(self, f, init_state=None, kernel=False):\n",
    "        if not kernel:\n",
    "            if init_state is None:\n",
    "                # init_state = jnp.zeros((f.shape[0], self.N, 1))\n",
    "                init_state = jnp.zeros((f.shape[0], 1, self.N))\n",
    "\n",
    "            if self.s_t == \"lsi\":\n",
    "                c_k, y_k = self.lsi_recurrence(\n",
    "                    A=self.GBT_A_list,\n",
    "                    B=self.GBT_B_list,\n",
    "                    C=self.C,\n",
    "                    D=self.D,\n",
    "                    c_0=init_state,\n",
    "                    f=f,\n",
    "                    alpha=self.GBT_alpha,\n",
    "                    dtype=self.dtype,\n",
    "                )\n",
    "                c_k = jnp.stack(c_k, axis=0)\n",
    "                y_k = jnp.stack(y_k, axis=0)\n",
    "\n",
    "            elif self.s_t == \"lti\":\n",
    "                c_k, y_k = self.lti_recurrence(\n",
    "                    A=self.A,\n",
    "                    B=self.B,\n",
    "                    C=self.C,\n",
    "                    D=self.D,\n",
    "                    c_0=init_state,\n",
    "                    f=f,\n",
    "                    alpha=self.GBT_alpha,\n",
    "                    step_size=self.step_size,\n",
    "                    dtype=self.dtype,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Incorrect value associated with invariance options, either pick 'lsi' or 'lti'.\")\n",
    "\n",
    "        else:\n",
    "            Ab, Bb, Cb, Db = self.discretize(\n",
    "                self.A,\n",
    "                self.B,\n",
    "                self.C,\n",
    "                self.D,\n",
    "                step=self.step_size,\n",
    "                alpha=self.GBT_alpha,\n",
    "            )\n",
    "            c_k, y_k = self.causal_convolution(\n",
    "                f, self.K_conv(Ab, Bb, Cb, Db, L=self.max_length)\n",
    "            )\n",
    "\n",
    "        return c_k, y_k\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        \"\"\"\n",
    "        Uses coeffecients to reconstruct the signal\n",
    "\n",
    "        Args:\n",
    "            c (jnp.ndarray): coefficients of the HiPPO projection\n",
    "\n",
    "        Returns:\n",
    "            reconstructed signal\n",
    "        \"\"\"\n",
    "        return (self.eval_matrix @ jnp.expand_dims(c, -1)).squeeze(-1)\n",
    "\n",
    "    def make_GBT_list(self, A, B, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        Creates the discretized GBT matrices for the given step size\n",
    "        \"\"\"\n",
    "        GBT_a_list = []\n",
    "        GBT_b_list = []\n",
    "        for i in range(1, self.max_length + 1):\n",
    "            # TODO: make this scale invariant optional\n",
    "            GBT_A, GBT_B = self.discretize(\n",
    "                A, B, step=i, alpha=self.GBT_alpha, dtype=dtype\n",
    "            )\n",
    "            GBT_a_list.append(GBT_A)\n",
    "            GBT_b_list.append(GBT_B)\n",
    "\n",
    "        return GBT_a_list, GBT_b_list\n",
    "\n",
    "    def create_eval_matrix(self, A, B):\n",
    "        \"\"\"\n",
    "        Creates the evaluation matrix used for reconstructing the signal\n",
    "        \"\"\"\n",
    "        eval_matrix = None\n",
    "        if self.measure == \"legt\":\n",
    "            L = self.max_length\n",
    "            vals = jnp.arange(0.0, 1.0, L)\n",
    "            # n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 1 - 2 * vals\n",
    "            eval_matrix = jax.scipy.special.lpmn_values(\n",
    "                m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "            ).T  # ss.eval_legendre(n, x).T\n",
    "\n",
    "        elif self.measure == \"legs\":\n",
    "            L = self.max_length\n",
    "            vals = jnp.linspace(0.0, 1.0, L)\n",
    "            # n = jnp.arange(self.N)[:, None]\n",
    "            zero_N = self.N - 1\n",
    "            x = 2 * vals - 1\n",
    "            eval_matrix = (\n",
    "                B[:, None]\n",
    "                * jax.scipy.special.lpmn_values(\n",
    "                    m=zero_N, n=zero_N, z=x, is_normalized=False\n",
    "                )\n",
    "            ).T  # ss.eval_legendre(n, x)).T\n",
    "\n",
    "        elif self.measure == \"lagt\":\n",
    "            raise NotImplementedError(\"Translated Laguerre measure not implemented yet\")\n",
    "\n",
    "        elif self.measure == \"fourier\":\n",
    "            raise NotImplementedError(\"Fourier measures are not implemented yet\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"invalid measure\")\n",
    "\n",
    "        return eval_matrix\n",
    "    \n",
    "    def discretize(self, A, B, step, alpha=0.5, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        function used for discretizing the HiPPO matrix\n",
    "\n",
    "        Args:\n",
    "            A (jnp.ndarray): matrix to be discretized\n",
    "            B (jnp.ndarray): matrix to be discretized\n",
    "            C (jnp.ndarray): matrix to be discretized\n",
    "            D (jnp.ndarray): matrix to be discretized\n",
    "            step (float): step size used for discretization\n",
    "            alpha (float, optional): used for determining which generalized bilinear transformation to use\n",
    "                - forward Euler corresponds to  = 0,\n",
    "                - backward Euler corresponds to  = 1,\n",
    "                - bilinear corresponds to  = 0.5,\n",
    "                - Zero-order Hold corresponds to  > 1\n",
    "        \"\"\"\n",
    "        I = jnp.eye(A.shape[0])\n",
    "        step_size = 1 / step\n",
    "        part1 = I - (step_size * alpha * A)\n",
    "        part2 = I + (step_size * (1 - alpha) * A)\n",
    "\n",
    "        GBT_A = jnp.linalg.lstsq(part1, part2, rcond=None)[0]\n",
    "\n",
    "        GBT_B = jnp.linalg.lstsq(part1, (step_size * B), rcond=None)[0]\n",
    "\n",
    "        if alpha > 1:  # Zero-order Hold\n",
    "            GBT_A = jax.scipy.linalg.expm(step_size * A)\n",
    "            GBT_B = (jnp.linalg.inv(A) @ (jax.scipy.linalg.expm(step_size * A) - I)) @ B\n",
    "\n",
    "        return GBT_A.astype(dtype), GBT_B.astype(dtype)\n",
    "\n",
    "    def lsi_recurrence(self, A, B, C, D, c_0, f, alpha=0.5, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            f (jnp.ndarray): the input sequence\n",
    "            c_0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "\n",
    "        c_k_list = []\n",
    "        y_k_list = []\n",
    "\n",
    "        c_k = c_0.copy()\n",
    "        for i in range(f.shape[1]):\n",
    "            # print(f\"--------------step {i}----------------\")\n",
    "            c_k, y_k = jax.vmap(self.lsi_step, in_axes=(None, None, None, None, 0, 0))(\n",
    "                A[i], B[i], C, D, c_k, f[:, i, :]\n",
    "            )\n",
    "            c_k_list.append((c_k.copy()).astype(dtype))\n",
    "            y_k_list.append((y_k.copy()).astype(dtype))\n",
    "\n",
    "        if self.verbose:\n",
    "            return c_k_list, y_k_list\n",
    "        else:\n",
    "            return c_k_list[-1], y_k_list[-1]\n",
    "\n",
    "    def lti_recurrence(self, A, B, C, D, c_0, f, alpha=0.5, step_size=1.0, dtype=jnp.float32):\n",
    "        \"\"\"\n",
    "        This is for returning the discretized hidden state often needed for an RNN.\n",
    "        Args:\n",
    "            Ab (jnp.ndarray): the discretized A matrix\n",
    "            Bb (jnp.ndarray): the discretized B matrix\n",
    "            Cb (jnp.ndarray): the discretized C matrix\n",
    "            f (jnp.ndarray): the input sequence\n",
    "            c_0 (jnp.ndarray): the initial hidden state\n",
    "        Returns:\n",
    "            the next hidden state (aka coefficients representing the function, f(t))\n",
    "        \"\"\"\n",
    "        Ad, Bd = self.discretize(\n",
    "            A=A, B=B, step=self.step_size, alpha=alpha, dtype=dtype\n",
    "        )\n",
    "\n",
    "        def lti_step(c_k_i, f_k):\n",
    "            \"\"\"\n",
    "            Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "            Args:\n",
    "                c_k_i: previous hidden state\n",
    "                f_k: output from function f at, descritized, time step, k.\n",
    "\n",
    "            Returns:\n",
    "                c_k: current hidden state\n",
    "                y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "            \"\"\"\n",
    "\n",
    "            # part1 = jnp.dot(Ad, c_k_i)\n",
    "            part1 = jnp.dot(c_k_i, Ad.T)\n",
    "            part2 = Bd.T * f_k\n",
    "            c_k = part1 + part2\n",
    "\n",
    "            part3 = jnp.dot(C, c_k)\n",
    "            part4 = D * f_k\n",
    "            y_k = part3 + part4\n",
    "            \n",
    "            # jax.debug.print(\"Ad shape:\\n{x1}\", x1=Ad.shape)\n",
    "            # jax.debug.print(\"Ad:\\n{x2}\", x2=Ad)\n",
    "            \n",
    "            # jax.debug.print(\"c_k_i shape:\\n{x3}\", x3=c_k_i.shape)\n",
    "            # jax.debug.print(\"c_k_i:\\n{x4}\", x4=c_k_i)\n",
    "            \n",
    "            # jax.debug.print(\"Bd shape:\\n{x5}\", x5=Bd.shape)\n",
    "            # jax.debug.print(\"Bd:\\n{x6}\", x6=Bd)\n",
    "            \n",
    "            # jax.debug.print(\"f_k shape:\\n{x7}\", x7=f_k.shape)\n",
    "            # jax.debug.print(\"f_k:\\n{x8}\", x8=f_k)\n",
    "            \n",
    "            # jax.debug.print(\"part1 shape:\\n{x9}\", x9=part1.shape)\n",
    "            # jax.debug.print(\"part1:\\n{x10}\", x10=part1)\n",
    "            \n",
    "            # jax.debug.print(\"part2 shape:\\n{x11}\", x11=part2.shape)\n",
    "            # jax.debug.print(\"part2:\\n{x12}\", x12=part2)\n",
    "            \n",
    "            # jax.debug.print(\"part3 shape:\\n{x13}\", x13=part3.shape)\n",
    "            # jax.debug.print(\"part3:\\n{x14}\", x14=part3)\n",
    "            \n",
    "            # jax.debug.print(\"part4 shape:\\n{x15}\", x15=part4.shape)\n",
    "            # jax.debug.print(\"part4:\\n{x16}\", x16=part4)\n",
    "\n",
    "            return c_k, (c_k, y_k)\n",
    "\n",
    "        c_k, (c_s, y_s) = jax.vmap(jax.lax.scan, in_axes=(None, 0, 0))(lti_step, c_0, f)\n",
    "\n",
    "        if self.verbose:\n",
    "            return c_s, y_s\n",
    "        else:\n",
    "            return c_k, y_s\n",
    "\n",
    "    def lsi_step(self, Ad, Bd, Cd, Dd, c_k_i, f_k):\n",
    "        \"\"\"\n",
    "        Get descretized coefficients of the hidden state by applying HiPPO matrix to input sequence, u_k, and previous hidden state, x_k_1.\n",
    "        Args:\n",
    "            c_k_i: previous hidden state\n",
    "            f_k: output from function f at, descritized, time step, k.\n",
    "\n",
    "        Returns:\n",
    "            c_k: current hidden state\n",
    "            y_k: current output of hidden state applied to Cb (sorry for being vague, I just dont know yet)\n",
    "        \"\"\"\n",
    "\n",
    "        # part1 = jnp.dot(Ad, c_k_i)\n",
    "        part1 = jnp.dot(c_k_i, Ad.T)\n",
    "        part2 = Bd.T * f_k\n",
    "        c_k = part1 + part2\n",
    "\n",
    "        part3 = jnp.dot(Cd, c_k)\n",
    "        part4 = Dd * f_k\n",
    "        y_k = part3 + part4\n",
    "        \n",
    "        # jax.debug.print(\"Ad shape:\\n{x1}\", x1=Ad.shape)\n",
    "        # jax.debug.print(\"Ad:\\n{x2}\", x2=Ad)\n",
    "        \n",
    "        # jax.debug.print(\"c_k_i shape:\\n{x3}\", x3=c_k_i.shape)\n",
    "        # jax.debug.print(\"c_k_i:\\n{x4}\", x4=c_k_i)\n",
    "        \n",
    "        # jax.debug.print(\"Bd shape:\\n{x5}\", x5=Bd.shape)\n",
    "        # jax.debug.print(\"Bd:\\n{x6}\", x6=Bd)\n",
    "        \n",
    "        # jax.debug.print(\"f_k shape:\\n{x7}\", x7=f_k.shape)\n",
    "        # jax.debug.print(\"f_k:\\n{x8}\", x8=f_k)\n",
    "        \n",
    "        # jax.debug.print(\"part1 shape:\\n{x9}\", x9=part1.shape)\n",
    "        # jax.debug.print(\"part1:\\n{x10}\", x10=part1)\n",
    "        \n",
    "        # jax.debug.print(\"part2 shape:\\n{x11}\", x11=part2.shape)\n",
    "        # jax.debug.print(\"part2:\\n{x12}\", x12=part2)\n",
    "        \n",
    "        # jax.debug.print(\"part3 shape:\\n{x13}\", x13=part3.shape)\n",
    "        # jax.debug.print(\"part3:\\n{x14}\", x14=part3)\n",
    "        \n",
    "        # jax.debug.print(\"part4 shape:\\n{x15}\", x15=part4.shape)\n",
    "        # jax.debug.print(\"part4:\\n{x16}\", x16=part4)\n",
    "\n",
    "        return c_k, y_k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LegS Bilinear Transform Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hippo_legs_lsi_bi_operator(hippo_legs, gu_hippo_legs, random_input, legs_key):\n",
    "    x_tensor = torch.tensor(random_input, dtype=torch.float32)\n",
    "    x_jnp = jnp.asarray(x_tensor, dtype=jnp.float32)  # convert torch array to jax array\n",
    "    \n",
    "    # My Implementation\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    print(f\"----------------------------My LSI Implementation Outputs----------------------------\")\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    params = hippo_legs.init(legs_key, f=x_jnp)\n",
    "    c_k, y_k_list = hippo_legs.apply(params, f=x_jnp)\n",
    "    c_k = jnp.moveaxis(c_k, 0, 1)\n",
    "    \n",
    "    # Gu's HiPPO LegS\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    print(f\"---------------------------Gu's LSI Implementation Outputs---------------------------\")\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    x_tensor = torch.moveaxis(x_tensor, 0, 1)\n",
    "    GU_c_k = gu_hippo_legs(x_tensor, fast=False)\n",
    "    gu_c = jnp.asarray(GU_c_k, dtype=jnp.float32)  # convert torch array to jax array\n",
    "    gu_c = jnp.moveaxis(gu_c, 0, 1)\n",
    "    \n",
    "    print(f\"-------------------------------------------------------------------------\")\n",
    "    print(f\"---------------------------Testing LSI Outputs---------------------------\")\n",
    "    print(f\"-------------------------------------------------------------------------\")\n",
    "    jax.debug.print(f\"inputted jnp-data shape: {x_jnp.shape}\")\n",
    "    jax.debug.print(f\"inputted tensor-data shape: {x_tensor.shape}\")\n",
    "    print(f\"c_k shape: {c_k.shape}\")\n",
    "    print(f\"gu_c shape: {gu_c.shape}\")\n",
    "    \n",
    "    for i in range(c_k.shape[0]):\n",
    "        for j in range(c_k.shape[1]):\n",
    "            print(\n",
    "                f\"batch {i} on trajectory {j} compare : {jnp.allclose(c_k[i,j,:,:], gu_c[i,j,:,:], rtol=1e-03, atol=1e-03)}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hippo_legs_lti_bi_operator(hippo_legs, gu_hippo_legs, random_input, legs_key):\n",
    "    x_tensor = torch.tensor(random_input, dtype=torch.float32)\n",
    "    x_jnp = jnp.asarray(x_tensor, dtype=jnp.float32)  # convert torch array to jax array\n",
    "    \n",
    "    # My Implementation\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    print(f\"----------------------------My LTI Implementation Outputs----------------------------\")\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    params = hippo_legs.init(legs_key, f=x_jnp)\n",
    "    c_k, y_k_list = hippo_legs.apply(params, f=x_jnp)\n",
    "    # jax.debug.print(f\"c_k: {c_k}\")\n",
    "    \n",
    "    # Gu's HiPPO LegS\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    print(f\"---------------------------Gu's LTI Implementation Outputs---------------------------\")\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    x_tensor = torch.moveaxis(x_tensor, 0, 1)\n",
    "    GU_c_k = gu_hippo_legs(x_tensor, fast=False)\n",
    "    gu_c = jnp.asarray(GU_c_k, dtype=jnp.float32)  # convert torch array to jax array\n",
    "    gu_c = jnp.moveaxis(gu_c, 0, 1)\n",
    "    # jax.debug.print(f\"gu_c: {gu_c}\")\n",
    "    \n",
    "    print(f\"-------------------------------------------------------------------------\")\n",
    "    print(f\"---------------------------Testing LTI Outputs---------------------------\")\n",
    "    print(f\"-------------------------------------------------------------------------\")\n",
    "    print(f\"inputted jnp-data shape: {x_jnp.shape}\")\n",
    "    print(f\"inputted tensor-data shape: {x_tensor.shape}\")\n",
    "    print(f\"c_k shape: {c_k.shape}\")\n",
    "    print(f\"gu_c shape: {gu_c.shape}\")\n",
    "    \n",
    "    for i in range(c_k.shape[0]):\n",
    "        for j in range(c_k.shape[1]):\n",
    "            print(\n",
    "                f\"batch {i} on trajectory {j} compare : {jnp.allclose(c_k[i,j,:,:], gu_c[i,j,:,:], rtol=1e-03, atol=1e-03)}\"\n",
    "            )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LegS ZOH Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hippo_legs_lsi_zoh_operator(hippo_legs, gu_hippo_legs, random_input, legs_key):\n",
    "    x_tensor = torch.tensor(random_input, dtype=torch.float32)\n",
    "    x_jnp = jnp.asarray(x_tensor, dtype=jnp.float32)  # convert torch array to jax array\n",
    "    \n",
    "    # My Implementation\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    print(f\"----------------------------My LTI Implementation Outputs----------------------------\")\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    params = hippo_legs.init(legs_key, f=x_jnp)\n",
    "    c_k, y_k_list = hippo_legs.apply(params, f=x_jnp)\n",
    "    # jax.debug.print(f\"c_k: {c_k}\")\n",
    "    \n",
    "    # Gu's HiPPO LegS\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    print(f\"---------------------------Gu's LTI Implementation Outputs---------------------------\")\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    x_tensor = torch.moveaxis(x_tensor, 0, 1)\n",
    "    GU_c_k = gu_hippo_legs(x_tensor, fast=False)\n",
    "    gu_c = jnp.asarray(GU_c_k, dtype=jnp.float32)  # convert torch array to jax array\n",
    "    gu_c = jnp.moveaxis(gu_c, 0, 1)\n",
    "    # jax.debug.print(f\"gu_c: {gu_c}\")\n",
    "    \n",
    "    print(f\"-------------------------------------------------------------------------\")\n",
    "    print(f\"---------------------------Testing LTI Outputs---------------------------\")\n",
    "    print(f\"-------------------------------------------------------------------------\")\n",
    "    print(f\"inputted jnp-data shape: {x_jnp.shape}\")\n",
    "    print(f\"inputted tensor-data shape: {x_tensor.shape}\")\n",
    "    print(f\"c_k shape: {c_k.shape}\")\n",
    "    print(f\"gu_c shape: {gu_c.shape}\")\n",
    "    \n",
    "    for i in range(c_k.shape[0]):\n",
    "        for j in range(c_k.shape[1]):\n",
    "            print(\n",
    "                f\"batch {i} on trajectory {j} compare : {jnp.allclose(c_k[i,j,:,:], gu_c[i,j,:,:], rtol=1e-03, atol=1e-03)}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hippo_legs_lti_zoh_operator(hippo_legs, gu_hippo_legs, random_input, legs_key):\n",
    "    x_tensor = torch.tensor(random_input, dtype=torch.float32)\n",
    "    x_jnp = jnp.asarray(x_tensor, dtype=jnp.float32)  # convert torch array to jax array\n",
    "    \n",
    "    # My Implementation\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    print(f\"----------------------------My LTI Implementation Outputs----------------------------\")\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    params = hippo_legs.init(legs_key, f=x_jnp)\n",
    "    c_k, y_k_list = hippo_legs.apply(params, f=x_jnp)\n",
    "    # jax.debug.print(f\"c_k: {c_k}\")\n",
    "    \n",
    "    # Gu's HiPPO LegS\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    print(f\"---------------------------Gu's LTI Implementation Outputs---------------------------\")\n",
    "    print(f\"-------------------------------------------------------------------------------------\")\n",
    "    x_tensor = torch.moveaxis(x_tensor, 0, 1)\n",
    "    GU_c_k = gu_hippo_legs(x_tensor, fast=False)\n",
    "    gu_c = jnp.asarray(GU_c_k, dtype=jnp.float32)  # convert torch array to jax array\n",
    "    gu_c = jnp.moveaxis(gu_c, 0, 1)\n",
    "    # jax.debug.print(f\"gu_c: {gu_c}\")\n",
    "    \n",
    "    print(f\"-------------------------------------------------------------------------\")\n",
    "    print(f\"---------------------------Testing LTI Outputs---------------------------\")\n",
    "    print(f\"-------------------------------------------------------------------------\")\n",
    "    print(f\"inputted jnp-data shape: {x_jnp.shape}\")\n",
    "    print(f\"inputted tensor-data shape: {x_tensor.shape}\")\n",
    "    print(f\"c_k shape: {c_k.shape}\")\n",
    "    print(f\"gu_c shape: {gu_c.shape}\")\n",
    "    \n",
    "    for i in range(c_k.shape[0]):\n",
    "        for j in range(c_k.shape[1]):\n",
    "            print(\n",
    "                f\"batch {i} on trajectory {j} compare : {jnp.allclose(c_k[i,j,:,:], gu_c[i,j,:,:], rtol=1e-03, atol=1e-03)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_16_input(key_generator, batch_size=16, data_size=784, input_size=28):\n",
    "    # x = jax.random.randint(key_generator, (batch_size, data_size), 0, 255)\n",
    "    x = jax.random.uniform(key_generator, (batch_size, data_size))\n",
    "    return jax.vmap(moving_window, in_axes=(0, None))(x, input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LSI_GBT(hippo, gu_hippo, A, B, random_input):\n",
    "    L = random_input.shape[1]\n",
    "    for i in range(1, L+1):\n",
    "        GBT_A, GBT_B = hippo.discretize(A, B, step=i, alpha=0.5, dtype=jnp.float32)\n",
    "        gu_GBT_A, gu_GBT_B = (\n",
    "            jnp.asarray(gu_hippo.A_stacked[i-1], dtype=jnp.float32),\n",
    "            jnp.expand_dims(jnp.asarray(gu_hippo.B_stacked[i-1], dtype=jnp.float32), axis=1),\n",
    "        )\n",
    "        \n",
    "        print(f\"GBT_A: {jnp.allclose(GBT_A, gu_GBT_A, rtol=1e-05, atol=1e-05)}\")\n",
    "        print(f\"GBT_B: {jnp.allclose(GBT_B, gu_GBT_B, rtol=1e-05, atol=1e-05)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LTI_GBT(hippo, gu_hippo, A, B, random_input):\n",
    "    L = random_input.shape[1]\n",
    "    GBT_A, GBT_B = hippo.discretize(A, B, step=1, alpha=0.5, dtype=jnp.float32)\n",
    "    gu_GBT_A, gu_GBT_B = (\n",
    "        jnp.asarray(gu_hippo.dA, dtype=jnp.float32),\n",
    "        jnp.expand_dims(jnp.asarray(gu_hippo.dB, dtype=jnp.float32), axis=1),\n",
    "    )\n",
    "    # print(f\"gu_GBT_A shape:{gu_GBT_A.shape}\\n\")\n",
    "    # print(f\"GBT_A shape: {GBT_A.shape}\\n\")\n",
    "    # print(f\"gu_GBT_B shape: {gu_GBT_B.shape}\\n\")\n",
    "    # print(f\"GBT_B shape: {GBT_B.shape}\")\n",
    "    \n",
    "    # print(f\"gu_GBT_A:\\n{gu_GBT_A}\\n\")\n",
    "    # print(f\"GBT_A:\\n{GBT_A}\\n\")\n",
    "    # print(f\"gu_GBT_B:\\n{gu_GBT_B}\\n\")\n",
    "    # print(f\"GBT_B:\\n{GBT_B}\")\n",
    "    \n",
    "    print(f\"GBT_A: {jnp.allclose(GBT_A, gu_GBT_A, rtol=1e-05, atol=1e-05)}\")\n",
    "    print(f\"GBT_B: {jnp.allclose(GBT_B, gu_GBT_B, rtol=1e-05, atol=1e-05)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hippos(matrices, gu_matrices):\n",
    "    A = matrices.A\n",
    "    B = matrices.B\n",
    "    gu_A = gu_matrices.A\n",
    "    gu_B = gu_matrices.B\n",
    "    \n",
    "    # print(f\"A shape: {A.shape}\")\n",
    "    # print(f\"B shape: {B.shape}\")\n",
    "    # print(f\"gu_A shape: {gu_A.shape}\")\n",
    "    # print(f\"gu_B shape: {gu_B.shape}\")\n",
    "    \n",
    "    # print(f\"gu_A:\\n{gu_A}\\n\")\n",
    "    # print(f\"A:\\n{A}\\n\")\n",
    "    # print(f\"gu_B:\\n{gu_B}\\n\")\n",
    "    # print(f\"B:\\n{B}\")\n",
    "    \n",
    "    print(f\"A: {jnp.allclose(A, gu_A, rtol=1e-05, atol=1e-05)}\")\n",
    "    print(f\"B: {jnp.allclose(B, gu_B, rtol=1e-05, atol=1e-05)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # N = 256\n",
    "    # L = 128\n",
    "    \n",
    "    batch_size = 2\n",
    "    data_size = 16\n",
    "    input_size = 1\n",
    "    \n",
    "    N = 32\n",
    "    L = data_size\n",
    "    \n",
    "    x_jnp = random_16_input(\n",
    "        key_generator=key3, \n",
    "        batch_size=batch_size, \n",
    "        data_size=data_size, \n",
    "        input_size=input_size\n",
    "    )\n",
    "    x_np = np.asarray(x_jnp)\n",
    "\n",
    "    x = torch.tensor(x_np, dtype=torch.float32)\n",
    "\n",
    "    print(f\"X shape: {x.shape}\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegT model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"\\nTesting HiPPO LegT model\")\n",
    "    # hippo_legt = HiPPO_LTI(N, dt=1.0 / L)\n",
    "\n",
    "    # c_k = hippo_legt(x)\n",
    "\n",
    "    # print(f\"Gu's Coeffiecients for LegT:\\n{c_k}\")\n",
    "    # print(f\"Gu's Coeffiecient shapes for LegT:\\n{c_k.shape}\")\n",
    "\n",
    "    # z = hippo_legt.reconstruct(c_k)\n",
    "    # print(f\"Gu's Reconstruction for LegT:\\n{z}\")\n",
    "    # print(f\"Gu's Reconstruction shape for LegT:\\n{z.shape}\")\n",
    "\n",
    "    # mse = loss(z[-1, 0, :L], x.squeeze(-1))\n",
    "    # print(f\"h-MSE shape:\\n{mse}\")\n",
    "    # print(f\"end of test for HiPPO LegT model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test HiPPO LegS model -----------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    print(\"\\nTesting HiPPO LegS model\")\n",
    "    gu_hippo_legs_lti_bi = HiPPO_LTI(\n",
    "        N=N,\n",
    "        method=\"legs\",\n",
    "        dt=1.0,\n",
    "        T=1.0,\n",
    "        discretization=0.5,\n",
    "        lambda_n=1.0,\n",
    "        alpha=0.0,\n",
    "        beta=1.0,\n",
    "        c=0.0,\n",
    "    )  # The Gu's\n",
    "    \n",
    "    gu_hippo_legs_lsi_bi = HiPPO_LSI(\n",
    "        N=N,\n",
    "        method=\"legs\",\n",
    "        max_length=L,\n",
    "        discretization=0.5,\n",
    "        lambda_n=1.0,\n",
    "        alpha=0.0,\n",
    "        beta=1.0\n",
    "    )  # The Gu's\n",
    "    \n",
    "    gu_hippo_legs_lti_zoh = HiPPO_LTI(\n",
    "        N=N,\n",
    "        method=\"legs\",\n",
    "        dt=1.0,\n",
    "        T=1.0,\n",
    "        discretization=\"zoh\",\n",
    "        lambda_n=1.0,\n",
    "        alpha=0.0,\n",
    "        beta=1.0,\n",
    "        c=0.0,\n",
    "    )  # The Gu's\n",
    "    \n",
    "    gu_hippo_legs_lsi_zoh = HiPPO_LSI(\n",
    "        N=N,\n",
    "        method=\"legs\",\n",
    "        max_length=L,\n",
    "        discretization=\"zoh\",\n",
    "        lambda_n=1.0,\n",
    "        alpha=0.0,\n",
    "        beta=1.0\n",
    "    )  # The Gu's\n",
    "    \n",
    "    # print(f\"gu_hippo_legs A_stacked: {gu_hippo_legs.A_stacked}\")\n",
    "    # c_k = gu_hippo_legs(x)\n",
    "\n",
    "    # print(f\"Gu's Coeffiecients  for LegS:\\n{c_k}\")\n",
    "    # print(f\"Gu's Coeffiecient shapes for LegS:\\n{c_k.shape}\")\n",
    "\n",
    "    # z = hippo_legs.reconstruct(c_k)\n",
    "\n",
    "    # print(f\"Gu's Reconstruction for LegS:\\n{z}\")\n",
    "    # print(f\"Gu's Reconstruction shape for LegS:\\n{z.shape}\")\n",
    "\n",
    "    # print(y-z)\n",
    "    print(f\"end of test for HiPPO LegS model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # ------------------------------ Test Generic HiPPO model --------------------------\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    the_measure = \"legs\"\n",
    "    print(f\"\\nTesting BRYANS HiPPO-{the_measure} model\")\n",
    "    matrices = TransMatrix(\n",
    "            N=N,\n",
    "            measure=\"legs\",\n",
    "            lambda_n=1.0,\n",
    "            alpha=0.0,\n",
    "            beta=1.0,\n",
    "            dtype=jnp.float32,\n",
    "        )\n",
    "    \n",
    "    gu_matrices = GuTransMatrix(\n",
    "            N=N,\n",
    "            measure=\"legs\",\n",
    "            lambda_n=1.0,\n",
    "            alpha=0.0,\n",
    "            beta=1.0,\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nTesting BRYANS HiPPO-{the_measure} model\")\n",
    "    test_hippos(matrices, gu_matrices)\n",
    "\n",
    "    A = matrices.A\n",
    "    B = matrices.B\n",
    "    \n",
    "    hippo_legs_lti_bi = HiPPO(\n",
    "        max_length=L,\n",
    "        step_size=1.0,\n",
    "        N=N,\n",
    "        lambda_n=1.0,\n",
    "        alpha=0.0,\n",
    "        beta=1.0,\n",
    "        GBT_alpha=0.5,\n",
    "        measure=\"legs\",\n",
    "        s_t=\"lti\",\n",
    "        dtype = jnp.float32,\n",
    "        verbose = True,\n",
    "    )  # Bryan's\n",
    "    \n",
    "    hippo_legs_lsi_bi = HiPPO(\n",
    "        max_length=L,\n",
    "        step_size=1.0,\n",
    "        N=N,\n",
    "        lambda_n=1.0,\n",
    "        alpha=0.0,\n",
    "        beta=1.0,\n",
    "        GBT_alpha=0.5,\n",
    "        measure=\"legs\",\n",
    "        s_t=\"lsi\",\n",
    "        dtype = jnp.float32,\n",
    "        verbose = True,\n",
    "    )  # Bryan's\n",
    "    \n",
    "    hippo_legs_lti_zoh = HiPPO(\n",
    "        max_length=L,\n",
    "        step_size=1.0,\n",
    "        N=N,\n",
    "        lambda_n=1.0,\n",
    "        alpha=0.0,\n",
    "        beta=1.0,\n",
    "        GBT_alpha=2.0,\n",
    "        measure=\"legs\",\n",
    "        s_t=\"lti\",\n",
    "        dtype = jnp.float32,\n",
    "        verbose = True,\n",
    "    )  # Bryan's\n",
    "    \n",
    "    hippo_legs_lsi_zoh = HiPPO(\n",
    "        max_length=L,\n",
    "        step_size=1.0,\n",
    "        N=N,\n",
    "        lambda_n=1.0,\n",
    "        alpha=0.0,\n",
    "        beta=1.0,\n",
    "        GBT_alpha=2.0,\n",
    "        measure=\"legs\",\n",
    "        s_t=\"lsi\",\n",
    "        dtype = jnp.float32,\n",
    "        verbose = True,\n",
    "    )  # Bryan's\n",
    "    \n",
    "    print(f\"Testing for correct LTI GBT matrices for HiPPO-{the_measure}\")\n",
    "    test_LTI_GBT(\n",
    "        hippo=hippo_legs_lti_bi, \n",
    "        gu_hippo=gu_hippo_legs_lti_bi, \n",
    "        A=A, \n",
    "        B=B, \n",
    "        random_input=x_np\n",
    "    )\n",
    "    \n",
    "    print(f\"Testing for correct LSI GBT matrices for HiPPO-{the_measure}\")\n",
    "    test_LSI_GBT(\n",
    "        hippo=hippo_legs_lsi_bi, \n",
    "        gu_hippo=gu_hippo_legs_lsi_bi, \n",
    "        A=A, \n",
    "        B=B, \n",
    "        random_input=x_np\n",
    "    )\n",
    "    \n",
    "    print(f\"Bryan's Coeffiecients for Bilinear LSI HiPPO-{the_measure}\")\n",
    "    \n",
    "    test_hippo_legs_lsi_bi_operator(\n",
    "        hippo_legs=hippo_legs_lsi_bi, \n",
    "        gu_hippo_legs=gu_hippo_legs_lsi_bi, \n",
    "        random_input=x_np, \n",
    "        legs_key=key2\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\\nBryan's Coeffiecients for Bilinear LTI HiPPO-{the_measure}\")\n",
    "    test_hippo_legs_lti_bi_operator(\n",
    "        hippo_legs=hippo_legs_lti_bi, \n",
    "        gu_hippo_legs=gu_hippo_legs_lti_bi, \n",
    "        random_input=x_np, \n",
    "        legs_key=key2\n",
    "    )\n",
    "    \n",
    "    print(f\"Bryan's Coeffiecients for ZOH LSI HiPPO-{the_measure}\")\n",
    "    \n",
    "    test_hippo_legs_lsi_zoh_operator(\n",
    "        hippo_legs=hippo_legs_lsi_zoh, \n",
    "        gu_hippo_legs=gu_hippo_legs_lsi_zoh, \n",
    "        random_input=x_np, \n",
    "        legs_key=key2\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\\nBryan's Coeffiecients for ZOH LTI HiPPO-{the_measure}\")\n",
    "    test_hippo_legs_lti_zoh_operator(\n",
    "        hippo_legs=hippo_legs_lti_zoh, \n",
    "        gu_hippo_legs=gu_hippo_legs_lti_zoh, \n",
    "        random_input=x_np, \n",
    "        legs_key=key2\n",
    "    )\n",
    "    \n",
    "    # y_legs = hippo_LegS_B.apply(\n",
    "    #     {\"params\": params}, c_k, method=hippo_LegS_B.reconstruct\n",
    "    # )\n",
    "\n",
    "    # print(f\"Bryan's Reconstruction for HiPPO-{the_measure}:\\n{y_legs}\")\n",
    "    # print(f\"Bryan's Reconstruction shape for HiPPO-{the_measure}:\\n{y_legs.shape}\")\n",
    "\n",
    "    print(f\"end of test for HiPPO-{the_measure} model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([2, 16, 1])\n",
      "\n",
      "Testing HiPPO LegT model\n",
      "\n",
      "Testing HiPPO LegS model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141413/1386819699.py:64: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  self.eval_matrix = torch.from_numpy(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "buffer source array is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test()\n",
      "Cell \u001b[0;32mIn[34], line 84\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m gu_hippo_legs_lsi_bi \u001b[39m=\u001b[39m HiPPO_LSI(\n\u001b[1;32m     63\u001b[0m     N\u001b[39m=\u001b[39mN,\n\u001b[1;32m     64\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlegs\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     beta\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m\n\u001b[1;32m     70\u001b[0m )  \u001b[39m# The Gu's\u001b[39;00m\n\u001b[1;32m     72\u001b[0m gu_hippo_legs_lti_zoh \u001b[39m=\u001b[39m HiPPO_LTI(\n\u001b[1;32m     73\u001b[0m     N\u001b[39m=\u001b[39mN,\n\u001b[1;32m     74\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlegs\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     c\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m,\n\u001b[1;32m     82\u001b[0m )  \u001b[39m# The Gu's\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m gu_hippo_legs_lsi_zoh \u001b[39m=\u001b[39m HiPPO_LSI(\n\u001b[1;32m     85\u001b[0m     N\u001b[39m=\u001b[39;49mN,\n\u001b[1;32m     86\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlegs\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     87\u001b[0m     max_length\u001b[39m=\u001b[39;49mL,\n\u001b[1;32m     88\u001b[0m     discretization\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mzoh\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     89\u001b[0m     lambda_n\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m     90\u001b[0m     alpha\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m     91\u001b[0m     beta\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m\n\u001b[1;32m     92\u001b[0m )  \u001b[39m# The Gu's\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m# print(f\"gu_hippo_legs A_stacked: {gu_hippo_legs.A_stacked}\")\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m# c_k = gu_hippo_legs(x)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m \u001b[39m# print(y-z)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend of test for HiPPO LegS model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 56\u001b[0m, in \u001b[0;36mHiPPO_LSI.__init__\u001b[0;34m(self, N, method, max_length, discretization, lambda_n, alpha, beta)\u001b[0m\n\u001b[1;32m     52\u001b[0m         B_stacked[t \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mlstsq(\n\u001b[1;32m     53\u001b[0m             np\u001b[39m.\u001b[39meye(N) \u001b[39m-\u001b[39m (At \u001b[39m*\u001b[39m alpha), Bt, rcond\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m         )[\u001b[39m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m     \u001b[39melse\u001b[39;00m:  \u001b[39m# ZOH\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m         A_stacked[t \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m la\u001b[39m.\u001b[39;49mexpm(A \u001b[39m*\u001b[39;49m (math\u001b[39m.\u001b[39;49mlog(t \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39m-\u001b[39;49m math\u001b[39m.\u001b[39;49mlog(t)))\n\u001b[1;32m     57\u001b[0m         B_stacked[t \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m la\u001b[39m.\u001b[39msolve_triangular(\n\u001b[1;32m     58\u001b[0m             A, A_stacked[t \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m@\u001b[39m B \u001b[39m-\u001b[39m B, lower\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     59\u001b[0m         )\n\u001b[1;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA_stacked \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(A_stacked\u001b[39m.\u001b[39mcopy())  \u001b[39m# (max_length, N, N)\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/s4mer-pkg-jZnBSgjq-py3.8/lib/python3.8/site-packages/scipy/linalg/_matfuncs.py:335\u001b[0m, in \u001b[0;36mexpm\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m product(\u001b[39m*\u001b[39m[\u001b[39mrange\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]]):\n\u001b[1;32m    333\u001b[0m     aw \u001b[39m=\u001b[39m a[ind]\n\u001b[0;32m--> 335\u001b[0m     lu \u001b[39m=\u001b[39m bandwidth(aw)\n\u001b[1;32m    336\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(lu):  \u001b[39m# a is diagonal?\u001b[39;00m\n\u001b[1;32m    337\u001b[0m         eA[ind] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdiag(np\u001b[39m.\u001b[39mexp(np\u001b[39m.\u001b[39mdiag(aw)))\n",
      "File \u001b[0;32m_cythonized_array_utils.pyx:104\u001b[0m, in \u001b[0;36mscipy.linalg._cythonized_array_utils.bandwidth\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_cythonized_array_utils.pyx:114\u001b[0m, in \u001b[0;36mscipy.linalg._cythonized_array_utils.bandwidth_c\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstringsource:660\u001b[0m, in \u001b[0;36mView.MemoryView.memoryview_cwrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstringsource:350\u001b[0m, in \u001b[0;36mView.MemoryView.memoryview.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: buffer source array is read-only"
     ]
    }
   ],
   "source": [
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('s4mer-pkg-jZnBSgjq-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a81e05d1d7f7eae781698b7c1b81c0d771335201ebad1d81045cb177cef974b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
