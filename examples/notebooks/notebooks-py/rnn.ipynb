{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.ops\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental.host_callback import id_print\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.linen.recurrent import RNNCellBase\n",
    "\n",
    "import optax\n",
    "\n",
    "import numpy as np  # convention: original numpy\n",
    "\n",
    "from typing import Any, Callable, Sequence, Optional, Tuple, Union\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import pprint\n",
    "\n",
    "from src.models.hippo.hippo import HiPPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 2\n",
    "rng, subkey = jax.random.split(key, num=num_copies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_batch(nest, batch_size: Optional[int]):\n",
    "    \"\"\"Adds a batch dimension at axis 0 to the leaves of a nested structure.\"\"\"\n",
    "    broadcast = lambda x: jnp.broadcast_to(x, (batch_size,) + x.shape)\n",
    "\n",
    "    return jax.tree_map(broadcast, nest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(RNNCellBase):\n",
    "    hidden_size: int\n",
    "\n",
    "    # @partial(\n",
    "    #     nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    # )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            W_xh = x_{t} @ W_{xh} - multiply the previous hidden state with\n",
    "            W_hh = H_{t-1} @ W_{hh} + b_{h} - this a linear layer\n",
    "\n",
    "            H_{t} = f_{w}(H_{t-1}, x)\n",
    "            H_{t} = tanh(H_{t-1} @ W_{hh}) + (x_{t} @ W_{xh})\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): hidden state size\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        ht_1, _ = carry\n",
    "        \n",
    "        print(f\"inside the rnn, input:\\n{input}\")\n",
    "\n",
    "        h_t = self.rnn_update(input, ht_1)\n",
    "\n",
    "        return (h_t, h_t), h_t\n",
    "\n",
    "    @partial(\n",
    "        nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    )\n",
    "    def rnn_update(self, input, ht_1):\n",
    "        print(f\"inside the rnn update, input:\\n{input}\")\n",
    "        print(f\"inside the rnn update, ht_1:\\n{ht_1}\")\n",
    "\n",
    "        W_hh = nn.Dense(self.hidden_size)(ht_1)\n",
    "        # id_print(W_hh, what=\"BLAH BLAH BLAH\", tap_with_device=True)\n",
    "        print(f\"W_hh:\\n{W_hh}\")\n",
    "        print(f\"input.shape:\\n{input.shape}\")\n",
    "        W_xh = nn.Dense(input.shape[0])(input)\n",
    "        h_t = nn.relu(W_hh + W_xh)  # H_{t} = tanh(H_{t-1} @ W_{hh}) + (x_{t} @ W_{xh})\n",
    "\n",
    "        return h_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(rng, batch_size, hidden_size, init_fn=nn.initializers.zeros):\n",
    "        \"\"\"Initialize the RNN cell carry.\n",
    "        Args:\n",
    "        rng: random number generator passed to the init_fn.\n",
    "        batch_dims: a tuple providing the shape of the batch dimensions.\n",
    "        hidden_size: the size or number of features of the memory.\n",
    "        init_fn: initializer function for the carry.\n",
    "        Returns:\n",
    "        An initialized carry for the given RNN cell.\n",
    "        \"\"\"\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(RNNCellBase):\n",
    "    hidden_size: int\n",
    "\n",
    "    @partial(\n",
    "        nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            i_{t} = sigmoid((W_{ii} @ x_{t} + b_{ii}) + (W_{hi} @ h_{t-1} + b_{hi}))\n",
    "            f_{t} = sigmoid((W_{if} @ x_{t} + b_{if}) + (W_{hf} @ h_{t-1} + b_{hf}))\n",
    "            g_{t} = tanh((W_{ig} @ x_{t} + b_{ig}) + (W_{hg} @ h_{t-1} + b_{hg}))\n",
    "            o_{t} = sigmoid((W_{io} @ x_{t} + b_{io}) + (W_{ho} @ h_{t-1} + b_{ho}))\n",
    "            c_{t} = f_{t} * c_{t-1} + i_{t} * g_{t}\n",
    "            h_{t} = o_{t} * tanh(c_{t})\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): hidden state size\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        print(f\"inside the LSTMCell, input:\\n{input}\")\n",
    "        print(f\"inside the LSTMCell, input type:\\n{type(input)}\")\n",
    "        \n",
    "        print(f\"inside the LSTMCell, carry:\\n{carry}\")\n",
    "        print(f\"inside the LSTMCell, carry type:\\n{type(carry)}\")\n",
    "        ht_1, ct_1 = carry\n",
    "        print(f\"carry split:\\n{ht_1}\\n{ct_1}\")\n",
    "\n",
    "        c_t, h_t = self.rnn_update(input, ht_1, ct_1)\n",
    "        print(f\"inside the LSTMCell, c_t:\\n{c_t}\")\n",
    "        print(f\"inside the LSTMCell, h_t:\\n{h_t}\")\n",
    "\n",
    "        return (h_t, c_t), h_t\n",
    "\n",
    "    # @partial(\n",
    "    #     nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    # )\n",
    "    def rnn_update(self, input, ht_1, ct_1):\n",
    "        print(f\"inside the LSTMCell rnn_update, input:\\n{input}\")\n",
    "        print(f\"inside the LSTMCell rnn_update, ht_1:\\n{ht_1}\")\n",
    "        print(f\"inside the LSTMCell rnn_update, ct_1:\\n{ct_1}\")\n",
    "        # i_ta = partial(\n",
    "        #     nn.Dense,\n",
    "        #     features=ht_1.shape()[0],\n",
    "        #     use_bias=False,\n",
    "        #     kernel_init=self.recurrent_kernel_init,\n",
    "        #     bias_init=self.bias_init,\n",
    "        # )\n",
    "\n",
    "        i_ta = nn.Dense(features=ht_1.shape()[0])(input)\n",
    "        i_tb = nn.Dense(features=self.hidden_size)(ht_1)\n",
    "        i_t = nn.sigmoid(i_ta + i_tb)  # input gate\n",
    "        print(f\"inside the LSTMCell, input gate output:\\n{i_t}\")\n",
    "\n",
    "        o_ta = nn.Dense(self.hidden_size)(input)\n",
    "        o_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        o_t = nn.sigmoid(o_ta + o_tb)  # output gate\n",
    "        print(f\"inside the LSTMCell, output gate output:\\n{o_t}\")\n",
    "\n",
    "        f_ia = nn.Dense(self.hidden_size)(\n",
    "            input\n",
    "        )  # b^{f}_{i} + \\sum\\limits_{j} U^{f}_{i, j} x^{t}_{j}\n",
    "        f_ib = nn.Dense(self.hidden_size)(\n",
    "            ht_1\n",
    "        )  # \\sum\\limits_{j} W^{f}_{i, j} h^{(t-1)}_{j}\n",
    "        f_i = nn.sigmoid(f_ia + f_ib)  # forget gate\n",
    "        print(f\"inside the LSTMCell, forget gate output:\\n{f_i}\")\n",
    "\n",
    "        g_ia = nn.Dense(self.hidden_size)(\n",
    "            input\n",
    "        )  # b^{g}_{i} + \\sum\\limits_{j} U^{g}_{i, j} x^{t}_{j}\n",
    "        g_ib = nn.Dense(self.hidden_size)(\n",
    "            ht_1\n",
    "        )  # \\sum\\limits_{j} W^{g}_{i, j} h^{(t-1)}_{j}\n",
    "        g_i = nn.tanh(g_ia + g_ib)  # (external) input gate\n",
    "        print(f\"inside the LSTMCell, (external) input gate output:\\n{g_i}\")\n",
    "\n",
    "        c_t = (f_i * ct_1) + (i_t * g_i)  # internal cell state update\n",
    "        print(f\"inside the LSTMCell, cell state output:\\n{c_t}\")\n",
    "\n",
    "        h_t = o_t * nn.tanh(c_t)  # hidden state update\n",
    "        print(f\"inside the LSTMCell, hidden state output:\\n{h_t}\")\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(rng, batch_size, hidden_size, init_fn=nn.initializers.zeros):\n",
    "        \"\"\"Initialize the RNN cell carry.\n",
    "        Args:\n",
    "        rng: random number generator passed to the init_fn.\n",
    "        batch_dims: a tuple providing the shape of the batch dimensions.\n",
    "        hidden_size: the size or number of features of the memory.\n",
    "        init_fn: initializer function for the carry.\n",
    "        Returns:\n",
    "        An initialized carry for the given RNN cell.\n",
    "        \"\"\"\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(RNNCellBase):\n",
    "    hidden_size: int\n",
    "\n",
    "    @partial(\n",
    "        nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            z_t = sigmoid((W_{iz} @ x_{t} + b_{iz}) + (W_{hz} @ h_{t-1} + b_{hz}))\n",
    "            r_t = sigmoid((W_{ir} @ x_{t} + b_{ir}) + (W_{hr} @ h_{t-1} + b_{hr}))\n",
    "            g_t = tanh(((W_{ig} @ x_{t} + b_{ig}) + r_t) * (W_{hg} @ h_{t-1} + b_{hg}))\n",
    "            h_t = (z_t * h_{t-1}) + ((1 - z_t) * g_i)\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): hidden state size\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        ht_1 = carry\n",
    "\n",
    "        h_t = self.rnn_update(input, ht_1)\n",
    "\n",
    "        return (h_t, h_t), h_t\n",
    "\n",
    "    # @partial(\n",
    "    #     nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    # )\n",
    "    def rnn_update(self, input, ht_1):\n",
    "\n",
    "        z_ta = nn.Dense(self.hidden_size)(input)\n",
    "        z_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        z_t = nn.sigmoid(z_ta + z_tb)  # reset gate\n",
    "\n",
    "        r_ta = nn.Dense(self.hidden_size)(input)\n",
    "        r_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        r_t = nn.sigmoid(r_ta + r_tb)  # update gate\n",
    "\n",
    "        g_ta = nn.Dense(self.hidden_size)(input)\n",
    "        g_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        g_t = nn.tanh((g_ta + r_t) * g_tb)  # (external) input gate\n",
    "\n",
    "        h_t = ((1 - z_t) * ht_1) + (z_t * g_t)  # internal cell state update\n",
    "\n",
    "        return h_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(rng, batch_size, hidden_size, init_fn=nn.initializers.zeros):\n",
    "        \"\"\"Initialize the RNN cell carry.\n",
    "        Args:\n",
    "        rng: random number generator passed to the init_fn.\n",
    "        batch_dims: a tuple providing the shape of the batch dimensions.\n",
    "        hidden_size: the size or number of features of the memory.\n",
    "        init_fn: initializer function for the carry.\n",
    "        Returns:\n",
    "        An initialized carry for the given RNN cell.\n",
    "        \"\"\"\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPOCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        RNN update function\n",
    "        τ(h, x) = (1 - g(h, x)) ◦ h + g(h, x) ◦ tanh(Lτ (h, x))\n",
    "        g(h, x) = σ(Lg(h,x))\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): hidden state size\n",
    "        output_size (int): output size\n",
    "        hippo (HiPPO): hippo model object\n",
    "        cell (RNNCellBase): choice of RNN cell object\n",
    "            - RNNCell\n",
    "            - LSTMCell\n",
    "            - GRUCell\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "    hippo: HiPPO\n",
    "    model: RNNCellBase\n",
    "\n",
    "    def setup(self):\n",
    "        self.cell = self.model(self.hidden_size)\n",
    "\n",
    "    # @partial(\n",
    "    #     nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    # )\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            RNN update function\n",
    "            τ(h, x) = (1 - g(h, x)) ◦ h + g(h, x) ◦ tanh(Lτ (h, x))\n",
    "            g(h, x) = σ(Lg(h,x))\n",
    "\n",
    "        Args:\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"inside hippo cell, input:\\n{input}\")\n",
    "        print(f\"inside hippo cell, carry:\\n{carry}\")\n",
    "        print(f\"inside hippo cell, the cell:\\n{self.cell}\")\n",
    "        _, h_t = self.cell(carry, input)\n",
    "        print(f\"inside hippo cell, h_t:\\n{h_t}\")\n",
    "\n",
    "        # y_t = nn.Dense(self.output_size)(h_t)  # f_t in the paper\n",
    "        # print(f\"inside hippo cell, y_t: \\n{y_t}\")\n",
    "        \n",
    "        # c_t = self.hippo(y_t, init_state=None, kernel=False)\n",
    "        # print(f\"inside hippo cell, c_t: \\n{c_t}\")\n",
    "\n",
    "        return self.rnn_update(input, h_t)\n",
    "    \n",
    "    @partial(\n",
    "        nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    )\n",
    "    def rnn_update(self, input, h_t):\n",
    "\n",
    "        y_t = nn.Dense(self.output_size)(h_t)  # f_t in the paper\n",
    "        print(f\"inside hippo cell, y_t: \\n{y_t}\")\n",
    "        \n",
    "        c_t = self.hippo(y_t, init_state=None, kernel=False)\n",
    "        print(f\"inside hippo cell, c_t: \\n{c_t}\")\n",
    "\n",
    "        return (h_t, c_t), h_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(rng, batch_size, hidden_size, init_fn=nn.initializers.zeros):\n",
    "        \"\"\"Initialize the RNN cell carry.\n",
    "        Args:\n",
    "        rng: random number generator passed to the init_fn.\n",
    "        batch_dims: a tuple providing the shape of the batch dimensions.\n",
    "        hidden_size: the size or number of features of the memory.\n",
    "        init_fn: initializer function for the carry.\n",
    "        Returns:\n",
    "        An initialized carry for the given RNN cell.\n",
    "        \"\"\"\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refer to https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/recurrent.py#L714-L762\n",
    "# also refer to https://dm-haiku.readthedocs.io/en/latest/api.html?highlight=DeepRNN#deeprnn\n",
    "class _DeepRNN(RNNCellBase):\n",
    "    hidden_size: int\n",
    "    layers: Sequence[Any]\n",
    "    skip_connections: bool\n",
    "    hidden_to_output_layer: bool\n",
    "    layer_name: Optional[str]\n",
    "\n",
    "    def setup(self):\n",
    "        if self.skip_connections:\n",
    "            for layer in self.layers:\n",
    "                if not (isinstance(layer, RNNCellBase) or isinstance(layer, HiPPOCell)):\n",
    "                    raise ValueError(\n",
    "                        \"skip_connections requires for all layers to be \"\n",
    "                        \"`hk.RNNCore`s. Layers is: {}\".format(self.layers)\n",
    "                    )\n",
    "                    # raise ValueError(\n",
    "                    #     f\"{self.layer_name} layer {layer} is not a RNNCellBase or HiPPOCell\"\n",
    "                    # )\n",
    "\n",
    "    def __call__(self, carry, inputs):\n",
    "        # carry = jnp.concatenate(carry, axis=1)\n",
    "        # print(f\"inside deep rnn, after concat, carry:\\n{carry}\")\n",
    "        current_carry = carry\n",
    "        next_states = []\n",
    "        h_t_outputs = []\n",
    "        c_t_outputs = []\n",
    "        state_idx = 0\n",
    "        print(f\"inside deep rnn, inputs:\\n{inputs}\")\n",
    "        print(f\"inside deep rnn, carry:\\n{carry}\")\n",
    "        h_t, c_t = carry  # c_t may actually be h_t in which case dont use it\n",
    "        (\n",
    "            h_t_copy,\n",
    "            c_t_copy,\n",
    "        ) = current_carry  # c_t may actually be h_t in which case dont use it\n",
    "        concat = lambda *args: jnp.concatenate(args, axis=-1)\n",
    "        print(f\"before main loop\")\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if self.skip_connections and idx > 0:\n",
    "                skip_h_t = jax.tree_map(concat, h_t, h_t_copy)\n",
    "                skip_c_t = jax.tree_map(concat, c_t, c_t_copy)\n",
    "                current_carry = tuple([skip_h_t, skip_c_t])\n",
    "\n",
    "            if isinstance(layer, RNNCellBase) or isinstance(layer, HiPPOCell):\n",
    "                print(f\"inside deep rnn, inputs:\\n{inputs}\")\n",
    "                print(f\"inside deep rnn, state_idx:\\n{state_idx}\")\n",
    "                print(f\"inside deep rnn, inputs[state_idx]:\\n{inputs[state_idx]}\")\n",
    "                h_t, c_t, next_state = layer(\n",
    "                    current_carry, inputs[state_idx]\n",
    "                )  # problem line\n",
    "                h_t_outputs.append(h_t)\n",
    "                c_t_outputs.append(c_t)\n",
    "                next_states.append(next_state)\n",
    "                state_idx += 1\n",
    "\n",
    "            else:\n",
    "                print(f\"current_carry before layer: {current_carry}\")\n",
    "                print(f\"layer: {layer}\")\n",
    "                current_carry = layer(current_carry)\n",
    "                print(f\"current_carry:\\n {current_carry}\")\n",
    "\n",
    "        print(f\"third conditional\")\n",
    "        if self.skip_connections:\n",
    "            skip_h_t_out = jax.tree_map(concat, *h_t_outputs)\n",
    "            skip_c_t_out = jax.tree_map(concat, *c_t_outputs)\n",
    "            next_carry = (skip_h_t_out, skip_c_t_out)\n",
    "        else:\n",
    "            next_carry = current_carry\n",
    "\n",
    "        print(f\"next_states before tuple:\\n\", next_states)\n",
    "        pp.pprint(layer)\n",
    "        print(f\"carry before return B:\\n\", next_carry)\n",
    "\n",
    "        return next_carry, next_states\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_state(\n",
    "        num_layers,\n",
    "        rng,\n",
    "        batch_size: tuple,\n",
    "        output_size: int,\n",
    "        init_fn=nn.initializers.zeros,\n",
    "    ):\n",
    "        states = []\n",
    "        for i in range(num_layers):\n",
    "            print(f\"Layer: {i}\\n\")\n",
    "            states.append(\n",
    "                _DeepRNN.init_state(\n",
    "                    rng=rng,\n",
    "                    batch_size=batch_size,\n",
    "                    output_size=output_size,\n",
    "                    init_fn=init_fn,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return states\n",
    "\n",
    "    @staticmethod\n",
    "    def init_state(\n",
    "        rng, batch_size: tuple, output_size: int, init_fn=nn.initializers.zeros\n",
    "    ):\n",
    "        print(f\"batch_size: {(batch_size,)}\")\n",
    "        print(f\"output_size: {(output_size,)}\")\n",
    "        mem_shape = (batch_size,) + (output_size,)\n",
    "        print(f\"state mem_shape: {mem_shape}\")\n",
    "\n",
    "        return init_fn(rng, mem_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(\n",
    "        rng, batch_size: tuple, hidden_size: int, init_fn=nn.initializers.zeros\n",
    "    ):\n",
    "        print(f\"batch_size: {batch_size}\")\n",
    "        print(f\"hidden_size: {(hidden_size,)}\")\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "        print(f\"carry mem_shape: {mem_shape}\")\n",
    "\n",
    "        return init_fn(rng, mem_shape), init_fn(rng, mem_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRNN(_DeepRNN):\n",
    "    r\"\"\"Wraps a sequence of cores and callables as a single core.\n",
    "        >>> deep_rnn = hk.DeepRNN([\n",
    "        ...     LSTMCell(hidden_size=4),\n",
    "        ...     jax.nn.relu,\n",
    "        ...     LSTMCell(hidden_size=2),\n",
    "        ... ])\n",
    "    The state of a :class:`DeepRNN` is a tuple with one element per\n",
    "    :class:`RNNCore`. If no layers are :class:`RNNCore`\\ s, the state is an empty\n",
    "    tuple.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        layers: Sequence[Any],\n",
    "        skip_connections: Optional[bool] = False,\n",
    "        hidden_to_output_layer: Optional[bool] = False,\n",
    "        name: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            hidden_size=hidden_size,\n",
    "            layers=layers,\n",
    "            skip_connections=skip_connections,\n",
    "            hidden_to_output_layer=hidden_to_output_layer,\n",
    "            layer_name=name,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Training Types\n",
    "refer to [this](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data_2_id(iterable):\n",
    "    \"\"\"\n",
    "    provides mapping to and from ids\n",
    "    \"\"\"\n",
    "    \n",
    "    id_2_data = {}\n",
    "    data_2_id = {}\n",
    "\n",
    "    for id, elem in enumerate(iterable):\n",
    "        id_2_data[id] = elem\n",
    "\n",
    "    for id, elem in enumerate(iterable):\n",
    "        data_2_id[elem] = id\n",
    "\n",
    "    return (id_2_data, data_2_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(i, n):\n",
    "    \"\"\"\n",
    "    create vector of size n with 1 at index i\n",
    "    \"\"\"\n",
    "    x = jnp.zeros(n, dtype=int)\n",
    "    print(f\"i:\\n{i}\")\n",
    "    print(f\"type(i):\\n{type(i)}\")\n",
    "    print(f\"n:\\n{n}\")\n",
    "    print(f\"type(n):\\n{type(n)}\")\n",
    "    x = x.at[i].set(1) #jax.vmap(x.at[i].set(1), in_axes=(0, 0), out_axes=0)(i, x)\n",
    "\n",
    "    print(f\"i:\\n{i}\")\n",
    "    print(f\"type(x):\\n{type(x)}\")\n",
    "    print(f\"x:\\n{x}\")\n",
    "    # array = x.at[i].set(1)\n",
    "    # print(f\"array:\\n{array}\")\n",
    "    # x = x[i].at[i].set(1)\n",
    "    # print(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(fname):\n",
    "  with open(fname, \"r\") as reader:\n",
    "    data = reader.read()\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "  chars = list(set(data))\n",
    "  vocab_size = len(chars)\n",
    "  char_to_id, id_to_char = map_data_2_id(chars)\n",
    "  char_to_id = {value:key for key, value in char_to_id.items()}\n",
    "  # data converted to ids\n",
    "  # data_id = [char_to_id[char] for char in data]\n",
    "  data_id = [char_to_id[char] for char in data]\n",
    "  return data_id, char_to_id, id_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 3, 2, 2, 2, 0, 1, 4]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"abcd...abcd...\"\n",
    "data_id, char_to_id, id_to_char = prep_data(data)\n",
    "data_id[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(char):\n",
    "    return one_hot(data_2_id[char], len(data_2_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(predictions, id_2_data):\n",
    "    return id_2_data[int(jnp.argmax(predictions))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check [this](https://github.com/deepmind/optax/blob/master/examples/quick_start.ipynb) out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_optimizer_fn(starting_learning_rate, name=\"adam\"):\n",
    "    # refer to https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules\n",
    "    optim = None\n",
    "\n",
    "    if name == \"sgd\":\n",
    "        optim = optax.sgd(starting_learning_rate)\n",
    "    elif name == \"adam\":\n",
    "        optim = optax.adam(\n",
    "            starting_learning_rate,\n",
    "        )\n",
    "    elif name == \"adagrad\":\n",
    "        optim = optax.adagrad(starting_learning_rate)\n",
    "    elif name == \"rmsprop\":\n",
    "        optim = optax.rmsprop(starting_learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"optimizer name not recognized\")\n",
    "\n",
    "    return optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_scheduler_fn(\n",
    "    start_learning_rate, steps, decay_rate, init_value, end_val, name\n",
    "):\n",
    "    # refer to https://optax.readthedocs.io/en/latest/api.html#schedules\n",
    "    scheduler = None\n",
    "\n",
    "    if name == \"constant\":\n",
    "        scheduler = optax.constant_schedule(init_value)\n",
    "\n",
    "    elif name == \"exp_decay\":\n",
    "        scheduler = optax.exponential_decay(\n",
    "            init_value=start_learning_rate, transition_steps=1000, decay_rate=0.99\n",
    "        )\n",
    "    elif name == \"linear\":\n",
    "        scheduler = optax.linear_schedule(init_value=init_value, end_value=end_val)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"scheduler name not recognized\")\n",
    "\n",
    "    return scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add transformations\n",
    "# refer to https://optax.readthedocs.io/en/latest/api.html#optax-transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # A simple update loop.\n",
    "#     for _ in range(1000):\n",
    "#     grads = jax.grad(compute_loss)(params, xs, ys)\n",
    "#     updates, opt_state = gradient_transform.update(grads, opt_state)\n",
    "#     params = optax.apply_updates(params, updates)\n",
    "\n",
    "#     assert jnp.allclose(params, target_params), \\\n",
    "#     'Optimization should retrieve the target params used to generate the data.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimizer_fn(start_learning_rate, params, num_weights, x, y):\n",
    "#     optimizer = optax.adam(start_learning_rate)\n",
    "#     # Obtain the `opt_state` that contains statistics for the optimizer.\n",
    "#     params = {'w': jnp.ones((num_weights,))}\n",
    "#     opt_state = optimizer.init(params)\n",
    "\n",
    "#     compute_loss = lambda params, x, y: optax.l2_loss(params['w'].dot(x), y)\n",
    "#     grads = jax.grad(compute_loss)(params, xs, ys)\n",
    "\n",
    "#     updates, opt_state = optimizer.update(grads, opt_state)\n",
    "#     params = optax.apply_updates(params, updates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    state_size: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int\n",
    "    hippo_order_N: int\n",
    "    batch_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        L = self.vocab_size\n",
    "\n",
    "        hippo = HiPPO(\n",
    "            N=self.hippo_order_N,\n",
    "            max_length=L,\n",
    "            measure=\"legs\",\n",
    "            step=1.0 / L,\n",
    "            GBT_alpha=0.5,\n",
    "            seq_L=L,\n",
    "            v=\"v\",\n",
    "            lambda_n=1.0,\n",
    "            fourier_type=\"fru\",\n",
    "            alpha=0.0,\n",
    "            beta=1.0,\n",
    "        )\n",
    "\n",
    "        cell1 = RNNCell\n",
    "        cell2 = RNNCell\n",
    "        cell3 = RNNCell\n",
    "\n",
    "        input_layers = [\n",
    "            HiPPOCell(\n",
    "                hippo=hippo, model=cell1, hidden_size=8, output_size=self.state_size\n",
    "            ),\n",
    "            HiPPOCell(\n",
    "                hippo=hippo, model=cell2, hidden_size=64, output_size=self.state_size\n",
    "            ),\n",
    "            HiPPOCell(\n",
    "                hippo=hippo, model=cell3, hidden_size=512, output_size=self.state_size\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self.deep_cell = DeepRNN(\n",
    "            hidden_size=self.hidden_size,\n",
    "            layers=input_layers,\n",
    "            skip_connections=True,\n",
    "            hidden_to_output_layer=False,\n",
    "            name=\"RNNLM\",\n",
    "        )\n",
    "\n",
    "    @partial(\n",
    "        nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    )\n",
    "    def __call__(self, carry, input):\n",
    "        print(f\"i before one_hot:\\n{input}\")\n",
    "        onehot = lambda x: nn.one_hot(x, self.vocab_size)\n",
    "        # leaves, treedef = jax.tree_util.tree_flatten(input)\n",
    "        # print(f\"inside call function of RNNLM, leaves:\\n{leaves}\")\n",
    "        \n",
    "        input = jax.tree_map(onehot, input)\n",
    "        \n",
    "        # input = one_hot(input, self.vocab_size)\n",
    "        print(f\"inside call function of RNNLM, input:\\n{input}\")\n",
    "        print(f\"inside call function of RNNLM, carry:\\n{carry}\")\n",
    "        carries, next_states = self.deep_cell(carry, input)\n",
    "        print(f\"inside call function of RNNLM, next_states:\\n{next_states}\")\n",
    "        print(f\"inside call function of RNNLM, carries:\\n{carries}\")\n",
    "        predictions = nn.softmax(nn.Dense(self.vocab_size)(next_states[-1]))\n",
    "        print(f\"inside call function of RNNLM, predictions:\\n{predictions}\")\n",
    "        return carries, next_states, predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(\n",
    "        rng, batch_size: tuple, hidden_size: int, init_fn=nn.initializers.zeros\n",
    "    ):\n",
    "        return DeepRNN.initialize_carry(\n",
    "            rng=rng,\n",
    "            batch_size=batch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            init_fn=init_fn,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_state(\n",
    "        num_layers,\n",
    "        rng,\n",
    "        batch_size: tuple,\n",
    "        output_size: int,\n",
    "        init_fn=nn.initializers.zeros,\n",
    "    ):\n",
    "        return DeepRNN.initialize_state(\n",
    "            num_layers=num_layers,\n",
    "            rng=rng,\n",
    "            batch_size=batch_size,\n",
    "            output_size=output_size,\n",
    "            init_fn=init_fn,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, params, bridge, initial=\"\", max_length=100):\n",
    "    char_to_id, id_to_char = bridge\n",
    "    state = model.init_state()\n",
    "    output = initial\n",
    "    if len(initial) > 0:\n",
    "        for char in initial[:-1]:\n",
    "            _, state, _ = model.apply(params, char_to_id[char], state)\n",
    "\n",
    "    next_char = initial[-1]\n",
    "    for i in range(max_length):\n",
    "        state, predictions = model.apply(params, state, char_to_id[next_char], state)\n",
    "        next_char = decode(predictions, id_to_char)\n",
    "        output += next_char\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to [this](https://github.com/manifest/flax-extra/blob/48efe1f1515893289b44646977bf5049a340b6c8/docs/notebooks/combinators.ipynb), [this](https://github.com/romanak/pyprobml/blob/65c82b9b43d2100cbc7c59e766161ee801c0f85f/notebooks/book1/15/rnn_jax.ipynb), [this](https://github.com/probml/pyprobml/blob/71d98dcdd3798525353eb1bfb9851b47e9d64bde/notebooks/book1/15/rnn_jax.ipynb) and [this](https://github.com/probml/probml-notebooks/blob/36cb173afce3f4a07a7b475cf8a7937025a60465/notebooks-d2l/rnn_jax.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Model: RNNLM\n",
      "batch_size: (10,)\n",
      "hidden_size: (4,)\n",
      "carry mem_shape: (10, 4)\n",
      "carry init:\n",
      "(DeviceArray([[0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.]], dtype=float32), DeviceArray([[0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0.]], dtype=float32))\n",
      "Layer: 0\n",
      "\n",
      "batch_size: (10,)\n",
      "output_size: (5,)\n",
      "state mem_shape: (10, 5)\n",
      "Layer: 1\n",
      "\n",
      "batch_size: (10,)\n",
      "output_size: (5,)\n",
      "state mem_shape: (10, 5)\n",
      "Layer: 2\n",
      "\n",
      "batch_size: (10,)\n",
      "output_size: (5,)\n",
      "state mem_shape: (10, 5)\n",
      "state init:\n",
      "[DeviceArray([[0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.]], dtype=float32), DeviceArray([[0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.]], dtype=float32), DeviceArray([[0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.]], dtype=float32)]\n",
      "Init Model: RNNLM\n",
      "batch_size: (10,)\n",
      "hidden_size: (4,)\n",
      "carry mem_shape: (10, 4)\n",
      "Layer: 0\n",
      "\n",
      "batch_size: (10,)\n",
      "output_size: (1,)\n",
      "state mem_shape: (10, 1)\n",
      "Layer: 1\n",
      "\n",
      "batch_size: (10,)\n",
      "output_size: (1,)\n",
      "state mem_shape: (10, 1)\n",
      "Layer: 2\n",
      "\n",
      "batch_size: (10,)\n",
      "output_size: (1,)\n",
      "state mem_shape: (10, 1)\n",
      "i before one_hot:\n",
      "[Traced<ShapedArray(float32[1]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[1]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[1]):JaxprTrace(level=1/0)>]\n",
      "inside call function of RNNLM, input:\n",
      "[Traced<ShapedArray(float32[1,5]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[1,5]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[1,5]):JaxprTrace(level=1/0)>]\n",
      "inside call function of RNNLM, carry:\n",
      "(Traced<ShapedArray(float32[10,4]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[10,4]):JaxprTrace(level=1/0)>)\n",
      "inside deep rnn, inputs:\n",
      "[Traced<ShapedArray(float32[1,5]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[1,5]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[1,5]):JaxprTrace(level=1/0)>]\n",
      "inside deep rnn, carry:\n",
      "(Traced<ShapedArray(float32[10,4]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[10,4]):JaxprTrace(level=1/0)>)\n",
      "before main loop\n",
      "inside deep rnn, inputs:\n",
      "[Traced<ShapedArray(float32[1,5]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[1,5]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[1,5]):JaxprTrace(level=1/0)>]\n",
      "inside deep rnn, state_idx:\n",
      "0\n",
      "inside deep rnn, inputs[state_idx]:\n",
      "Traced<ShapedArray(float32[1,5]):JaxprTrace(level=1/0)>\n",
      "inside hippo cell, input:\n",
      "Traced<ShapedArray(float32[1,5]):JaxprTrace(level=1/0)>\n",
      "inside hippo cell, carry:\n",
      "(Traced<ShapedArray(float32[10,4]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[10,4]):JaxprTrace(level=1/0)>)\n",
      "inside hippo cell, the cell:\n",
      "RNNCell(\n",
      "    # attributes\n",
      "    hidden_size = 8\n",
      ")\n",
      "inside the rnn, input:\n",
      "Traced<ShapedArray(float32[1,5]):JaxprTrace(level=1/0)>\n",
      "inside the rnn update, input:\n",
      "Traced<ShapedArray(float32[1,5]):JaxprTrace(level=2/0)>\n",
      "inside the rnn update, ht_1:\n",
      "Traced<ShapedArray(float32[4]):JaxprTrace(level=2/0)>\n",
      "W_hh:\n",
      "Traced<ShapedArray(float32[8]):JaxprTrace(level=2/0)>\n",
      "input.shape:\n",
      "(1, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=23'>24</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstate init:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39minitialize_state(num_layers\u001b[39m=\u001b[39mnum_layers,rng\u001b[39m=\u001b[39mrng,batch_size\u001b[39m=\u001b[39mbatch_size,output_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(char_to_id), init_fn\u001b[39m=\u001b[39mnn\u001b[39m.\u001b[39minitializers\u001b[39m.\u001b[39mzeros,)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=24'>25</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInit Model: RNNLM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=27'>28</a>\u001b[0m params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49minit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=28'>29</a>\u001b[0m     subkey,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=29'>30</a>\u001b[0m     model\u001b[39m.\u001b[39;49minitialize_carry(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=30'>31</a>\u001b[0m         rng\u001b[39m=\u001b[39;49mkey,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=31'>32</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49m(batch_size,),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=32'>33</a>\u001b[0m         hidden_size\u001b[39m=\u001b[39;49mhidden_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=33'>34</a>\u001b[0m         init_fn\u001b[39m=\u001b[39;49mnn\u001b[39m.\u001b[39;49minitializers\u001b[39m.\u001b[39;49mzeros,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=34'>35</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=35'>36</a>\u001b[0m     model\u001b[39m.\u001b[39;49minitialize_state(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=36'>37</a>\u001b[0m         num_layers\u001b[39m=\u001b[39;49mnum_layers,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=37'>38</a>\u001b[0m         rng\u001b[39m=\u001b[39;49msubkey,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=38'>39</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=39'>40</a>\u001b[0m         output_size\u001b[39m=\u001b[39;49moutput_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=40'>41</a>\u001b[0m         init_fn\u001b[39m=\u001b[39;49mnn\u001b[39m.\u001b[39;49minitializers\u001b[39m.\u001b[39;49mzeros,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=41'>42</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=42'>43</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel state size: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mstate_size\u001b[39m}\u001b[39;00m\u001b[39m, vocab size: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mvocab_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=45'>46</a>\u001b[0m \u001b[39m# output: Model state size: 8, vocab size: 5\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=46'>47</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=47'>48</a>\u001b[0m \u001b[39m# run a single example through the model to test that it works\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/s4mer-rmt3vFtN-py3.9/lib/python3.9/site-packages/flax/core/axes_scan.py:138\u001b[0m, in \u001b[0;36mscan.<locals>.scan_fn\u001b[0;34m(broadcast_in, init, *args)\u001b[0m\n\u001b[1;32m    135\u001b[0m f_flat, out_tree \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mapi_util\u001b[39m.\u001b[39mflatten_fun_nokwargs(\n\u001b[1;32m    136\u001b[0m     lu\u001b[39m.\u001b[39mwrap_init(broadcast_body), in_tree)\n\u001b[1;32m    137\u001b[0m in_pvals \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(pe\u001b[39m.\u001b[39mPartialVal\u001b[39m.\u001b[39munknown, in_avals))\n\u001b[0;32m--> 138\u001b[0m _, out_pvals, _ \u001b[39m=\u001b[39m pe\u001b[39m.\u001b[39;49mtrace_to_jaxpr_nounits(f_flat, in_pvals)\n\u001b[1;32m    140\u001b[0m out_flat \u001b[39m=\u001b[39m []\n\u001b[1;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m pv, const \u001b[39min\u001b[39;00m out_pvals:\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/s4mer-rmt3vFtN-py3.9/lib/python3.9/site-packages/flax/core/axes_scan.py:114\u001b[0m, in \u001b[0;36mscan.<locals>.scan_fn.<locals>.body_fn\u001b[0;34m(c, xs, init_mode)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbody_fn\u001b[39m(c, xs, init_mode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m   \u001b[39m# inject constants\u001b[39;00m\n\u001b[1;32m    112\u001b[0m   xs \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mtree_map(\u001b[39mlambda\u001b[39;00m ax, arg, x: (arg \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m broadcast \u001b[39melse\u001b[39;00m x),\n\u001b[1;32m    113\u001b[0m                          in_axes, args, xs)\n\u001b[0;32m--> 114\u001b[0m   broadcast_out, c, ys \u001b[39m=\u001b[39m fn(broadcast_in, c, \u001b[39m*\u001b[39;49mxs)\n\u001b[1;32m    116\u001b[0m   \u001b[39mif\u001b[39;00m init_mode:\n\u001b[1;32m    117\u001b[0m     ys \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mtree_map(\u001b[39mlambda\u001b[39;00m ax, y: (y \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m broadcast \u001b[39melse\u001b[39;00m ()),\n\u001b[1;32m    118\u001b[0m                            out_axes, ys)\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "\u001b[1;32m/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb Cell 36\u001b[0m in \u001b[0;36mRNNLM.__call__\u001b[0;34m(self, carry, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=60'>61</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minside call function of RNNLM, input:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=61'>62</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minside call function of RNNLM, carry:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mcarry\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=62'>63</a>\u001b[0m carries, next_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeep_cell(carry, \u001b[39minput\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=63'>64</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minside call function of RNNLM, next_states:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnext_states\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=64'>65</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minside call function of RNNLM, carries:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mcarries\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "\u001b[1;32m/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb Cell 36\u001b[0m in \u001b[0;36m_DeepRNN.__call__\u001b[0;34m(self, carry, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=46'>47</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minside deep rnn, state_idx:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mstate_idx\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minside deep rnn, inputs[state_idx]:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00minputs[state_idx]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=48'>49</a>\u001b[0m h_t, c_t, next_state \u001b[39m=\u001b[39m layer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=49'>50</a>\u001b[0m     current_carry, inputs[state_idx]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=50'>51</a>\u001b[0m )  \u001b[39m# problem line\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=51'>52</a>\u001b[0m h_t_outputs\u001b[39m.\u001b[39mappend(h_t)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=52'>53</a>\u001b[0m c_t_outputs\u001b[39m.\u001b[39mappend(c_t)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "\u001b[1;32m/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb Cell 36\u001b[0m in \u001b[0;36mHiPPOCell.__call__\u001b[0;34m(self, carry, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minside hippo cell, carry:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mcarry\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minside hippo cell, the cell:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=46'>47</a>\u001b[0m _, h_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcell(carry, \u001b[39minput\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minside hippo cell, h_t:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mh_t\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=49'>50</a>\u001b[0m \u001b[39m# y_t = nn.Dense(self.output_size)(h_t)  # f_t in the paper\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=50'>51</a>\u001b[0m \u001b[39m# print(f\"inside hippo cell, y_t: \\n{y_t}\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=51'>52</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=52'>53</a>\u001b[0m \u001b[39m# c_t = self.hippo(y_t, init_state=None, kernel=False)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=53'>54</a>\u001b[0m \u001b[39m# print(f\"inside hippo cell, c_t: \\n{c_t}\")\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "\u001b[1;32m/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb Cell 36\u001b[0m in \u001b[0;36mRNNCell.__call__\u001b[0;34m(self, carry, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=24'>25</a>\u001b[0m ht_1, _ \u001b[39m=\u001b[39m carry\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minside the rnn, input:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=28'>29</a>\u001b[0m h_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn_update(\u001b[39minput\u001b[39;49m, ht_1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000035?line=30'>31</a>\u001b[0m \u001b[39mreturn\u001b[39;00m (h_t, h_t), h_t\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/s4mer-rmt3vFtN-py3.9/lib/python3.9/site-packages/flax/core/axes_scan.py:138\u001b[0m, in \u001b[0;36mscan.<locals>.scan_fn\u001b[0;34m(broadcast_in, init, *args)\u001b[0m\n\u001b[1;32m    135\u001b[0m f_flat, out_tree \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mapi_util\u001b[39m.\u001b[39mflatten_fun_nokwargs(\n\u001b[1;32m    136\u001b[0m     lu\u001b[39m.\u001b[39mwrap_init(broadcast_body), in_tree)\n\u001b[1;32m    137\u001b[0m in_pvals \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(pe\u001b[39m.\u001b[39mPartialVal\u001b[39m.\u001b[39munknown, in_avals))\n\u001b[0;32m--> 138\u001b[0m _, out_pvals, _ \u001b[39m=\u001b[39m pe\u001b[39m.\u001b[39;49mtrace_to_jaxpr_nounits(f_flat, in_pvals)\n\u001b[1;32m    140\u001b[0m out_flat \u001b[39m=\u001b[39m []\n\u001b[1;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m pv, const \u001b[39min\u001b[39;00m out_pvals:\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/s4mer-rmt3vFtN-py3.9/lib/python3.9/site-packages/flax/core/axes_scan.py:114\u001b[0m, in \u001b[0;36mscan.<locals>.scan_fn.<locals>.body_fn\u001b[0;34m(c, xs, init_mode)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbody_fn\u001b[39m(c, xs, init_mode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m   \u001b[39m# inject constants\u001b[39;00m\n\u001b[1;32m    112\u001b[0m   xs \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mtree_map(\u001b[39mlambda\u001b[39;00m ax, arg, x: (arg \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m broadcast \u001b[39melse\u001b[39;00m x),\n\u001b[1;32m    113\u001b[0m                          in_axes, args, xs)\n\u001b[0;32m--> 114\u001b[0m   broadcast_out, c, ys \u001b[39m=\u001b[39m fn(broadcast_in, c, \u001b[39m*\u001b[39;49mxs)\n\u001b[1;32m    116\u001b[0m   \u001b[39mif\u001b[39;00m init_mode:\n\u001b[1;32m    117\u001b[0m     ys \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mtree_map(\u001b[39mlambda\u001b[39;00m ax, y: (y \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m broadcast \u001b[39melse\u001b[39;00m ()),\n\u001b[1;32m    118\u001b[0m                            out_axes, ys)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/s4mer-rmt3vFtN-py3.9/lib/python3.9/site-packages/flax/core/lift.py:754\u001b[0m, in \u001b[0;36mscan.<locals>.inner.<locals>.scanned\u001b[0;34m(broadcast_vars, carry, scan_variable_groups, rng_groups, args)\u001b[0m\n\u001b[1;32m    751\u001b[0m   variable_groups, rng_groups \u001b[39m=\u001b[39m data_transform(variable_groups,\n\u001b[1;32m    752\u001b[0m                                                rng_groups)\n\u001b[1;32m    753\u001b[0m scope \u001b[39m=\u001b[39m scope_fn(variable_groups, rng_groups)\n\u001b[0;32m--> 754\u001b[0m c, y \u001b[39m=\u001b[39m fn(scope, c, \u001b[39m*\u001b[39margs)\n\u001b[1;32m    755\u001b[0m out_vars \u001b[39m=\u001b[39m repack_fn(scope)\n\u001b[1;32m    756\u001b[0m broadcast_vars_out \u001b[39m=\u001b[39m out_vars[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# randomness is handled using explicit keys in Jax\n",
    "key, subkey = jax.random.split(subkey)\n",
    "\n",
    "num_layers = 3\n",
    "state_size = 8\n",
    "hidden_size = 4\n",
    "output_size = 1 #len(char_to_id)\n",
    "hippo_order_N = 16\n",
    "batch_size = 10\n",
    "\n",
    "print(f\"Create Model: RNNLM\")\n",
    "# model = CharRNN(state_size, len(char_to_id))\n",
    "model = RNNLM(\n",
    "    state_size=state_size,\n",
    "    vocab_size=len(char_to_id),\n",
    "    hidden_size=hidden_size,\n",
    "    hippo_order_N=hippo_order_N,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "print(\n",
    "    f\"carry init:\\n{model.initialize_carry(rng=subkey, batch_size=(batch_size,), hidden_size=hidden_size, init_fn=nn.initializers.zeros)}\"\n",
    ")\n",
    "print(\n",
    "    f\"state init:\\n{model.initialize_state(num_layers=num_layers,rng=rng,batch_size=batch_size,output_size=len(char_to_id), init_fn=nn.initializers.zeros,)}\"\n",
    ")\n",
    "\n",
    "print(f\"Init Model: RNNLM\")\n",
    "params = model.init(\n",
    "    subkey,\n",
    "    model.initialize_carry(\n",
    "        rng=key,\n",
    "        batch_size=(batch_size,),\n",
    "        hidden_size=hidden_size,\n",
    "        init_fn=nn.initializers.zeros,\n",
    "    ),\n",
    "    model.initialize_state(\n",
    "        num_layers=num_layers,\n",
    "        rng=subkey,\n",
    "        batch_size=batch_size,\n",
    "        output_size=output_size,\n",
    "        init_fn=nn.initializers.zeros,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Model state size: {model.state_size}, vocab size: {model.vocab_size}\")\n",
    "# output: Model state size: 8, vocab size: 5\n",
    "\n",
    "# run a single example through the model to test that it works\n",
    "new_state, predictions = model.apply(params, model.initial_carry(), 0)\n",
    "assert predictions.shape[0] == model.vocab_size\n",
    "\n",
    "# calling sample on random model leads to random output\n",
    "sample(model, params, (char_to_id, id_to_char), \"abc\", max_length=10)\n",
    "# output: 'abcadbaadbadd'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.10 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def chunker(seq, size):\n",
    "    \"\"\"\n",
    "    chunks a sequences into two subsequences\n",
    "    one for inputs, another for targets, by\n",
    "    shifting the input by 1\n",
    "    \"\"\"\n",
    "    n = len(seq)\n",
    "    p = 0\n",
    "    while p + 1 <= n:\n",
    "        # ensure the last chunk is of equal size\n",
    "        yield seq[p : min(n - 1, p + size)], seq[(p + 1) : (p + size + 1)]\n",
    "        p += size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.10 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def rnn_loss(params, model, carries, inputs, targets):\n",
    "    # use lax.scan to efficiently generate a loop over the inputs\n",
    "    # this function returns the final state, and predictions for every step\n",
    "    # note: scan input array needs have shape [length, 1]\n",
    "    final_state, predictions = jax.lax.scan(\n",
    "        lambda carry, input: model.apply(params, carry, input), carries, np.array([inputs]).T\n",
    "    )\n",
    "    loss = np.mean(jax.vmap(optax.softmax_cross_entropy)(predictions, np.array([targets]).T))\n",
    "    return loss, final_state\n",
    "\n",
    "\n",
    "# we want both the loss an gradient, we set has_aux because rnn_loss also return final state\n",
    "# use static_argnums=1 to indicate that the model is static;\n",
    "# a different model input will require recomplication\n",
    "# finally, we jit the function to improve runtime\n",
    "rnn_loss_grad = jax.jit(jax.value_and_grad(rnn_loss, has_aux=True), static_argnums=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.10 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def batch_step(model, optimizer, state, inputs, targets):\n",
    "    (loss, state), grad = rnn_loss_grad(optimizer.target, model, state, inputs, targets)\n",
    "    new_optimizer = optimizer.apply_gradient(grad)\n",
    "    return new_optimizer, loss, state\n",
    "\n",
    "\n",
    "def epoch_step(model, optimizer, data, batch_size):\n",
    "    state = model.init_state()\n",
    "    total_loss = 0\n",
    "    for n, (inputs, targets) in enumerate(chunker(data, batch_size)):\n",
    "        optimizer, loss, state = batch_step(model, optimizer, state, inputs, targets)\n",
    "\n",
    "        total_loss += loss\n",
    "    return optimizer, total_loss / (n + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.10 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('s4mer-rmt3vFtN-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "713952653653500d49378c840e7443049b55adc71879625da92cd4c6ea3482f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
