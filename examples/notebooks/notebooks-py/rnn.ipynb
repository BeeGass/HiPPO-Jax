{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.ops\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.linen.recurrent import RNNCellBase\n",
    "\n",
    "import optax\n",
    "\n",
    "import numpy as np  # convention: original numpy\n",
    "\n",
    "from typing import Any, Callable, Sequence, Optional, Tuple, Union\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import pprint\n",
    "\n",
    "from src.models.hippo.hippo import HiPPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 2\n",
    "rng, subkey = jax.random.split(key, num=num_copies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_batch(nest, batch_size: Optional[int]):\n",
    "    \"\"\"Adds a batch dimension at axis 0 to the leaves of a nested structure.\"\"\"\n",
    "    broadcast = lambda x: jnp.broadcast_to(x, (batch_size,) + x.shape)\n",
    "\n",
    "    return jax.tree_map(broadcast, nest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(RNNCellBase):\n",
    "    hidden_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            W_xh = x_{t} @ W_{xh} - multiply the previous hidden state with\n",
    "            W_hh = H_{t-1} @ W_{hh} + b_{h} - this a linear layer\n",
    "\n",
    "            H_{t} = f_{w}(H_{t-1}, x)\n",
    "            H_{t} = tanh(H_{t-1} @ W_{hh}) + (x_{t} @ W_{xh})\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): hidden state size\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        ht_1, _ = carry\n",
    "\n",
    "        h_t = self.rnn_update(input, ht_1)\n",
    "\n",
    "        return (h_t, h_t), h_t\n",
    "\n",
    "    @partial(\n",
    "        nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    )\n",
    "    def rnn_update(self, input, ht_1):\n",
    "\n",
    "        W_hh = nn.Dense(self.hidden_size)(ht_1)\n",
    "        W_xh = nn.Dense(self.hidden_size)(input)\n",
    "        h_t = nn.relu(W_hh + W_xh)  # H_{t} = tanh(H_{t-1} @ W_{hh}) + (x_{t} @ W_{xh})\n",
    "\n",
    "        return h_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(rng, batch_size, hidden_size, init_fn=nn.initializers.zeros):\n",
    "        \"\"\"Initialize the RNN cell carry.\n",
    "        Args:\n",
    "        rng: random number generator passed to the init_fn.\n",
    "        batch_dims: a tuple providing the shape of the batch dimensions.\n",
    "        hidden_size: the size or number of features of the memory.\n",
    "        init_fn: initializer function for the carry.\n",
    "        Returns:\n",
    "        An initialized carry for the given RNN cell.\n",
    "        \"\"\"\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(RNNCellBase):\n",
    "    hidden_size: int\n",
    "\n",
    "    # @partial(\n",
    "    #     nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    # )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            i_{t} = sigmoid((W_{ii} @ x_{t} + b_{ii}) + (W_{hi} @ h_{t-1} + b_{hi}))\n",
    "            f_{t} = sigmoid((W_{if} @ x_{t} + b_{if}) + (W_{hf} @ h_{t-1} + b_{hf}))\n",
    "            g_{t} = tanh((W_{ig} @ x_{t} + b_{ig}) + (W_{hg} @ h_{t-1} + b_{hg}))\n",
    "            o_{t} = sigmoid((W_{io} @ x_{t} + b_{io}) + (W_{ho} @ h_{t-1} + b_{ho}))\n",
    "            c_{t} = f_{t} * c_{t-1} + i_{t} * g_{t}\n",
    "            h_{t} = o_{t} * tanh(c_{t})\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): hidden state size\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        print(f\"inside the LSTMCell, input:\\n{input}\")\n",
    "        print(f\"inside the LSTMCell, input type:\\n{type(input)}\")\n",
    "        ht_1, ct_1 = carry\n",
    "        print(f\"carry split:\\n{ht_1}\\n{ct_1}\")\n",
    "\n",
    "        c_t, h_t = self.rnn_update(input, ht_1, ct_1)\n",
    "        print(f\"inside the LSTMCell, c_t:\\n{c_t}\")\n",
    "        print(f\"inside the LSTMCell, h_t:\\n{h_t}\")\n",
    "\n",
    "        return (h_t, c_t), h_t\n",
    "\n",
    "    @partial(\n",
    "        nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    )\n",
    "    def rnn_update(self, input, ht_1, ct_1):\n",
    "        print(f\"inside the LSTMCell rnn_update, input:\\n{input}\")\n",
    "        print(f\"inside the LSTMCell rnn_update, ht_1:\\n{ht_1}\")\n",
    "        print(f\"inside the LSTMCell rnn_update, ct_1:\\n{ct_1}\")\n",
    "        # i_ta = partial(\n",
    "        #     nn.Dense,\n",
    "        #     features=ht_1.shape()[0],\n",
    "        #     use_bias=False,\n",
    "        #     kernel_init=self.recurrent_kernel_init,\n",
    "        #     bias_init=self.bias_init,\n",
    "        # )\n",
    "\n",
    "        i_ta = nn.Dense(features=ht_1.shape()[0])(input)\n",
    "        i_tb = nn.Dense(features=self.hidden_size)(ht_1)\n",
    "        i_t = nn.sigmoid(i_ta + i_tb)  # input gate\n",
    "        print(f\"inside the LSTMCell, input gate output:\\n{i_t}\")\n",
    "\n",
    "        o_ta = nn.Dense(self.hidden_size)(input)\n",
    "        o_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        o_t = nn.sigmoid(o_ta + o_tb)  # output gate\n",
    "        print(f\"inside the LSTMCell, output gate output:\\n{o_t}\")\n",
    "\n",
    "        f_ia = nn.Dense(self.hidden_size)(\n",
    "            input\n",
    "        )  # b^{f}_{i} + \\sum\\limits_{j} U^{f}_{i, j} x^{t}_{j}\n",
    "        f_ib = nn.Dense(self.hidden_size)(\n",
    "            ht_1\n",
    "        )  # \\sum\\limits_{j} W^{f}_{i, j} h^{(t-1)}_{j}\n",
    "        f_i = nn.sigmoid(f_ia + f_ib)  # forget gate\n",
    "        print(f\"inside the LSTMCell, forget gate output:\\n{f_i}\")\n",
    "\n",
    "        g_ia = nn.Dense(self.hidden_size)(\n",
    "            input\n",
    "        )  # b^{g}_{i} + \\sum\\limits_{j} U^{g}_{i, j} x^{t}_{j}\n",
    "        g_ib = nn.Dense(self.hidden_size)(\n",
    "            ht_1\n",
    "        )  # \\sum\\limits_{j} W^{g}_{i, j} h^{(t-1)}_{j}\n",
    "        g_i = nn.tanh(g_ia + g_ib)  # (external) input gate\n",
    "        print(f\"inside the LSTMCell, (external) input gate output:\\n{g_i}\")\n",
    "\n",
    "        c_t = (f_i * ct_1) + (i_t * g_i)  # internal cell state update\n",
    "        print(f\"inside the LSTMCell, cell state output:\\n{c_t}\")\n",
    "\n",
    "        h_t = o_t * nn.tanh(c_t)  # hidden state update\n",
    "        print(f\"inside the LSTMCell, hidden state output:\\n{h_t}\")\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(rng, batch_size, hidden_size, init_fn=nn.initializers.zeros):\n",
    "        \"\"\"Initialize the RNN cell carry.\n",
    "        Args:\n",
    "        rng: random number generator passed to the init_fn.\n",
    "        batch_dims: a tuple providing the shape of the batch dimensions.\n",
    "        hidden_size: the size or number of features of the memory.\n",
    "        init_fn: initializer function for the carry.\n",
    "        Returns:\n",
    "        An initialized carry for the given RNN cell.\n",
    "        \"\"\"\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(RNNCellBase):\n",
    "    hidden_size: int\n",
    "\n",
    "    # @partial(\n",
    "    #     nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    # )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            z_t = sigmoid((W_{iz} @ x_{t} + b_{iz}) + (W_{hz} @ h_{t-1} + b_{hz}))\n",
    "            r_t = sigmoid((W_{ir} @ x_{t} + b_{ir}) + (W_{hr} @ h_{t-1} + b_{hr}))\n",
    "            g_t = tanh(((W_{ig} @ x_{t} + b_{ig}) + r_t) * (W_{hg} @ h_{t-1} + b_{hg}))\n",
    "            h_t = (z_t * h_{t-1}) + ((1 - z_t) * g_i)\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): hidden state size\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        ht_1 = carry\n",
    "\n",
    "        h_t = self.rnn_update(input, ht_1)\n",
    "\n",
    "        return (h_t, h_t), h_t\n",
    "\n",
    "    @partial(\n",
    "        nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    )\n",
    "    def rnn_update(self, input, ht_1):\n",
    "\n",
    "        z_ta = nn.Dense(self.hidden_size)(input)\n",
    "        z_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        z_t = nn.sigmoid(z_ta + z_tb)  # reset gate\n",
    "\n",
    "        r_ta = nn.Dense(self.hidden_size)(input)\n",
    "        r_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        r_t = nn.sigmoid(r_ta + r_tb)  # update gate\n",
    "\n",
    "        g_ta = nn.Dense(self.hidden_size)(input)\n",
    "        g_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        g_t = nn.tanh((g_ta + r_t) * g_tb)  # (external) input gate\n",
    "\n",
    "        h_t = ((1 - z_t) * ht_1) + (z_t * g_t)  # internal cell state update\n",
    "\n",
    "        return h_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(rng, batch_size, hidden_size, init_fn=nn.initializers.zeros):\n",
    "        \"\"\"Initialize the RNN cell carry.\n",
    "        Args:\n",
    "        rng: random number generator passed to the init_fn.\n",
    "        batch_dims: a tuple providing the shape of the batch dimensions.\n",
    "        hidden_size: the size or number of features of the memory.\n",
    "        init_fn: initializer function for the carry.\n",
    "        Returns:\n",
    "        An initialized carry for the given RNN cell.\n",
    "        \"\"\"\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPOCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        RNN update function\n",
    "        τ(h, x) = (1 - g(h, x)) ◦ h + g(h, x) ◦ tanh(Lτ (h, x))\n",
    "        g(h, x) = σ(Lg(h,x))\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): hidden state size\n",
    "        output_size (int): output size\n",
    "        hippo (HiPPO): hippo model object\n",
    "        cell (RNNCellBase): choice of RNN cell object\n",
    "            - RNNCell\n",
    "            - LSTMCell\n",
    "            - GRUCell\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "    hippo: HiPPO\n",
    "    model: RNNCellBase\n",
    "\n",
    "    def setup(self):\n",
    "        self.cell = self.model(self.hidden_size)\n",
    "\n",
    "    @partial(\n",
    "        nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    )\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            RNN update function\n",
    "            τ(h, x) = (1 - g(h, x)) ◦ h + g(h, x) ◦ tanh(Lτ (h, x))\n",
    "            g(h, x) = σ(Lg(h,x))\n",
    "\n",
    "        Args:\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"inside hippo cell, input: {input}\")\n",
    "        print(f\"inside hippo cell, carry: {carry}\")\n",
    "        _, h_t = self.cell(carry, input)\n",
    "\n",
    "        y_t = nn.Dense(self.output_size)(h_t)  # f_t in the paper\n",
    "\n",
    "        c_t = self.hippo(y_t, init_state=None, kernel=False)\n",
    "\n",
    "        return (h_t, c_t), h_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(rng, batch_size, hidden_size, init_fn=nn.initializers.zeros):\n",
    "        \"\"\"Initialize the RNN cell carry.\n",
    "        Args:\n",
    "        rng: random number generator passed to the init_fn.\n",
    "        batch_dims: a tuple providing the shape of the batch dimensions.\n",
    "        hidden_size: the size or number of features of the memory.\n",
    "        init_fn: initializer function for the carry.\n",
    "        Returns:\n",
    "        An initialized carry for the given RNN cell.\n",
    "        \"\"\"\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refer to https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/recurrent.py#L714-L762\n",
    "# also refer to https://dm-haiku.readthedocs.io/en/latest/api.html?highlight=DeepRNN#deeprnn\n",
    "class _DeepRNN(RNNCellBase):\n",
    "    layers: Sequence[Any]\n",
    "    skip_connections: bool\n",
    "    hidden_to_output_layer: bool\n",
    "    layer_name: Optional[str]\n",
    "\n",
    "    def setup(self):\n",
    "        if self.skip_connections:\n",
    "            for layer in self.layers:\n",
    "                if not (isinstance(layer, RNNCellBase) or isinstance(layer, HiPPOCell)):\n",
    "                    raise ValueError(\n",
    "                        \"skip_connections requires for all layers to be \"\n",
    "                        \"`hk.RNNCore`s. Layers is: {}\".format(self.layers)\n",
    "                    )\n",
    "                    # raise ValueError(\n",
    "                    #     f\"{self.layer_name} layer {layer} is not a RNNCellBase or HiPPOCell\"\n",
    "                    # )\n",
    "\n",
    "    def __call__(self, carry, inputs):\n",
    "        current_carry = carry\n",
    "        next_states = []\n",
    "        h_t_outputs = []\n",
    "        c_t_outputs = []\n",
    "        state_idx = 0\n",
    "        h_t, c_t = current_carry  # c_t may actually be h_t in which case dont use it\n",
    "        (\n",
    "            h_t_copy,\n",
    "            c_t_copy,\n",
    "        ) = current_carry  # c_t may actually be h_t in which case dont use it\n",
    "        concat = lambda *args: jnp.concatenate(args, axis=-1)\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if self.skip_connections and idx > 0:\n",
    "                skip_h_t = jax.tree_map(concat, h_t, h_t_copy)\n",
    "                skip_c_t = jax.tree_map(concat, c_t, c_t_copy)\n",
    "                current_carry = (skip_h_t, skip_c_t)\n",
    "\n",
    "            if isinstance(layer, RNNCellBase) or isinstance(layer, HiPPOCell):\n",
    "                print(f\"current_carry before layer: {current_carry}\")\n",
    "                # print(f\"layer: {layer}\")\n",
    "                current_carry, next_state = layer(current_carry, inputs[state_idx])\n",
    "                print(f\"current_carry:\\n {current_carry}\")\n",
    "                if self.hidden_to_output_layer:\n",
    "                    next_state = nn.Dense(next_state.shape[0])(next_state)\n",
    "\n",
    "                h_t, c_t = current_carry\n",
    "                print(f\"h_t, c_t before return:\\n {h_t}, {c_t}\")\n",
    "                h_t_outputs.append(h_t)\n",
    "                c_t_outputs.append(c_t)\n",
    "                next_states.append(next_state)\n",
    "                state_idx += 1\n",
    "\n",
    "            else:\n",
    "                print(f\"current_carry before layer: {current_carry}\")\n",
    "                print(f\"layer: {layer}\")\n",
    "                current_carry = layer(current_carry)\n",
    "                print(f\"current_carry:\\n {current_carry}\")\n",
    "\n",
    "        if self.skip_connections:\n",
    "            skip_h_t_out = jax.tree_map(concat, *h_t_outputs)\n",
    "            skip_c_t_out = jax.tree_map(concat, *c_t_outputs)\n",
    "            out = (skip_h_t_out, skip_c_t_out)\n",
    "        else:\n",
    "            out = current_carry\n",
    "\n",
    "        print(f\"next_states before tuple:\\n\", next_states)\n",
    "        pp.pprint(layer)\n",
    "        print(f\"carry before return B:\\n\", out)\n",
    "\n",
    "        return out, tuple(next_states)\n",
    "\n",
    "    def initialize_carry(self, rng, batch_size: int, hidden_size: int):\n",
    "        carry_list = []\n",
    "        key_list = jax.random.split(rng, num=len(self.layers))\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, RNNCellBase) or isinstance(layer, HiPPOCell):\n",
    "                carry_list.append(\n",
    "                    layer.initialize_carry(\n",
    "                        rng=key_list[i], batch_size=batch_size, hidden_size=hidden_size\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                carry_list.append(\n",
    "                    layer.initialize_carry(rng=key_list[i], batch_size=batch_size)\n",
    "                )\n",
    "\n",
    "        return tuple(carry_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRNN(_DeepRNN):\n",
    "    r\"\"\"Wraps a sequence of cores and callables as a single core.\n",
    "        >>> deep_rnn = hk.DeepRNN([\n",
    "        ...     LSTMCell(hidden_size=4),\n",
    "        ...     jax.nn.relu,\n",
    "        ...     LSTMCell(hidden_size=2),\n",
    "        ... ])\n",
    "    The state of a :class:`DeepRNN` is a tuple with one element per\n",
    "    :class:`RNNCore`. If no layers are :class:`RNNCore`\\ s, the state is an empty\n",
    "    tuple.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers: Sequence[Any],\n",
    "        skip_connections: Optional[bool] = False,\n",
    "        hidden_to_output_layer: Optional[bool] = False,\n",
    "        name: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            layers,\n",
    "            skip_connections=skip_connections,\n",
    "            hidden_to_output_layer=hidden_to_output_layer,\n",
    "            layer_name=name,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Training Types\n",
    "refer to [this](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one2one(params, init_carry, input, sequence_length, cell):\n",
    "    _, h_t = cell.apply(params, init_carry, input)\n",
    "    y_t = nn.Dense(input.shape[0])(h_t)  # output\n",
    "    return y_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one2many(params, init_carry, input, T_y, cell, tf_bool=True):\n",
    "    output = []\n",
    "    if not tf_bool:\n",
    "        for i in range(len(T_y)):\n",
    "            if i == 0:\n",
    "                carry, h_t = cell.apply(params, init_carry, input[i])\n",
    "            else:\n",
    "                carry, h_t = cell.apply(params, carry, output[i - 1])\n",
    "\n",
    "            y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "            output.append(y_t)\n",
    "    else:\n",
    "        for i in range(len(input)):\n",
    "            carry, h_t = cell.apply(params, init_carry, input[i])\n",
    "            y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "            output.append(y_t)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many2one(params, init_carry, input, T_x, cell, tf_bool=True):\n",
    "    y_t = None\n",
    "    carry = init_carry\n",
    "    for i in range(len(T_x)):\n",
    "        carry, h_t = cell.apply(params, carry, input[i])\n",
    "        if i == (len(T_x) - 1):\n",
    "            y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "\n",
    "    return y_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many2many(params, init_carry, input, T_xy, cell, tf_bool=True):\n",
    "    output = []\n",
    "    carry = init_carry\n",
    "    for i in range(len(T_xy)):\n",
    "        carry, h_t = cell.apply(params, carry, input[i])\n",
    "        y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "        output.append(y_t)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many2many(params, init_carry, input, T_x, T_y, cell, tf_bool=True):\n",
    "    assert T_x != T_y, \"T_x and T_y must be different\"\n",
    "    output = []\n",
    "    carry = init_carry\n",
    "    for i in range(len(T_x)):\n",
    "        carry, h_t = cell.apply(params, carry, input[i])\n",
    "\n",
    "    for i in range(len(T_y)):\n",
    "        carry, h_t = cell.apply(params, carry, input[i])\n",
    "        y_t = nn.Dense(input[i].shape[0])(h_t)  # output\n",
    "        output.append(y_t)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data_2_id(iterable):\n",
    "    \"\"\"\n",
    "    provides mapping to and from ids\n",
    "    \"\"\"\n",
    "    \n",
    "    id_2_data = {}\n",
    "    data_2_id = {}\n",
    "\n",
    "    for id, elem in enumerate(iterable):\n",
    "        id_2_data[id] = elem\n",
    "\n",
    "    for id, elem in enumerate(iterable):\n",
    "        data_2_id[elem] = id\n",
    "\n",
    "    return (id_2_data, data_2_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(i, n):\n",
    "    \"\"\"\n",
    "    create vector of size n with 1 at index i\n",
    "    \"\"\"\n",
    "    x = defaultdict(lambda: jnp.zeros(n))\n",
    "    # x[i] = 1\n",
    "    return x[i].at[i].set(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(char):\n",
    "    return one_hot(data_2_id[char], len(data_2_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(predictions, id_2_data):\n",
    "    return id_2_data[int(jnp.argmax(predictions))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(fname):\n",
    "  with open(fname, \"r\") as reader:\n",
    "    data = reader.read()\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "  chars = list(set(data))\n",
    "  vocab_size = len(chars)\n",
    "  char_to_id, id_to_char = map_data_2_id(chars)\n",
    "  char_to_id = {value:key for key, value in char_to_id.items()}\n",
    "  # data converted to ids\n",
    "  # data_id = [char_to_id[char] for char in data]\n",
    "  data_id = [char_to_id[char] for char in data]\n",
    "  return data_id, char_to_id, id_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 0, 4, 4, 4, 2, 3, 1]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"abcd...abcd...\"\n",
    "data_id, char_to_id, id_to_char = prep_data(data)\n",
    "data_id[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check [this](https://github.com/deepmind/optax/blob/master/examples/quick_start.ipynb) out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_optimizer_fn(starting_learning_rate, name=\"adam\"):\n",
    "    # refer to https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules\n",
    "    optim = None\n",
    "\n",
    "    if name == \"sgd\":\n",
    "        optim = optax.sgd(starting_learning_rate)\n",
    "    elif name == \"adam\":\n",
    "        optim = optax.adam(\n",
    "            starting_learning_rate,\n",
    "        )\n",
    "    elif name == \"adagrad\":\n",
    "        optim = optax.adagrad(starting_learning_rate)\n",
    "    elif name == \"rmsprop\":\n",
    "        optim = optax.rmsprop(starting_learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"optimizer name not recognized\")\n",
    "\n",
    "    return optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_scheduler_fn(\n",
    "    start_learning_rate, steps, decay_rate, init_value, end_val, name\n",
    "):\n",
    "    # refer to https://optax.readthedocs.io/en/latest/api.html#schedules\n",
    "    scheduler = None\n",
    "\n",
    "    if name == \"constant\":\n",
    "        scheduler = optax.constant_schedule(init_value)\n",
    "\n",
    "    elif name == \"exp_decay\":\n",
    "        scheduler = optax.exponential_decay(\n",
    "            init_value=start_learning_rate, transition_steps=1000, decay_rate=0.99\n",
    "        )\n",
    "    elif name == \"linear\":\n",
    "        scheduler = optax.linear_schedule(init_value=init_value, end_value=end_val)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"scheduler name not recognized\")\n",
    "\n",
    "    return scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add transformations\n",
    "# refer to https://optax.readthedocs.io/en/latest/api.html#optax-transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # A simple update loop.\n",
    "#     for _ in range(1000):\n",
    "#     grads = jax.grad(compute_loss)(params, xs, ys)\n",
    "#     updates, opt_state = gradient_transform.update(grads, opt_state)\n",
    "#     params = optax.apply_updates(params, updates)\n",
    "\n",
    "#     assert jnp.allclose(params, target_params), \\\n",
    "#     'Optimization should retrieve the target params used to generate the data.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimizer_fn(start_learning_rate, params, num_weights, x, y):\n",
    "#     optimizer = optax.adam(start_learning_rate)\n",
    "#     # Obtain the `opt_state` that contains statistics for the optimizer.\n",
    "#     params = {'w': jnp.ones((num_weights,))}\n",
    "#     opt_state = optimizer.init(params)\n",
    "\n",
    "#     compute_loss = lambda params, x, y: optax.l2_loss(params['w'].dot(x), y)\n",
    "#     grads = jax.grad(compute_loss)(params, xs, ys)\n",
    "\n",
    "#     updates, opt_state = optimizer.update(grads, opt_state)\n",
    "#     params = optax.apply_updates(params, updates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    state_size: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int\n",
    "    hippo_order_N: int\n",
    "    batch_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        L = self.vocab_size\n",
    "        N = self.state_size\n",
    "\n",
    "        hippo = HiPPO(\n",
    "            N=N,\n",
    "            max_length=L,\n",
    "            measure=\"legs\",\n",
    "            step=1.0 / L,\n",
    "            GBT_alpha=0.5,\n",
    "            seq_L=L,\n",
    "            v=\"v\",\n",
    "            lambda_n=1.0,\n",
    "            fourier_type=\"fru\",\n",
    "            alpha=0.0,\n",
    "            beta=1.0,\n",
    "        )\n",
    "\n",
    "        cell1 = LSTMCell\n",
    "        cell2 = LSTMCell\n",
    "        cell3 = LSTMCell\n",
    "\n",
    "        input_layers = [\n",
    "            HiPPOCell(hippo=hippo, model=cell1, hidden_size=8, output_size=N),\n",
    "            HiPPOCell(hippo=hippo, model=cell2, hidden_size=64, output_size=N),\n",
    "            HiPPOCell(hippo=hippo, model=cell3, hidden_size=512, output_size=N),\n",
    "        ]\n",
    "\n",
    "        self.deep_cell = DeepRNN(\n",
    "            layers=input_layers,\n",
    "            skip_connections=True,\n",
    "            hidden_to_output_layer=True,\n",
    "            name=\"RNNLM\",\n",
    "        )\n",
    "        \n",
    "    @partial(\n",
    "        nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    )\n",
    "    def __call__(self, carry, i):\n",
    "        input = one_hot(i, self.vocab_size)\n",
    "        print(input)\n",
    "        carries, next_states = self.deep_cell(carry, input)\n",
    "        print(next_states)\n",
    "        predictions = nn.softmax(nn.Dense(self.vocab_size)(next_states[-1]))\n",
    "        return carries, next_states, predictions\n",
    "\n",
    "    def initialize_carry(self, rng, batch_size, hidden_size):\n",
    "        return self.deep_cell.initialize_carry(\n",
    "            rng=rng, batch_size=batch_size, hidden_size=hidden_size\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, params, bridge, initial=\"\", max_length=100):\n",
    "    char_to_id, id_to_char = bridge\n",
    "    state = model.init_state()\n",
    "    output = initial\n",
    "    if len(initial) > 0:\n",
    "        for char in initial[:-1]:\n",
    "            _, state, _ = model.apply(params, char_to_id[char], state)\n",
    "\n",
    "    next_char = initial[-1]\n",
    "    for i in range(max_length):\n",
    "        state, predictions = model.apply(params, state, char_to_id[next_char], state)\n",
    "        next_char = decode(predictions, id_to_char)\n",
    "        output += next_char\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to [this](https://github.com/manifest/flax-extra/blob/48efe1f1515893289b44646977bf5049a340b6c8/docs/notebooks/combinators.ipynb), [this](https://github.com/romanak/pyprobml/blob/65c82b9b43d2100cbc7c59e766161ee801c0f85f/notebooks/book1/15/rnn_jax.ipynb), [this](https://github.com/probml/pyprobml/blob/71d98dcdd3798525353eb1bfb9851b47e9d64bde/notebooks/book1/15/rnn_jax.ipynb) and [this](https://github.com/probml/probml-notebooks/blob/36cb173afce3f4a07a7b475cf8a7937025a60465/notebooks-d2l/rnn_jax.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "\"RNNLM\" object has no attribute \"deep_cell\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb Cell 40\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=9'>10</a>\u001b[0m \u001b[39m# model = CharRNN(state_size, len(char_to_id))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=10'>11</a>\u001b[0m model \u001b[39m=\u001b[39m RNNLM(state_size, \u001b[39mlen\u001b[39m(char_to_id), hidden_size, hippo_order_N, batch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=11'>12</a>\u001b[0m params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=12'>13</a>\u001b[0m     subkey,\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=13'>14</a>\u001b[0m     model\u001b[39m.\u001b[39;49minitialize_carry(rng\u001b[39m=\u001b[39;49mkey, batch_size\u001b[39m=\u001b[39;49mbatch_size, hidden_size\u001b[39m=\u001b[39;49mhidden_size),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=14'>15</a>\u001b[0m     \u001b[39m0\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=15'>16</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel state size: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mstate_size\u001b[39m}\u001b[39;00m\u001b[39m, vocab size: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mvocab_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=18'>19</a>\u001b[0m \u001b[39m# output: Model state size: 8, vocab size: 5\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=19'>20</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=20'>21</a>\u001b[0m \u001b[39m# run a single example through the model to test that it works\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/jax-pytorch/lib/python3.8/site-packages/flax/linen/transforms.py:1246\u001b[0m, in \u001b[0;36mnamed_call.<locals>.wrapped_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(prewrapped_fn)\n\u001b[1;32m   1243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1244\u001b[0m   \u001b[39mif\u001b[39;00m ((\u001b[39mnot\u001b[39;00m force \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m linen_module\u001b[39m.\u001b[39m_use_named_call)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m       \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39min_setup):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1246\u001b[0m     \u001b[39mreturn\u001b[39;00m prewrapped_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1247\u001b[0m   fn_name \u001b[39m=\u001b[39m class_fn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m   1248\u001b[0m   method_suffix \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mfn_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m fn_name \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__call__\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/jax-pytorch/lib/python3.8/site-packages/flax/linen/module.py:352\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    351\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 352\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    353\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/jax-pytorch/lib/python3.8/site-packages/flax/linen/module.py:651\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m _context\u001b[39m.\u001b[39mmodule_stack\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m)\n\u001b[1;32m    650\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 651\u001b[0m   y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    652\u001b[0m   \u001b[39mif\u001b[39;00m _context\u001b[39m.\u001b[39mcapture_stack:\n\u001b[1;32m    653\u001b[0m     filter_fn \u001b[39m=\u001b[39m _context\u001b[39m.\u001b[39mcapture_stack[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32m/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb Cell 40\u001b[0m in \u001b[0;36mRNNLM.initialize_carry\u001b[0;34m(self, rng, batch_size, hidden_size)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=53'>54</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitialize_carry\u001b[39m(\u001b[39mself\u001b[39m, rng, batch_size, hidden_size):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=54'>55</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeep_cell\u001b[39m.\u001b[39minitialize_carry(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=55'>56</a>\u001b[0m         rng\u001b[39m=\u001b[39mrng, batch_size\u001b[39m=\u001b[39mbatch_size, hidden_size\u001b[39m=\u001b[39mhidden_size\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beegass/Documents/Coding/s4mer/examples/notebooks/notebooks-py/rnn.ipynb#ch0000039?line=56'>57</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/jax-pytorch/lib/python3.8/site-packages/flax/linen/module.py:717\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    715\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[name]\n\u001b[1;32m    716\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    718\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: \"RNNLM\" object has no attribute \"deep_cell\""
     ]
    }
   ],
   "source": [
    "state_size = 8\n",
    "\n",
    "# randomness is handled using explicit keys in Jax\n",
    "key, subkey = jax.random.split(subkey)\n",
    "\n",
    "hidden_size = 4\n",
    "hippo_order_N = 16\n",
    "batch_size = 1\n",
    "\n",
    "# model = CharRNN(state_size, len(char_to_id))\n",
    "model = RNNLM(state_size, len(char_to_id), hidden_size, hippo_order_N, batch_size)\n",
    "params = model.init(\n",
    "    subkey,\n",
    "    model.initialize_carry(rng=key, batch_size=batch_size, hidden_size=hidden_size),\n",
    "    0,\n",
    ")\n",
    "\n",
    "print(f\"Model state size: {model.state_size}, vocab size: {model.vocab_size}\")\n",
    "# output: Model state size: 8, vocab size: 5\n",
    "\n",
    "# run a single example through the model to test that it works\n",
    "new_state, predictions = model.apply(params, model.initial_carry(), 0)\n",
    "assert predictions.shape[0] == model.vocab_size\n",
    "\n",
    "# calling sample on random model leads to random output\n",
    "sample(model, params, (char_to_id, id_to_char), \"abc\", max_length=10)\n",
    "# output: 'abcadbaadbadd'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    \"\"\"\n",
    "    chunks a sequences into two subsequences\n",
    "    one for inputs, another for targets, by\n",
    "    shifting the input by 1\n",
    "    \"\"\"\n",
    "    n = len(seq)\n",
    "    p = 0\n",
    "    while p + 1 <= n:\n",
    "        # ensure the last chunk is of equal size\n",
    "        yield seq[p : min(n - 1, p + size)], seq[(p + 1) : (p + size + 1)]\n",
    "        p += size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_loss(params, model, carries, inputs, targets):\n",
    "    # use lax.scan to efficiently generate a loop over the inputs\n",
    "    # this function returns the final state, and predictions for every step\n",
    "    # note: scan input array needs have shape [length, 1]\n",
    "    final_state, predictions = jax.lax.scan(\n",
    "        lambda carry, input: model.apply(params, carry, input), carries, np.array([inputs]).T\n",
    "    )\n",
    "    loss = np.mean(jax.vmap(optax.softmax_cross_entropy)(predictions, np.array([targets]).T))\n",
    "    return loss, final_state\n",
    "\n",
    "\n",
    "# we want both the loss an gradient, we set has_aux because rnn_loss also return final state\n",
    "# use static_argnums=1 to indicate that the model is static;\n",
    "# a different model input will require recomplication\n",
    "# finally, we jit the function to improve runtime\n",
    "rnn_loss_grad = jax.jit(jax.value_and_grad(rnn_loss, has_aux=True), static_argnums=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_step(model, optimizer, state, inputs, targets):\n",
    "    (loss, state), grad = rnn_loss_grad(optimizer.target, model, state, inputs, targets)\n",
    "    new_optimizer = optimizer.apply_gradient(grad)\n",
    "    return new_optimizer, loss, state\n",
    "\n",
    "\n",
    "def epoch_step(model, optimizer, data, batch_size):\n",
    "    state = model.init_state()\n",
    "    total_loss = 0\n",
    "    for n, (inputs, targets) in enumerate(chunker(data, batch_size)):\n",
    "        optimizer, loss, state = batch_step(model, optimizer, state, inputs, targets)\n",
    "\n",
    "        total_loss += loss\n",
    "    return optimizer, total_loss / (n + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('jax-pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbeba54f39f0ad863615cd2814766ffd78084a8276e5c331aa23b4d5ff4f068c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
