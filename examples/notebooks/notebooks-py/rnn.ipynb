{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.ops\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental.host_callback import id_print\n",
    "from jax.tree_util import Partial\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.linen.recurrent import RNNCellBase\n",
    "\n",
    "import optax\n",
    "\n",
    "import numpy as np  # convention: original numpy\n",
    "\n",
    "from typing import Any, Callable, Sequence, Optional, Tuple, Union\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import pprint\n",
    "\n",
    "from src.models.hippo.hippo import HiPPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 2\n",
    "rng, subkey = jax.random.split(key, num=num_copies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_batch(nest, batch_size: Optional[int]):\n",
    "    \"\"\"Adds a batch dimension at axis 0 to the leaves of a nested structure.\"\"\"\n",
    "    broadcast = lambda x: jnp.broadcast_to(x, (batch_size,) + x.shape)\n",
    "\n",
    "    return jax.tree_map(broadcast, nest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(RNNCellBase):\n",
    "    hidden_size: int\n",
    "\n",
    "    # def setup(self):\n",
    "        # self.dense_h = Partial(\n",
    "        #     nn.Dense,\n",
    "        #     features=self.hidden_size,\n",
    "        #     use_bias=True,\n",
    "        #     kernel_init=nn.initializers.orthogonal(),\n",
    "        #     bias_init=nn.initializers.zeros,\n",
    "        #     dtype=None,\n",
    "        #     param_dtype=jnp.float32,\n",
    "        # )\n",
    "\n",
    "        # self.dense_o = Partial(\n",
    "        #     nn.Dense,\n",
    "        #     features=self.hidden_size,\n",
    "        #     use_bias=False,\n",
    "        #     kernel_init=nn.initializers.orthogonal(),\n",
    "        #     bias_init=nn.initializers.zeros,\n",
    "        #     dtype=None,\n",
    "        #     param_dtype=jnp.float32,\n",
    "        # )\n",
    "\n",
    "    # @partial(\n",
    "    #     nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    # )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            W_xh = x_{t} @ W_{xh} - multiply the previous hidden state with\n",
    "            W_hh = H_{t-1} @ W_{hh} + b_{h} - this a linear layer\n",
    "\n",
    "            H_{t} = f_{w}(H_{t-1}, x)\n",
    "            H_{t} = tanh(H_{t-1} @ W_{hh}) + (x_{t} @ W_{xh})\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): hidden state size\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        ht_1, _ = carry\n",
    "\n",
    "        print(f\"inside the rnn, input:\\n{input.shape}\")\n",
    "\n",
    "        h_t = self.rnn_update(ht_1, input)\n",
    "\n",
    "        return (h_t, h_t), h_t\n",
    "\n",
    "    def rnn_update(self, ht_1, input):\n",
    "        print(f\"inside the rnn update, input:\\n{input.shape}\")\n",
    "        print(f\"inside the rnn update, ht_1:\\n{ht_1.shape}\")\n",
    "\n",
    "        # print(f\"self.dense_h(name='dense rnn_wxh layer')(ht_1):\\n{self.dense_h(name='dense rnn_wxh layer')(ht_1)}\")\n",
    "        # W_hh = self.dense_h(ht_1)\n",
    "\n",
    "        W_hh = nn.Dense(self.hidden_size)(ht_1)\n",
    "        print(f\"W_hh:\\n{W_hh.shape}\")\n",
    "        # id_print(W_hh, what=\"BLAH BLAH BLAH\", tap_with_device=True)\n",
    "        # print(f\"W_hh:\\n{W_hh}\")\n",
    "        # print(f\"input.shape:\\n{input.shape}\")\n",
    "        W_xh = nn.Dense(self.hidden_size)(input)\n",
    "        print(f\"W_xh:\\n{W_xh.shape}\")\n",
    "        # W_xh = self.dense_o(name=\"dense rnn_wxh layer\")(input)\n",
    "        print(f\"W_hh shape:\\n{W_hh.shape}\")\n",
    "        print(f\"W_xh shape:\\n{W_xh.shape}\")\n",
    "        h_t = nn.relu(W_hh + W_xh)  # H_{t} = tanh(H_{t-1} @ W_{hh}) + (x_{t} @ W_{xh})\n",
    "        print(f\"inside the rnn update in an rnn, h_t:\\n{h_t.shape}\")\n",
    "        \n",
    "        return h_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(rng, batch_size, hidden_size, init_fn=nn.initializers.zeros):\n",
    "        \"\"\"Initialize the RNN cell carry.\n",
    "        Args:\n",
    "        rng: random number generator passed to the init_fn.\n",
    "        batch_dims: a tuple providing the shape of the batch dimensions.\n",
    "        hidden_size: the size or number of features of the memory.\n",
    "        init_fn: initializer function for the carry.\n",
    "        Returns:\n",
    "        An initialized carry for the given RNN cell.\n",
    "        \"\"\"\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(RNNCellBase):\n",
    "    hidden_size: int\n",
    "\n",
    "    @partial(\n",
    "        nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            i_{t} = sigmoid((W_{ii} @ x_{t} + b_{ii}) + (W_{hi} @ h_{t-1} + b_{hi}))\n",
    "            f_{t} = sigmoid((W_{if} @ x_{t} + b_{if}) + (W_{hf} @ h_{t-1} + b_{hf}))\n",
    "            g_{t} = tanh((W_{ig} @ x_{t} + b_{ig}) + (W_{hg} @ h_{t-1} + b_{hg}))\n",
    "            o_{t} = sigmoid((W_{io} @ x_{t} + b_{io}) + (W_{ho} @ h_{t-1} + b_{ho}))\n",
    "            c_{t} = f_{t} * c_{t-1} + i_{t} * g_{t}\n",
    "            h_{t} = o_{t} * tanh(c_{t})\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): hidden state size\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        print(f\"inside the LSTMCell, input:\\n{input.shape}\")\n",
    "        print(f\"inside the LSTMCell, input type:\\n{type(input)}\")\n",
    "        \n",
    "        print(f\"inside the LSTMCell, carry:\\n{carry.shape}\")\n",
    "        print(f\"inside the LSTMCell, carry type:\\n{type(carry)}\")\n",
    "        ht_1, ct_1 = carry\n",
    "        print(f\"carry split:\\n{ht_1.shape}\\n{ct_1.shape}\")\n",
    "\n",
    "        c_t, h_t = self.rnn_update(input, ht_1, ct_1)\n",
    "        print(f\"inside the LSTMCell, c_t:\\n{c_t.shape}\")\n",
    "        print(f\"inside the LSTMCell, h_t:\\n{h_t.shape}\")\n",
    "\n",
    "        return (h_t, c_t), h_t\n",
    "\n",
    "    # @partial(\n",
    "    #     nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    # )\n",
    "    def rnn_update(self, input, ht_1, ct_1):\n",
    "        print(f\"inside the LSTMCell rnn_update, input:\\n{input.shape}\")\n",
    "        print(f\"inside the LSTMCell rnn_update, ht_1:\\n{ht_1.shape}\")\n",
    "        print(f\"inside the LSTMCell rnn_update, ct_1:\\n{ct_1.shape}\")\n",
    "        # i_ta = partial(\n",
    "        #     nn.Dense,\n",
    "        #     features=ht_1.shape()[0],\n",
    "        #     use_bias=False,\n",
    "        #     kernel_init=self.recurrent_kernel_init,\n",
    "        #     bias_init=self.bias_init,\n",
    "        # )\n",
    "\n",
    "        i_ta = nn.Dense(features=ht_1.shape()[0])(input)\n",
    "        i_tb = nn.Dense(features=self.hidden_size)(ht_1)\n",
    "        i_t = nn.sigmoid(i_ta + i_tb)  # input gate\n",
    "        print(f\"inside the LSTMCell, input gate output:\\n{i_t.shape}\")\n",
    "\n",
    "        o_ta = nn.Dense(self.hidden_size)(input)\n",
    "        o_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        o_t = nn.sigmoid(o_ta + o_tb)  # output gate\n",
    "        print(f\"inside the LSTMCell, output gate output:\\n{o_t.shape}\")\n",
    "\n",
    "        f_ia = nn.Dense(self.hidden_size)(\n",
    "            input\n",
    "        )  # b^{f}_{i} + \\sum\\limits_{j} U^{f}_{i, j} x^{t}_{j}\n",
    "        f_ib = nn.Dense(self.hidden_size)(\n",
    "            ht_1\n",
    "        )  # \\sum\\limits_{j} W^{f}_{i, j} h^{(t-1)}_{j}\n",
    "        f_i = nn.sigmoid(f_ia + f_ib)  # forget gate\n",
    "        print(f\"inside the LSTMCell, forget gate output:\\n{f_i.shape}\")\n",
    "\n",
    "        g_ia = nn.Dense(self.hidden_size)(\n",
    "            input\n",
    "        )  # b^{g}_{i} + \\sum\\limits_{j} U^{g}_{i, j} x^{t}_{j}\n",
    "        g_ib = nn.Dense(self.hidden_size)(\n",
    "            ht_1\n",
    "        )  # \\sum\\limits_{j} W^{g}_{i, j} h^{(t-1)}_{j}\n",
    "        g_i = nn.tanh(g_ia + g_ib)  # (external) input gate\n",
    "        print(f\"inside the LSTMCell, (external) input gate output:\\n{g_i.shape}\")\n",
    "\n",
    "        c_t = (f_i * ct_1) + (i_t * g_i)  # internal cell state update\n",
    "        print(f\"inside the LSTMCell, cell state output:\\n{c_t.shape}\")\n",
    "\n",
    "        h_t = o_t * nn.tanh(c_t)  # hidden state update\n",
    "        print(f\"inside the LSTMCell, hidden state output:\\n{h_t.shape}\")\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(rng, batch_size, hidden_size, init_fn=nn.initializers.zeros):\n",
    "        \"\"\"Initialize the RNN cell carry.\n",
    "        Args:\n",
    "        rng: random number generator passed to the init_fn.\n",
    "        batch_dims: a tuple providing the shape of the batch dimensions.\n",
    "        hidden_size: the size or number of features of the memory.\n",
    "        init_fn: initializer function for the carry.\n",
    "        Returns:\n",
    "        An initialized carry for the given RNN cell.\n",
    "        \"\"\"\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(RNNCellBase):\n",
    "    hidden_size: int\n",
    "\n",
    "    @partial(\n",
    "        nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            z_t = sigmoid((W_{iz} @ x_{t} + b_{iz}) + (W_{hz} @ h_{t-1} + b_{hz}))\n",
    "            r_t = sigmoid((W_{ir} @ x_{t} + b_{ir}) + (W_{hr} @ h_{t-1} + b_{hr}))\n",
    "            g_t = tanh(((W_{ig} @ x_{t} + b_{ig}) + r_t) * (W_{hg} @ h_{t-1} + b_{hg}))\n",
    "            h_t = (z_t * h_{t-1}) + ((1 - z_t) * g_i)\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): hidden state size\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "        ht_1 = carry\n",
    "\n",
    "        h_t = self.rnn_update(input, ht_1)\n",
    "\n",
    "        return (h_t, h_t), h_t\n",
    "\n",
    "    # @partial(\n",
    "    #     nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    # )\n",
    "    def rnn_update(self, input, ht_1):\n",
    "\n",
    "        z_ta = nn.Dense(self.hidden_size)(input)\n",
    "        z_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        z_t = nn.sigmoid(z_ta + z_tb)  # reset gate\n",
    "\n",
    "        r_ta = nn.Dense(self.hidden_size)(input)\n",
    "        r_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        r_t = nn.sigmoid(r_ta + r_tb)  # update gate\n",
    "\n",
    "        g_ta = nn.Dense(self.hidden_size)(input)\n",
    "        g_tb = nn.Dense(self.hidden_size)(ht_1)\n",
    "        g_t = nn.tanh((g_ta + r_t) * g_tb)  # (external) input gate\n",
    "\n",
    "        h_t = ((1 - z_t) * ht_1) + (z_t * g_t)  # internal cell state update\n",
    "\n",
    "        return h_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(rng, batch_size, hidden_size, init_fn=nn.initializers.zeros):\n",
    "        \"\"\"Initialize the RNN cell carry.\n",
    "        Args:\n",
    "        rng: random number generator passed to the init_fn.\n",
    "        batch_dims: a tuple providing the shape of the batch dimensions.\n",
    "        hidden_size: the size or number of features of the memory.\n",
    "        init_fn: initializer function for the carry.\n",
    "        Returns:\n",
    "        An initialized carry for the given RNN cell.\n",
    "        \"\"\"\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPOCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        RNN update function\n",
    "        τ(h, x) = (1 - g(h, x)) ◦ h + g(h, x) ◦ tanh(Lτ (h, x))\n",
    "        g(h, x) = σ(Lg(h,x))\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): hidden state size\n",
    "        output_size (int): output size\n",
    "        hippo (HiPPO): hippo model object\n",
    "        cell (RNNCellBase): choice of RNN cell object\n",
    "            - RNNCell\n",
    "            - LSTMCell\n",
    "            - GRUCell\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "    hippo: HiPPO\n",
    "    model: RNNCellBase\n",
    "\n",
    "    # def setup(self):\n",
    "    #     self.dense_y = Partial(\n",
    "    #         nn.Dense,\n",
    "    #         features=self.output_size,\n",
    "    #         use_bias=True,\n",
    "    #         kernel_init=nn.initializers.orthogonal(),\n",
    "    #         bias_init=nn.initializers.zeros,\n",
    "    #         dtype=None,\n",
    "    #         param_dtype=jnp.float32,\n",
    "    #     )\n",
    "    #     self.cell = self.model(self.hidden_size)\n",
    "\n",
    "    # @partial(\n",
    "    #     nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    # )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, input):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            RNN update function\n",
    "            τ(h, x) = (1 - g(h, x)) ◦ h + g(h, x) ◦ tanh(Lτ (h, x))\n",
    "            g(h, x) = σ(Lg(h,x))\n",
    "\n",
    "        Args:\n",
    "            carry (jnp.ndarray): hidden state from previous time step\n",
    "            input (jnp.ndarray): # input vector\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the new carry and the output.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"inside hippo cell, input:\\n{input}\")\n",
    "        h_t, c_t = carry\n",
    "        print(f\"inside hippo cell, h_t:\\n{h_t}\\nc_t:\\n{c_t}\")\n",
    "        print(f\"inside hippo cell, the cell:\\n{self.model}\")\n",
    "        _, h_t = self.model(self.hidden_size)(carry, input)\n",
    "        print(f\"inside hippo cell, h_t:\\n{h_t.shape}\")\n",
    "\n",
    "        # y_t = nn.Dense(self.output_size)(h_t)  # f_t in the paper\n",
    "        # print(f\"inside hippo cell, y_t: \\n{y_t}\")\n",
    "\n",
    "        # c_t = self.hippo(y_t, init_state=None, kernel=False)\n",
    "        # print(f\"inside hippo cell, c_t: \\n{c_t}\")\n",
    "\n",
    "        return self.rnn_update(input, h_t)\n",
    "\n",
    "    # @partial(\n",
    "    #     nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    # )\n",
    "    def rnn_update(self, input, h_t):\n",
    "\n",
    "        # y_t = self.dense_y(name=\"dense hippo input layer\")(h_t)\n",
    "        print(f\"inside hippo cell in the update, h_t:\\n{h_t.shape}\")\n",
    "        print(f\"inside hippo cell in the update, input:\\n{input.shape}\")\n",
    "        y_t = nn.Dense(1)(h_t)  # f_t in the paper\n",
    "        print(f\"inside hippo cell, before reshape y_t: \\n{y_t.shape}\")\n",
    "        y_t = jnp.swapaxes(y_t, 1, 0)\n",
    "        y_t = jnp.swapaxes(y_t, 2, 1)\n",
    "        print(f\"inside hippo cell, y_t: \\n{y_t.shape}\")\n",
    "        \n",
    "        c_t = self.hippo(y_t, init_state=h_t, kernel=False)\n",
    "        print(f\"inside hippo cell, c_t: \\n{c_t}\")\n",
    "        print(f\"inside hippo cell, c_t: \\n{c_t.shape}\")\n",
    "\n",
    "        return (h_t, c_t), h_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(rng, batch_size, hidden_size, init_fn=nn.initializers.zeros):\n",
    "        \"\"\"Initialize the RNN cell carry.\n",
    "        Args:\n",
    "        rng: random number generator passed to the init_fn.\n",
    "        batch_dims: a tuple providing the shape of the batch dimensions.\n",
    "        hidden_size: the size or number of features of the memory.\n",
    "        init_fn: initializer function for the carry.\n",
    "        Returns:\n",
    "        An initialized carry for the given RNN cell.\n",
    "        \"\"\"\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refer to https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/recurrent.py#L714-L762\n",
    "# also refer to https://dm-haiku.readthedocs.io/en/latest/api.html?highlight=DeepRNN#deeprnn\n",
    "class _DeepRNN(RNNCellBase):\n",
    "    hidden_size: int\n",
    "    layers: Sequence[Any]\n",
    "    skip_connections: bool\n",
    "    hidden_to_output_layer: bool\n",
    "    layer_name: Optional[str]\n",
    "\n",
    "    def setup(self):\n",
    "        if self.skip_connections:\n",
    "            for layer in self.layers:\n",
    "                if not (isinstance(layer, RNNCellBase) or isinstance(layer, HiPPOCell)):\n",
    "                    raise ValueError(\n",
    "                        \"skip_connections requires for all layers to be \"\n",
    "                        \"`hk.RNNCore`s. Layers is: {}\".format(self.layers)\n",
    "                    )\n",
    "                    # raise ValueError(\n",
    "                    #     f\"{self.layer_name} layer {layer} is not a RNNCellBase or HiPPOCell\"\n",
    "                    # )\n",
    "\n",
    "    def __call__(self, carry, inputs):\n",
    "        current_carry = carry\n",
    "        next_states = []\n",
    "        h_t_outputs = []\n",
    "        c_t_outputs = []\n",
    "        state_idx = 0\n",
    "        # print(f\"inside deep rnn, inputs:\\n{inputs.shape}\")\n",
    "        # print(f\"inside deep rnn, carry:\\n{carry.shape}\")\n",
    "        h_t, c_t = carry  # c_t may actually be h_t in which case dont use it\n",
    "        (\n",
    "            h_t_copy,\n",
    "            c_t_copy,\n",
    "        ) = current_carry  # c_t may actually be h_t in which case dont use it\n",
    "        concat = lambda *args: jnp.concatenate(args, axis=-1)\n",
    "        print(f\"before main loop\")\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            print(f\"inside deep rnn cell, h_t:\\n{h_t.shape}\\nc_t:\\n{c_t.shape}\")\n",
    "            print(f\"inside deep rnn cell, THE INPUTS:\\n{inputs[idx].shape}\")\n",
    "            if self.skip_connections and idx > 0:\n",
    "                skip_h_t = jax.tree_map(concat, h_t, h_t_copy)\n",
    "                skip_c_t = jax.tree_map(concat, c_t, c_t_copy)\n",
    "                current_carry = tuple([skip_h_t, skip_c_t])\n",
    "\n",
    "            if isinstance(layer, RNNCellBase) or isinstance(layer, HiPPOCell):\n",
    "                # print(f\"inside deep rnn, inputs:\\n{inputs}\")\n",
    "                # print(f\"inside deep rnn, state_idx:\\n{state_idx}\")\n",
    "                print(f\"inside deep rnn, inputs[state_idx]:\\n{inputs[state_idx].shape}\")\n",
    "                h_t, c_t, next_state = layer(\n",
    "                    current_carry, inputs[state_idx]\n",
    "                )  # problem line\n",
    "                h_t_outputs.append(h_t)\n",
    "                c_t_outputs.append(c_t)\n",
    "                next_states.append(next_state)\n",
    "                state_idx += 1\n",
    "\n",
    "            else:\n",
    "                print(f\"current_carry before layer: {current_carry.shape}\")\n",
    "                print(f\"layer: {layer}\")\n",
    "                current_carry = layer(current_carry)\n",
    "                print(f\"current_carry:\\n {current_carry.shape}\")\n",
    "\n",
    "        print(f\"third conditional\")\n",
    "        if self.skip_connections:\n",
    "            skip_h_t_out = jax.tree_map(concat, *h_t_outputs)\n",
    "            skip_c_t_out = jax.tree_map(concat, *c_t_outputs)\n",
    "            next_carry = (skip_h_t_out, skip_c_t_out)\n",
    "        else:\n",
    "            next_carry = current_carry\n",
    "\n",
    "        print(f\"next_states before tuple:\\n\", next_states)\n",
    "        pp.pprint(layer)\n",
    "        print(f\"carry before return B:\\n\", next_carry)\n",
    "\n",
    "        return next_carry, next_states\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_state(\n",
    "        num_layers,\n",
    "        rng,\n",
    "        batch_size: tuple,\n",
    "        output_size: int,\n",
    "        init_fn=nn.initializers.zeros,\n",
    "    ):\n",
    "        states = []\n",
    "        for i in range(num_layers):\n",
    "            print(f\"Layer: {i}\\n\")\n",
    "            states.append(\n",
    "                _DeepRNN.init_state(\n",
    "                    rng=rng,\n",
    "                    batch_size=batch_size,\n",
    "                    output_size=output_size,\n",
    "                    init_fn=init_fn,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return states\n",
    "\n",
    "    @staticmethod\n",
    "    def init_state(\n",
    "        rng, batch_size: tuple, output_size: int, init_fn=nn.initializers.zeros\n",
    "    ):\n",
    "        print(f\"batch_size: {(batch_size,)}\")\n",
    "        print(f\"output_size: {(output_size,)}\")\n",
    "        mem_shape = (batch_size,) + (output_size,)\n",
    "        print(f\"state mem_shape: {mem_shape}\")\n",
    "\n",
    "        return init_fn(rng, mem_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(\n",
    "        rng, batch_size: tuple, hidden_size: int, init_fn=nn.initializers.zeros\n",
    "    ):\n",
    "        print(f\"batch_size: {batch_size}\")\n",
    "        print(f\"hidden_size: {(hidden_size,)}\")\n",
    "        mem_shape = batch_size + (1, hidden_size)\n",
    "        print(f\"carry mem_shape: {mem_shape}\")\n",
    "\n",
    "        return init_fn(rng, mem_shape), init_fn(rng, mem_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRNN(_DeepRNN):\n",
    "    r\"\"\"Wraps a sequence of cores and callables as a single core.\n",
    "        >>> deep_rnn = hk.DeepRNN([\n",
    "        ...     LSTMCell(hidden_size=4),\n",
    "        ...     jax.nn.relu,\n",
    "        ...     LSTMCell(hidden_size=2),\n",
    "        ... ])\n",
    "    The state of a :class:`DeepRNN` is a tuple with one element per\n",
    "    :class:`RNNCore`. If no layers are :class:`RNNCore`\\ s, the state is an empty\n",
    "    tuple.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        layers: Sequence[Any],\n",
    "        skip_connections: Optional[bool] = False,\n",
    "        hidden_to_output_layer: Optional[bool] = False,\n",
    "        name: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            hidden_size=hidden_size,\n",
    "            layers=layers,\n",
    "            skip_connections=skip_connections,\n",
    "            hidden_to_output_layer=hidden_to_output_layer,\n",
    "            layer_name=name,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Training Types\n",
    "refer to [this](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data_2_id(iterable):\n",
    "    \"\"\"\n",
    "    provides mapping to and from ids\n",
    "    \"\"\"\n",
    "    \n",
    "    id_2_data = {}\n",
    "    data_2_id = {}\n",
    "\n",
    "    for id, elem in enumerate(iterable):\n",
    "        id_2_data[id] = elem\n",
    "\n",
    "    for id, elem in enumerate(iterable):\n",
    "        data_2_id[elem] = id\n",
    "\n",
    "    return (id_2_data, data_2_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(i, n):\n",
    "    \"\"\"\n",
    "    create vector of size n with 1 at index i\n",
    "    \"\"\"\n",
    "    x = jnp.zeros(n, dtype=int)\n",
    "    print(f\"i:\\n{i}\")\n",
    "    print(f\"type(i):\\n{type(i)}\")\n",
    "    print(f\"n:\\n{n}\")\n",
    "    print(f\"type(n):\\n{type(n)}\")\n",
    "    x = x.at[i].set(1) #jax.vmap(x.at[i].set(1), in_axes=(0, 0), out_axes=0)(i, x)\n",
    "\n",
    "    print(f\"i:\\n{i}\")\n",
    "    print(f\"type(x):\\n{type(x)}\")\n",
    "    print(f\"x:\\n{x}\")\n",
    "    # array = x.at[i].set(1)\n",
    "    # print(f\"array:\\n{array}\")\n",
    "    # x = x[i].at[i].set(1)\n",
    "    # print(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(fname):\n",
    "  with open(fname, \"r\") as reader:\n",
    "    data = reader.read()\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "  chars = list(set(data))\n",
    "  vocab_size = len(chars)\n",
    "  char_to_id, id_to_char = map_data_2_id(chars)\n",
    "  char_to_id = {value:key for key, value in char_to_id.items()}\n",
    "  # data converted to ids\n",
    "  # data_id = [char_to_id[char] for char in data]\n",
    "  data_id = [char_to_id[char] for char in data]\n",
    "  return data_id, char_to_id, id_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 0, 1, 1, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"abcd...abcd...\"\n",
    "data_id, char_to_id, id_to_char = prep_data(data)\n",
    "data_id[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(char):\n",
    "    return one_hot(data_2_id[char], len(data_2_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(predictions, id_2_data):\n",
    "    return id_2_data[int(jnp.argmax(predictions))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check [this](https://github.com/deepmind/optax/blob/master/examples/quick_start.ipynb) out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_optimizer_fn(starting_learning_rate, name=\"adam\"):\n",
    "    # refer to https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules\n",
    "    optim = None\n",
    "\n",
    "    if name == \"sgd\":\n",
    "        optim = optax.sgd(starting_learning_rate)\n",
    "    elif name == \"adam\":\n",
    "        optim = optax.adam(\n",
    "            starting_learning_rate,\n",
    "        )\n",
    "    elif name == \"adagrad\":\n",
    "        optim = optax.adagrad(starting_learning_rate)\n",
    "    elif name == \"rmsprop\":\n",
    "        optim = optax.rmsprop(starting_learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"optimizer name not recognized\")\n",
    "\n",
    "    return optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_scheduler_fn(\n",
    "    start_learning_rate, steps, decay_rate, init_value, end_val, name\n",
    "):\n",
    "    # refer to https://optax.readthedocs.io/en/latest/api.html#schedules\n",
    "    scheduler = None\n",
    "\n",
    "    if name == \"constant\":\n",
    "        scheduler = optax.constant_schedule(init_value)\n",
    "\n",
    "    elif name == \"exp_decay\":\n",
    "        scheduler = optax.exponential_decay(\n",
    "            init_value=start_learning_rate, transition_steps=1000, decay_rate=0.99\n",
    "        )\n",
    "    elif name == \"linear\":\n",
    "        scheduler = optax.linear_schedule(init_value=init_value, end_value=end_val)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"scheduler name not recognized\")\n",
    "\n",
    "    return scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add transformations\n",
    "# refer to https://optax.readthedocs.io/en/latest/api.html#optax-transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # A simple update loop.\n",
    "#     for _ in range(1000):\n",
    "#     grads = jax.grad(compute_loss)(params, xs, ys)\n",
    "#     updates, opt_state = gradient_transform.update(grads, opt_state)\n",
    "#     params = optax.apply_updates(params, updates)\n",
    "\n",
    "#     assert jnp.allclose(params, target_params), \\\n",
    "#     'Optimization should retrieve the target params used to generate the data.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimizer_fn(start_learning_rate, params, num_weights, x, y):\n",
    "#     optimizer = optax.adam(start_learning_rate)\n",
    "#     # Obtain the `opt_state` that contains statistics for the optimizer.\n",
    "#     params = {'w': jnp.ones((num_weights,))}\n",
    "#     opt_state = optimizer.init(params)\n",
    "\n",
    "#     compute_loss = lambda params, x, y: optax.l2_loss(params['w'].dot(x), y)\n",
    "#     grads = jax.grad(compute_loss)(params, xs, ys)\n",
    "\n",
    "#     updates, opt_state = optimizer.update(grads, opt_state)\n",
    "#     params = optax.apply_updates(params, updates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    state_size: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int\n",
    "    hippo_order_N: int\n",
    "    batch_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        L = self.vocab_size\n",
    "\n",
    "        hippo = HiPPO(\n",
    "            N=self.hippo_order_N,\n",
    "            max_length=L,\n",
    "            measure=\"legs\",\n",
    "            step=1.0 / L,\n",
    "            GBT_alpha=0.5,\n",
    "            seq_L=L,\n",
    "            v=\"v\",\n",
    "            lambda_n=1.0,\n",
    "            fourier_type=\"fru\",\n",
    "            alpha=0.0,\n",
    "            beta=1.0,\n",
    "        )\n",
    "\n",
    "        cell1 = RNNCell\n",
    "        cell2 = RNNCell\n",
    "        cell3 = RNNCell\n",
    "\n",
    "        input_layers = [\n",
    "            HiPPOCell(\n",
    "                hippo=hippo, model=cell1, hidden_size=8, output_size=L\n",
    "            ),\n",
    "            HiPPOCell(\n",
    "                hippo=hippo, model=cell2, hidden_size=64, output_size=L\n",
    "            ),\n",
    "            HiPPOCell(\n",
    "                hippo=hippo, model=cell3, hidden_size=512, output_size=L\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self.deep_cell = DeepRNN(\n",
    "            hidden_size=self.hidden_size,\n",
    "            layers=input_layers,\n",
    "            skip_connections=True,\n",
    "            hidden_to_output_layer=False,\n",
    "            name=\"RNNLM\",\n",
    "        )\n",
    "\n",
    "    # @partial(\n",
    "    #     nn.transforms.scan, variable_broadcast=\"params\", split_rngs={\"params\": False}\n",
    "    # )\n",
    "    def __call__(self, carry, input):\n",
    "        # print(f\"i before one_hot:\\n{input}\")\n",
    "        onehot = lambda x: nn.one_hot(x, self.vocab_size)\n",
    "        # leaves, treedef = jax.tree_util.tree_flatten(input)\n",
    "        # print(f\"inside call function of RNNLM, leaves:\\n{leaves}\")\n",
    "\n",
    "        input = jax.tree_map(onehot, input)\n",
    "\n",
    "        # input = one_hot(input, self.vocab_size)\n",
    "        # print(f\"inside call function of RNNLM, input:\\n{input[0].shape}\")\n",
    "        # print(f\"inside call function of RNNLM, carry:\\n{carry.shape}\")\n",
    "        # deep_cell = lambda x: self.deep_cell(x, input)\n",
    "        print(f\"the deep_cell:\\n{self.deep_cell}\")\n",
    "        # carries, next_states = jax.tree_map(deep_cell, input)\n",
    "        carries, next_states = self.deep_cell(carry, input)\n",
    "        print(f\"inside call function of RNNLM, next_states:\\n{next_states}\")\n",
    "        print(f\"inside call function of RNNLM, carries:\\n{carries}\")\n",
    "        predictions = nn.softmax(nn.Dense(self.vocab_size)(next_states[-1]))\n",
    "        print(f\"inside call function of RNNLM, predictions:\\n{predictions}\")\n",
    "        return carries, next_states, predictions\n",
    "\n",
    "    # def __call__(self, carry, input):\n",
    "\n",
    "    #     print(f\"inside call function of RNNLM, input:\\n{input}\")\n",
    "    #     print(f\"inside call function of RNNLM, carry:\\n{carry}\")\n",
    "    #     onehot = lambda x: nn.one_hot(x, self.vocab_size)\n",
    "    #     carries, next_states = jax.pmap(self.deep_cell, axis_name=(1, None))(\n",
    "    #         carry, jax.tree_map(onehot, input)\n",
    "    #     )\n",
    "    #     print(f\"inside call function of RNNLM, next_states:\\n{next_states}\")\n",
    "    #     print(f\"inside call function of RNNLM, carries:\\n{carries}\")\n",
    "    #     predictions = nn.softmax(nn.Dense(self.vocab_size)(next_states[-1]))\n",
    "    #     print(f\"inside call function of RNNLM, predictions:\\n{predictions}\")\n",
    "    #     return carries, next_states, predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(\n",
    "        rng, batch_size: tuple, hidden_size: int, init_fn=nn.initializers.zeros\n",
    "    ):\n",
    "        return DeepRNN.initialize_carry(\n",
    "            rng=rng,\n",
    "            batch_size=batch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            init_fn=init_fn,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_state(\n",
    "        num_layers,\n",
    "        rng,\n",
    "        batch_size: tuple,\n",
    "        output_size: int,\n",
    "        init_fn=nn.initializers.zeros,\n",
    "    ):\n",
    "        return DeepRNN.initialize_state(\n",
    "            num_layers=num_layers,\n",
    "            rng=rng,\n",
    "            batch_size=batch_size,\n",
    "            output_size=output_size,\n",
    "            init_fn=init_fn,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, params, bridge, initial=\"\", max_length=100):\n",
    "    char_to_id, id_to_char = bridge\n",
    "    state = model.init_state()\n",
    "    output = initial\n",
    "    if len(initial) > 0:\n",
    "        for char in initial[:-1]:\n",
    "            _, state, _ = model.apply(params, char_to_id[char], state)\n",
    "\n",
    "    next_char = initial[-1]\n",
    "    for i in range(max_length):\n",
    "        state, predictions = model.apply(params, state, char_to_id[next_char], state)\n",
    "        next_char = decode(predictions, id_to_char)\n",
    "        output += next_char\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to [this](https://github.com/manifest/flax-extra/blob/48efe1f1515893289b44646977bf5049a340b6c8/docs/notebooks/combinators.ipynb), [this](https://github.com/romanak/pyprobml/blob/65c82b9b43d2100cbc7c59e766161ee801c0f85f/notebooks/book1/15/rnn_jax.ipynb), [this](https://github.com/probml/pyprobml/blob/71d98dcdd3798525353eb1bfb9851b47e9d64bde/notebooks/book1/15/rnn_jax.ipynb) and [this](https://github.com/probml/probml-notebooks/blob/36cb173afce3f4a07a7b475cf8a7937025a60465/notebooks-d2l/rnn_jax.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_size:\n",
      "5\n",
      "Create Model: RNNLM\n",
      "batch_size: (10,)\n",
      "hidden_size: (4,)\n",
      "carry mem_shape: (10, 1, 4)\n",
      "carry init:\n",
      "(DeviceArray([[[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]]], dtype=float32), DeviceArray([[[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]],\n",
      "\n",
      "             [[0., 0., 0., 0.]]], dtype=float32))\n",
      "Layer: 0\n",
      "\n",
      "batch_size: (10,)\n",
      "output_size: (5,)\n",
      "state mem_shape: (10, 5)\n",
      "Layer: 1\n",
      "\n",
      "batch_size: (10,)\n",
      "output_size: (5,)\n",
      "state mem_shape: (10, 5)\n",
      "Layer: 2\n",
      "\n",
      "batch_size: (10,)\n",
      "output_size: (5,)\n",
      "state mem_shape: (10, 5)\n",
      "state init:\n",
      "[DeviceArray([[0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.]], dtype=float32), DeviceArray([[0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.]], dtype=float32), DeviceArray([[0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.],\n",
      "             [0., 0., 0., 0., 0.]], dtype=float32)]\n",
      "Init Model: RNNLM\n",
      "batch_size: (10,)\n",
      "hidden_size: (4,)\n",
      "carry mem_shape: (10, 1, 4)\n",
      "Layer: 0\n",
      "\n",
      "batch_size: (10,)\n",
      "output_size: (5,)\n",
      "state mem_shape: (10, 5)\n",
      "Layer: 1\n",
      "\n",
      "batch_size: (10,)\n",
      "output_size: (5,)\n",
      "state mem_shape: (10, 5)\n",
      "Layer: 2\n",
      "\n",
      "batch_size: (10,)\n",
      "output_size: (5,)\n",
      "state mem_shape: (10, 5)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'v'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 29\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m     25\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstate init:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39minitialize_state(num_layers\u001b[39m=\u001b[39mnum_layers,rng\u001b[39m=\u001b[39mrng,batch_size\u001b[39m=\u001b[39mbatch_size,output_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(char_to_id), init_fn\u001b[39m=\u001b[39mnn\u001b[39m.\u001b[39minitializers\u001b[39m.\u001b[39mzeros,)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInit Model: RNNLM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49minit(\n\u001b[1;32m     30\u001b[0m     subkey,\n\u001b[1;32m     31\u001b[0m     model\u001b[39m.\u001b[39;49minitialize_carry(\n\u001b[1;32m     32\u001b[0m         rng\u001b[39m=\u001b[39;49mkey,\n\u001b[1;32m     33\u001b[0m         batch_size\u001b[39m=\u001b[39;49m(batch_size,),\n\u001b[1;32m     34\u001b[0m         hidden_size\u001b[39m=\u001b[39;49mhidden_size,\n\u001b[1;32m     35\u001b[0m         init_fn\u001b[39m=\u001b[39;49mnn\u001b[39m.\u001b[39;49minitializers\u001b[39m.\u001b[39;49mzeros,\n\u001b[1;32m     36\u001b[0m     ),\n\u001b[1;32m     37\u001b[0m     model\u001b[39m.\u001b[39;49minitialize_state(\n\u001b[1;32m     38\u001b[0m         num_layers\u001b[39m=\u001b[39;49mnum_layers,\n\u001b[1;32m     39\u001b[0m         rng\u001b[39m=\u001b[39;49msubkey,\n\u001b[1;32m     40\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     41\u001b[0m         output_size\u001b[39m=\u001b[39;49moutput_size,\n\u001b[1;32m     42\u001b[0m         init_fn\u001b[39m=\u001b[39;49mnn\u001b[39m.\u001b[39;49minitializers\u001b[39m.\u001b[39;49mzeros,\n\u001b[1;32m     43\u001b[0m     ),\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel state size: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mstate_size\u001b[39m}\u001b[39;00m\u001b[39m, vocab size: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mvocab_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[39m# output: Model state size: 8, vocab size: 5\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m# run a single example through the model to test that it works\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [25], line 11\u001b[0m, in \u001b[0;36mRNNLM.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m      9\u001b[0m     L \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size\n\u001b[0;32m---> 11\u001b[0m     hippo \u001b[39m=\u001b[39m HiPPO(\n\u001b[1;32m     12\u001b[0m         N\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhippo_order_N,\n\u001b[1;32m     13\u001b[0m         max_length\u001b[39m=\u001b[39;49mL,\n\u001b[1;32m     14\u001b[0m         measure\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlegs\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m         step\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m \u001b[39m/\u001b[39;49m L,\n\u001b[1;32m     16\u001b[0m         GBT_alpha\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m         seq_L\u001b[39m=\u001b[39;49mL,\n\u001b[1;32m     18\u001b[0m         v\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m         lambda_n\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m         fourier_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfru\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     21\u001b[0m         alpha\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     25\u001b[0m     cell1 \u001b[39m=\u001b[39m RNNCell\n\u001b[1;32m     26\u001b[0m     cell2 \u001b[39m=\u001b[39m RNNCell\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'v'"
     ]
    }
   ],
   "source": [
    "# randomness is handled using explicit keys in Jax\n",
    "key, subkey = jax.random.split(subkey)\n",
    "\n",
    "num_layers = 3\n",
    "state_size = 8\n",
    "hidden_size = 4\n",
    "output_size = len(char_to_id)\n",
    "print(f\"output_size:\\n{output_size}\")\n",
    "#hippo_order_N = 16\n",
    "batch_size = 10\n",
    "\n",
    "print(f\"Create Model: RNNLM\")\n",
    "# model = CharRNN(state_size, len(char_to_id))\n",
    "model = RNNLM(\n",
    "    state_size=state_size,\n",
    "    vocab_size=output_size,\n",
    "    hidden_size=hidden_size,\n",
    "    hippo_order_N=output_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "print(\n",
    "    f\"carry init:\\n{model.initialize_carry(rng=subkey, batch_size=(batch_size,), hidden_size=hidden_size, init_fn=nn.initializers.zeros)}\"\n",
    ")\n",
    "print(\n",
    "    f\"state init:\\n{model.initialize_state(num_layers=num_layers,rng=rng,batch_size=batch_size,output_size=len(char_to_id), init_fn=nn.initializers.zeros,)}\"\n",
    ")\n",
    "\n",
    "print(f\"Init Model: RNNLM\")\n",
    "params = model.init(\n",
    "    subkey,\n",
    "    model.initialize_carry(\n",
    "        rng=key,\n",
    "        batch_size=(batch_size,),\n",
    "        hidden_size=hidden_size,\n",
    "        init_fn=nn.initializers.zeros,\n",
    "    ),\n",
    "    model.initialize_state(\n",
    "        num_layers=num_layers,\n",
    "        rng=subkey,\n",
    "        batch_size=batch_size,\n",
    "        output_size=output_size,\n",
    "        init_fn=nn.initializers.zeros,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Model state size: {model.state_size}, vocab size: {model.vocab_size}\")\n",
    "# output: Model state size: 8, vocab size: 5\n",
    "\n",
    "# run a single example through the model to test that it works\n",
    "new_state, predictions = model.apply(params, model.initial_carry(), 0)\n",
    "assert predictions.shape[0] == model.vocab_size\n",
    "\n",
    "# calling sample on random model leads to random output\n",
    "sample(model, params, (char_to_id, id_to_char), \"abc\", max_length=10)\n",
    "# output: 'abcadbaadbadd'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.10 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def chunker(seq, size):\n",
    "    \"\"\"\n",
    "    chunks a sequences into two subsequences\n",
    "    one for inputs, another for targets, by\n",
    "    shifting the input by 1\n",
    "    \"\"\"\n",
    "    n = len(seq)\n",
    "    p = 0\n",
    "    while p + 1 <= n:\n",
    "        # ensure the last chunk is of equal size\n",
    "        yield seq[p : min(n - 1, p + size)], seq[(p + 1) : (p + size + 1)]\n",
    "        p += size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.10 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def rnn_loss(params, model, carries, inputs, targets):\n",
    "    # use lax.scan to efficiently generate a loop over the inputs\n",
    "    # this function returns the final state, and predictions for every step\n",
    "    # note: scan input array needs have shape [length, 1]\n",
    "    final_state, predictions = jax.lax.scan(\n",
    "        lambda carry, input: model.apply(params, carry, input), carries, np.array([inputs]).T\n",
    "    )\n",
    "    loss = np.mean(jax.vmap(optax.softmax_cross_entropy)(predictions, np.array([targets]).T))\n",
    "    return loss, final_state\n",
    "\n",
    "\n",
    "# we want both the loss an gradient, we set has_aux because rnn_loss also return final state\n",
    "# use static_argnums=1 to indicate that the model is static;\n",
    "# a different model input will require recomplication\n",
    "# finally, we jit the function to improve runtime\n",
    "rnn_loss_grad = jax.jit(jax.value_and_grad(rnn_loss, has_aux=True), static_argnums=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.10 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def batch_step(model, optimizer, state, inputs, targets):\n",
    "    (loss, state), grad = rnn_loss_grad(optimizer.target, model, state, inputs, targets)\n",
    "    new_optimizer = optimizer.apply_gradient(grad)\n",
    "    return new_optimizer, loss, state\n",
    "\n",
    "\n",
    "def epoch_step(model, optimizer, data, batch_size):\n",
    "    state = model.init_state()\n",
    "    total_loss = 0\n",
    "    for n, (inputs, targets) in enumerate(chunker(data, batch_size)):\n",
    "        optimizer, loss, state = batch_step(model, optimizer, state, inputs, targets)\n",
    "\n",
    "        total_loss += loss\n",
    "    return optimizer, total_loss / (n + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.10 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('s4mer-pkg-jZnBSgjq-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a81e05d1d7f7eae781698b7c1b81c0d771335201ebad1d81045cb177cef974b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
