{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../../../\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 17:56:48.057098: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 17:56:48.169242: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-15 17:56:48.752131: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-11-15 17:56:48.752205: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-11-15 17:56:48.752212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.linen.activation import tanh\n",
    "from flax.linen.activation import sigmoid\n",
    "from flax.linen.initializers import zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from dataclasses import field\n",
    "from typing import Any, Callable, Optional, Sequence\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "import wandb\n",
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.hippo.hippo import HiPPO\n",
    "from src.models.hippo.transition import TransMatrix\n",
    "from src.data.process import moving_window, rolling_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        W_xh = x_{t} @ W_{xh} - multiply the previous hidden state with\n",
    "        W_hh = H_{t-1} @ W_{hh} + b_{h} - this a linear layer\n",
    "\n",
    "        H_{t} = f_{w}(H_{t-1}, x)\n",
    "        H_{t} = \\phi(H_{t-1} @ W_{hh}) + (x_{t} @ W_{xh})\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    input_size: int\n",
    "    hidden_size: int\n",
    "    bias: bool = True\n",
    "    param_dtype: Any = jnp.float32\n",
    "    activation_fn: Callable[..., Any] = tanh\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense_i = nn.Dense(\n",
    "            self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "        self.dense_h = nn.Dense(\n",
    "            self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "\n",
    "    def __call__(self, carry, input):\n",
    "        ht_1, _ = carry\n",
    "\n",
    "        w_hh = self.dense_h(ht_1)\n",
    "        w_xh = self.dense_i(input)\n",
    "\n",
    "        h_t = self.activation_fn(\n",
    "            (w_hh + w_xh)\n",
    "        )  # H_{t} = tanh(H_{t-1} @ W_{hh}) + (x_{t} @ W_{xh})\n",
    "\n",
    "        return (h_t, h_t), h_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(\n",
    "        rng,\n",
    "        batch_size: tuple,\n",
    "        hidden_size: int,\n",
    "        init_fn=nn.initializers.zeros,\n",
    "    ):\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        i_{t} = sigmoid((W_{ii} @ x_{t} + b_{ii}) + (W_{hi} @ h_{t-1} + b_{hi}))\n",
    "        f_{t} = sigmoid((W_{if} @ x_{t} + b_{if}) + (W_{hf} @ h_{t-1} + b_{hf}))\n",
    "        g_{t} = tanh((W_{ig} @ x_{t} + b_{ig}) + (W_{hg} @ h_{t-1} + b_{hg}))\n",
    "        o_{t} = sigmoid((W_{io} @ x_{t} + b_{io}) + (W_{ho} @ h_{t-1} + b_{ho}))\n",
    "        c_{t} = f_{t} * c_{t-1} + i_{t} * g_{t}\n",
    "        h_{t} = o_{t} * tanh(c_{t})\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): hidden state size\n",
    "        carry (jnp.ndarray): hidden state from previous time step\n",
    "        input (jnp.ndarray): # input vector\n",
    "\n",
    "    Returns:\n",
    "        A tuple with the new carry and the output.\n",
    "    \"\"\"\n",
    "\n",
    "    input_size: int\n",
    "    hidden_size: int\n",
    "    bias: bool = True\n",
    "    param_dtype: Any = jnp.float32\n",
    "    gate_fn: Callable[..., Any] = sigmoid\n",
    "    activation_fn: Callable[..., Any] = tanh\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense_i_ti = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "        self.dense_i_th = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "\n",
    "        self.dense_o_ti = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "        self.dense_o_th = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "\n",
    "        self.dense_f_ti = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "        self.dense_f_th = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "\n",
    "        self.dense_g_ti = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "        self.dense_g_th = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "\n",
    "    def __call__(self, carry, input):\n",
    "        h_t, c_t = carry\n",
    "\n",
    "        i_ti = self.dense_i_ti(input)\n",
    "        i_th = self.dense_i_th(h_t)\n",
    "        i_t = self.gate_fn(i_ti + i_th)\n",
    "\n",
    "        o_ti = self.dense_o_ti(input)\n",
    "        o_th = self.dense_o_th(h_t)\n",
    "        o_t = self.gate_fn(o_ti + o_th)\n",
    "\n",
    "        f_ti = self.dense_f_ti(input)\n",
    "        f_th = self.dense_f_th(h_t)\n",
    "        f_t = self.gate_fn(f_ti + f_th)\n",
    "\n",
    "        g_ti = self.dense_g_ti(input)\n",
    "        g_th = self.dense_g_th(h_t)\n",
    "        g_t = self.activation_fn(g_ti + g_th)\n",
    "\n",
    "        c_t = (f_t * c_t) + (i_t * g_t)\n",
    "        h_t = o_t * self.activation_fn(c_t)\n",
    "\n",
    "        return (h_t, c_t), h_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(\n",
    "        rng,\n",
    "        batch_size: tuple,\n",
    "        hidden_size: int,\n",
    "        init_fn=nn.initializers.zeros,\n",
    "    ):\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        z_t = sigmoid((W_{iz} @ x_{t} + b_{iz}) + (W_{hz} @ h_{t-1} + b_{hz}))\n",
    "        r_t = sigmoid((W_{ir} @ x_{t} + b_{ir}) + (W_{hr} @ h_{t-1} + b_{hr}))\n",
    "        n_t = tanh(((W_{in} @ x_{t} + b_{in}) + r_t) * (W_{hn} @ h_{t-1} + b_{hn}))\n",
    "        h_t = (z_t * h_{t-1}) + ((1 - z_t) * n_i)\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): hidden state size\n",
    "        carry (jnp.ndarray): hidden state from previous time step\n",
    "        input (jnp.ndarray): # input vector\n",
    "\n",
    "    Returns:\n",
    "        A tuple with the new carry and the output.\n",
    "    \"\"\"\n",
    "\n",
    "    input_size: int\n",
    "    hidden_size: int\n",
    "    bias: bool = True\n",
    "    param_dtype: Any = jnp.float32\n",
    "    gate_fn: Callable[..., Any] = sigmoid\n",
    "    activation_fn: Callable[..., Any] = tanh\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense_z_ti = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "        self.dense_z_th = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "\n",
    "        self.dense_r_ti = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "        self.dense_r_th = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "\n",
    "        self.dense_n_ti = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "        self.dense_n_th = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "\n",
    "    def __call__(self, carry, input):\n",
    "        h_t_1, h_t_1 = carry\n",
    "\n",
    "        z_ti = self.dense_z_ti(input)\n",
    "        z_th = self.dense_z_th(h_t_1)\n",
    "        z_t = self.gate_fn(z_ti + z_th)\n",
    "\n",
    "        r_ti = self.dense_r_ti(input)\n",
    "        r_th = self.dense_r_th(h_t_1)\n",
    "        r_t = self.gate_fn(r_ti + r_th)\n",
    "\n",
    "        n_ti = self.dense_n_ti(input)\n",
    "        n_th = self.dense_n_th(h_t_1)\n",
    "        n_t = self.activation_fn(n_ti + (r_t * n_th))\n",
    "\n",
    "        h_t = ((1 - z_t) * n_t) + (z_t * h_t_1)\n",
    "\n",
    "        return (h_t, h_t), h_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(\n",
    "        rng,\n",
    "        batch_size: tuple,\n",
    "        hidden_size: int,\n",
    "        init_fn=nn.initializers.zeros,\n",
    "    ):\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiPPOCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        z_t = sigmoid((W_{iz} @ x_{t} + b_{iz}) + (W_{hz} @ h_{t-1} + b_{hz}))\n",
    "        r_t = sigmoid((W_{ir} @ x_{t} + b_{ir}) + (W_{hr} @ h_{t-1} + b_{hr}))\n",
    "        n_t = tanh(((W_{in} @ x_{t} + b_{in}) + r_t) * (W_{hn} @ h_{t-1} + b_{hn}))\n",
    "        h_t = (z_t * h_{t-1}) + ((1 - z_t) * g_i)\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): hidden state size\n",
    "        carry (jnp.ndarray): hidden state from previous time step\n",
    "        input (jnp.ndarray): # input vector\n",
    "\n",
    "    Returns:\n",
    "        A tuple with the new carry and the output.\n",
    "    \"\"\"\n",
    "\n",
    "    input_size: int\n",
    "    hidden_size: int\n",
    "    bias: bool = True\n",
    "    param_dtype: Any = jnp.float32\n",
    "    gate_fn: Callable[..., Any] = sigmoid\n",
    "    activation_fn: Callable[..., Any] = tanh\n",
    "    measure: str = \"legs\"\n",
    "    lambda_n: float = 1.0\n",
    "    fourier_type: str = \"fru\"\n",
    "    alpha: float = 0.0\n",
    "    beta: float = 1.0\n",
    "    GBT_alpha: float = 0.5\n",
    "    rnn_cell: Callable[..., Any] = GRUCell\n",
    "\n",
    "    def setup(self):\n",
    "        hippo_matrices = TransMatrix(\n",
    "            N=self.hidden_size,\n",
    "            measure=self.measure,\n",
    "            lambda_n=self.lambda_n,\n",
    "            fourier_type=self.fourier_type,\n",
    "            alpha=self.alpha,\n",
    "            beta=self.beta,\n",
    "        )\n",
    "        A = hippo_matrices.A_matrix\n",
    "        B = hippo_matrices.B_matrix\n",
    "        L = self.input_size\n",
    "\n",
    "        self.hippo = HiPPO(\n",
    "            N=self.hidden_size,\n",
    "            max_length=L,\n",
    "            step=1.0 / L,\n",
    "            GBT_alpha=self.GBT_alpha,\n",
    "            seq_L=L,\n",
    "            A=A,\n",
    "            B=B,\n",
    "            measure=self.measure,\n",
    "        )\n",
    "\n",
    "        self.rnn = self.rnn_cell(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            bias=self.bias,\n",
    "            param_dtype=self.param_dtype,\n",
    "            gate_fn=self.gate_fn,\n",
    "            activation_fn=self.activation_fn,\n",
    "        )\n",
    "\n",
    "        self.dense_f_th = nn.Dense(\n",
    "            features=self.hidden_size, use_bias=self.bias, param_dtype=self.param_dtype\n",
    "        )\n",
    "\n",
    "    def __call__(self, carry, input):\n",
    "        _, c_t_1 = carry\n",
    "\n",
    "        carry, _ = self.rnn(carry, input)\n",
    "        h_t, _ = carry\n",
    "\n",
    "        f_t = self.dense_f_th(h_t)\n",
    "        c_t = self.hippo(f=f_t, init_state=c_t_1, t_step=f_t.shape[0], kernel=False)\n",
    "\n",
    "        return (h_t, c_t), h_t\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(\n",
    "        rng,\n",
    "        batch_size: tuple,\n",
    "        hidden_size: int,\n",
    "        init_fn=nn.initializers.zeros,\n",
    "    ):\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRNN(nn.Module):\n",
    "    output_size: int\n",
    "    layers: Sequence[Any]\n",
    "    skip_connections: bool\n",
    "    layer_name: Optional[str] = None\n",
    "\n",
    "    def setup(self):\n",
    "        if self.skip_connections:\n",
    "            for layer in self.layers:\n",
    "                if not isinstance(layer, nn.Module):\n",
    "                    raise ValueError(\n",
    "                        \"skip_connections requires for all layers to be \"\n",
    "                        \"`nn.Module. Layers is: {}\".format(self.layers)\n",
    "                    )\n",
    "\n",
    "        self.dense_out = nn.Dense(features=self.output_size)\n",
    "\n",
    "    def __call__(self, carry, input):\n",
    "        out_carry = None\n",
    "        output = None\n",
    "        h_t, c_t = carry\n",
    "        h_t_list = []\n",
    "        c_t_list = []\n",
    "        states = []\n",
    "\n",
    "        for t in range(input.shape[1]):\n",
    "            for idx, layer in enumerate(self.layers):\n",
    "                if isinstance(layer, nn.Module):\n",
    "                    if idx == 0:\n",
    "                        out_carry, output = layer(carry, input[:, t, :])\n",
    "                        h_t, c_t = out_carry\n",
    "\n",
    "                    else:\n",
    "                        h_t_1, c_t_1 = out_carry\n",
    "                        out_carry, output = layer(carry, h_t_1)\n",
    "                        h_t, c_t = out_carry\n",
    "                        if self.skip_connections:\n",
    "                            h_t = jnp.concatenate([h_t, h_t_1], axis=1)\n",
    "                            c_t = jnp.concatenate([c_t, c_t_1], axis=1)\n",
    "                            out_carry = tuple([h_t, c_t])\n",
    "                else:\n",
    "                    out_carry, output = layer(out_carry)\n",
    "\n",
    "                h_t_list.append(h_t)\n",
    "                c_t_list.append(c_t)\n",
    "                states.append(output)\n",
    "\n",
    "            carry = out_carry\n",
    "\n",
    "        next_carry = None\n",
    "        concat = lambda *args: jnp.concatenate(args, axis=-1)\n",
    "        if self.skip_connections:\n",
    "            h_t = jax.tree_map(concat, *h_t_list)\n",
    "            c_t = jax.tree_map(concat, *c_t_list)\n",
    "            next_carry = tuple([h_t, c_t])\n",
    "        else:\n",
    "            next_carry = out_carry\n",
    "\n",
    "        return next_carry, self.dense_out(output)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(\n",
    "        rng,\n",
    "        batch_size: tuple,\n",
    "        hidden_size: int,\n",
    "        init_fn=nn.initializers.zeros,\n",
    "    ):\n",
    "        key1, key2 = jax.random.split(rng)\n",
    "        # mem_shape = batch_size + (input_size, hidden_size)\n",
    "        mem_shape = batch_size + (hidden_size,)\n",
    "        return init_fn(key1, mem_shape), init_fn(key2, mem_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jit, static_argnums=(1,))\n",
    "def moving_window(a, size: int):\n",
    "    starts = jnp.arange(len(a) - size + 1)\n",
    "    return vmap(lambda start: jax.lax.dynamic_slice(a, (start,), (size,)))(starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(a: jnp.ndarray, window: int):\n",
    "    idx = jnp.arange(len(a) - window + 1)[:, None] + jnp.arange(window)[None, :]\n",
    "    return a[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    seed = 1701\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "\n",
    "    num_copies = 4\n",
    "    rng, key, subkey, subsubkey = jax.random.split(key, num=num_copies)\n",
    "\n",
    "    hidden_size = 256\n",
    "\n",
    "    # batch size, sequence length, input size\n",
    "    batch_size = 32\n",
    "    data_size = 28 * 28\n",
    "    input_size = 5\n",
    "\n",
    "    # fake data\n",
    "    x = jax.random.randint(rng, (batch_size, data_size), 0, 244)\n",
    "    x = vmap(moving_window, in_axes=(0, None))(x, input_size)\n",
    "\n",
    "    layer_list = []\n",
    "    num_of_rnns = 3\n",
    "    rnn_type = \"rnn\"\n",
    "    if rnn_type == \"rnn\":\n",
    "        layer_list = [\n",
    "            RNNCell(input_size=input_size, hidden_size=hidden_size)\n",
    "            for _ in range(num_of_rnns)\n",
    "        ]\n",
    "\n",
    "    elif rnn_type == \"lstm\":\n",
    "        layer_list = [\n",
    "            LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
    "            for _ in range(num_of_rnns)\n",
    "        ]\n",
    "\n",
    "    elif rnn_type == \"gru\":\n",
    "        layer_list = [\n",
    "            GRUCell(input_size=input_size, hidden_size=hidden_size)\n",
    "            for _ in range(num_of_rnns)\n",
    "        ]\n",
    "\n",
    "    elif rnn_type == \"hippo\":\n",
    "        layer_list = [\n",
    "            HiPPOCell(input_size=input_size, hidden_size=hidden_size)\n",
    "            for _ in range(num_of_rnns)\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"rnn_type must be one of: rnn, lstm, gru, hippo\")\n",
    "\n",
    "    # model\n",
    "    model = DeepRNN(\n",
    "        output_size=10,\n",
    "        layers=layer_list,\n",
    "        skip_connections=False,\n",
    "    )\n",
    "\n",
    "    # get model params\n",
    "    params = model.init(\n",
    "        key,\n",
    "        model.initialize_carry(\n",
    "            rng=subkey,\n",
    "            batch_size=(batch_size,),\n",
    "            hidden_size=hidden_size,\n",
    "            init_fn=nn.initializers.zeros,\n",
    "        ),\n",
    "        x,\n",
    "    )\n",
    "\n",
    "    carry, out = model.apply(\n",
    "        params,\n",
    "        model.initialize_carry(\n",
    "            rng=subsubkey,\n",
    "            batch_size=(batch_size,),\n",
    "            hidden_size=hidden_size,\n",
    "            init_fn=nn.initializers.zeros,\n",
    "        ),\n",
    "        x,\n",
    "    )\n",
    "\n",
    "    return carry, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester():\n",
    "    for i in range(1):\n",
    "        test_carry, testx = test()\n",
    "        xdims = testx.shape\n",
    "        carrydims = test_carry[0].shape\n",
    "        if i % 10 == 0 or i == 1 or i == 100:\n",
    "            print(f\"output array shape:\\n{xdims}\\n\")\n",
    "            print(f\"h_t array shape:\\n{carrydims}\\n\")\n",
    "        assert xdims == (32, 10)\n",
    "    print(\"Size test: passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n",
    "    ds_builder = tfds.builder(\"mnist\")\n",
    "    ds_builder.download_and_prepare()\n",
    "    train_ds = tfds.as_numpy(ds_builder.as_dataset(split=\"train\", batch_size=-1))\n",
    "    test_ds = tfds.as_numpy(ds_builder.as_dataset(split=\"test\", batch_size=-1))\n",
    "    train_ds[\"image\"] = jnp.float32(train_ds[\"image\"]) / 255.0\n",
    "    test_ds[\"image\"] = jnp.float32(test_ds[\"image\"]) / 255.0\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds, test_ds = get_datasets()\n",
    "# print(train_ds[\"image\"].shape)\n",
    "# print(test_ds[\"image\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_rnn_cell(cfg):\n",
    "    # set rnn cell from rnn_type\n",
    "    rnn_list = []\n",
    "    if cfg[\"models\"][\"cells\"][\"cell_type\"] == \"rnn\":\n",
    "        rnn_list = [\n",
    "            RNNCell(\n",
    "                input_size=cfg[\"models\"][\"cells\"][\"rnn\"][\"input_size\"],\n",
    "                hidden_size=cfg[\"models\"][\"cells\"][\"rnn\"][\"hidden_size\"],\n",
    "                bias=cfg[\"models\"][\"cells\"][\"rnn\"][\"bias\"],\n",
    "                param_dtype=cfg[\"models\"][\"cells\"][\"rnn\"][\"param_dtype\"],\n",
    "                activation_fn=cfg[\"models\"][\"cells\"][\"rnn\"][\"activation_fn\"],\n",
    "            )\n",
    "            for _ in range(cfg[\"models\"][\"deep_rnn\"][\"stack_number\"])\n",
    "        ]\n",
    "\n",
    "    elif cfg[\"models\"][\"cells\"][\"cell_type\"] == \"lstm\":\n",
    "        rnn_list = [\n",
    "            LSTMCell(\n",
    "                input_size=cfg[\"models\"][\"cells\"][\"gated_rnn\"][\"input_size\"],\n",
    "                hidden_size=cfg[\"models\"][\"cells\"][\"gated_rnn\"][\"hidden_size\"],\n",
    "                bias=cfg[\"models\"][\"cells\"][\"gated_rnn\"][\"bias\"],\n",
    "                param_dtype=cfg[\"models\"][\"cells\"][\"gated_rnn\"][\"param_dtype\"],\n",
    "                gate_fn=cfg[\"models\"][\"cells\"][\"gated_rnn\"][\"gate_fn\"],\n",
    "                activation_fn=cfg[\"models\"][\"cells\"][\"gated_rnn\"][\"activation_fn\"],\n",
    "            )\n",
    "            for _ in range(cfg[\"models\"][\"deep_rnn\"][\"stack_number\"])\n",
    "        ]\n",
    "\n",
    "    elif cfg[\"models\"][\"cells\"][\"cell_type\"] == \"gru\":\n",
    "        rnn_list = [\n",
    "            GRUCell(\n",
    "                input_size=cfg[\"models\"][\"cells\"][\"gated_rnn\"][\"input_size\"],\n",
    "                hidden_size=cfg[\"models\"][\"cells\"][\"gated_rnn\"][\"hidden_size\"],\n",
    "                bias=cfg[\"models\"][\"cells\"][\"gated_rnn\"][\"bias\"],\n",
    "                param_dtype=cfg[\"models\"][\"cells\"][\"gated_rnn\"][\"param_dtype\"],\n",
    "                gate_fn=cfg[\"models\"][\"cells\"][\"gated_rnn\"][\"gate_fn\"],\n",
    "                activation_fn=cfg[\"models\"][\"cells\"][\"gated_rnn\"][\"activation_fn\"],\n",
    "            )\n",
    "            for _ in range(cfg[\"models\"][\"deep_rnn\"][\"stack_number\"])\n",
    "        ]\n",
    "\n",
    "    elif cfg[\"models\"][\"cells\"][\"cell_type\"] == \"hippo\":\n",
    "        rnn_list = [\n",
    "            HiPPOCell(\n",
    "                input_size=cfg[\"models\"][\"cells\"][\"hippo\"][\"input_size\"],\n",
    "                hidden_size=cfg[\"models\"][\"cells\"][\"hippo\"][\"hidden_size\"],\n",
    "                bias=cfg[\"models\"][\"cells\"][\"hippo\"][\"bias\"],\n",
    "                param_dtype=cfg[\"models\"][\"cells\"][\"hippo\"][\"param_dtype\"],\n",
    "                gate_fn=cfg[\"models\"][\"cells\"][\"hippo\"][\"gate_fn\"],\n",
    "                activation_fn=cfg[\"models\"][\"cells\"][\"hippo\"][\"activation_fn\"],\n",
    "                measure=cfg[\"models\"][\"cells\"][\"hippo\"][\"measure\"],\n",
    "                lambda_n=cfg[\"models\"][\"cells\"][\"hippo\"][\"lambda_n\"],\n",
    "                fourier_type=cfg[\"models\"][\"cells\"][\"hippo\"][\"fourier_type\"],\n",
    "                alpha=cfg[\"models\"][\"cells\"][\"hippo\"][\"alpha\"],\n",
    "                beta=cfg[\"models\"][\"cells\"][\"hippo\"][\"beta\"],\n",
    "                rnn_cell=cfg[\"models\"][\"cells\"][\"hippo\"][\"rnn_cell\"],\n",
    "            )\n",
    "            for _ in range(cfg[\"models\"][\"deep_rnn\"][\"stack_number\"])\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown rnn type\")\n",
    "\n",
    "    return rnn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_model(key, cfg):\n",
    "    # set model from net_type\n",
    "    model = None\n",
    "    params = None\n",
    "\n",
    "    if cfg[\"models\"][\"model_type\"] == \"rnn\":\n",
    "        rnn_list = pick_rnn_cell(cfg)\n",
    "        model = DeepRNN(\n",
    "            output_size=cfg[\"models\"][\"deep_rnn\"][\"output_size\"],\n",
    "            layers=rnn_list,\n",
    "            skip_connections=cfg[\"models\"][\"deep_rnn\"][\"skip_connections\"],\n",
    "        )\n",
    "        init_carry = model.initialize_carry(\n",
    "            rng=key,\n",
    "            batch_size=(cfg[\"training\"][\"batch_size\"],),\n",
    "            hidden_size=cfg[\"models\"][\"deep_rnn\"][\"hidden_size\"],\n",
    "            init_fn=nn.initializers.zeros,\n",
    "        )\n",
    "        params = model.init(input, init_carry)\n",
    "\n",
    "    elif cfg[\"models\"][\"model_type\"] == \"hippo\":\n",
    "        L = cfg[\"training\"][\"input_length\"]\n",
    "        hippo_matrices = TransMatrix(\n",
    "            N=cfg[\"models\"][\"hippo\"][\"n\"],\n",
    "            measure=cfg[\"models\"][\"hippo\"][\"measure\"],\n",
    "            lambda_n=cfg[\"models\"][\"hippo\"][\"lambda_n\"],\n",
    "            fourier_type=cfg[\"models\"][\"hippo\"][\"fourier_type\"],\n",
    "            alpha=cfg[\"models\"][\"hippo\"][\"alpha\"],\n",
    "            beta=cfg[\"models\"][\"hippo\"][\"beta\"],\n",
    "        )\n",
    "        model = HiPPO(\n",
    "            N=cfg[\"models\"][\"hippo\"][\"n\"],\n",
    "            max_length=L,\n",
    "            step=1.0 / L,\n",
    "            GBT_alpha=cfg[\"models\"][\"hippo\"][\"GBT_alpha\"],\n",
    "            seq_L=L,\n",
    "            A=hippo_matrices.A_matrix,\n",
    "            B=hippo_matrices.B_matrix,\n",
    "            measure=cfg[\"models\"][\"hippo\"][\"measure\"],\n",
    "        )\n",
    "        params = model.init(f, init_state=None, t_step=0, kernel=False)\n",
    "\n",
    "    elif cfg[\"models\"][\"model_type\"] == \"s4\":\n",
    "        raise NotImplementedError\n",
    "        # model = S4()\n",
    "        # params = model.init()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type\")\n",
    "\n",
    "    return model, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(cfg, data):\n",
    "    # preprocess data\n",
    "    x = None\n",
    "    if cfg[\"models\"][\"model_type\"] == \"rnn\":\n",
    "        x = vmap(jnp.ravel, in_axes=0)(x)\n",
    "        x = vmap(moving_window, in_axes=(0, None))(x, cfg[\"training\"][\"input_length\"])\n",
    "\n",
    "    elif cfg[\"models\"][\"model_type\"] == \"hippo\":\n",
    "        raise NotImplementedError\n",
    "\n",
    "    elif cfg[\"models\"][\"model_type\"] == \"s4\":\n",
    "        raise NotImplementedError\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type to preprocess for\")\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(cfg, labels):\n",
    "    # preprocess data\n",
    "    y = None\n",
    "    if cfg[\"models\"][\"model_type\"] == \"rnn\":\n",
    "        y = jax.nn.one_hot(labels, 10)\n",
    "\n",
    "    elif cfg[\"models\"][\"model_type\"] == \"hippo\":\n",
    "        raise NotImplementedError\n",
    "\n",
    "    elif cfg[\"models\"][\"model_type\"] == \"s4\":\n",
    "        raise NotImplementedError\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type to preprocess for\")\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_optim(cfg, model, params):\n",
    "\n",
    "    tx = None\n",
    "    if cfg[\"training\"][\"optimizer\"] == \"adam\":\n",
    "        tx = optax.adam(\n",
    "            learning_rate=cfg[\"training\"][\"learning_rate\"],\n",
    "            weight_decay=cfg[\"training\"][\"weight_decay\"],\n",
    "        )\n",
    "    elif cfg[\"training\"][\"optimizer\"] == \"sgd\":\n",
    "        tx = optax.sgd(learning_rate=cfg[\"training\"][\"learning_rate\"])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown optimizer\")\n",
    "\n",
    "    tx_state = tx.init(params)\n",
    "\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply, params=params, tx=tx, opt_state=tx_state\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def apply_model(state, data, labels):\n",
    "    \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({\"params\": params}, data)\n",
    "        one_hot = jax.nn.one_hot(labels, 10)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    return grads, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1123130/738331274.py:1: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"config\", config_name=\"train\")\n"
     ]
    }
   ],
   "source": [
    "@hydra.main(config_path=\"config\", config_name=\"train\")\n",
    "def recurrent_train(\n",
    "    cfg: DictConfig,\n",
    ") -> None:  # num_epochs, opt_state, net_type=\"RNN\", train_key=None):\n",
    "    \"\"\"\n",
    "    Implements a learning loop over epochs.\n",
    "\n",
    "    Args:\n",
    "        cfg: Hydra config\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    with wandb.init(\n",
    "        project=\"BeeGass-Sequential\", entity=\"beegass\", config=cfg\n",
    "    ):  # initialize wandb project for logging\n",
    "\n",
    "        # get keys for parameters\n",
    "        seed = cfg[\"training\"][\"seed\"]\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "\n",
    "        num_copies = cfg[\"training\"][\"key_num\"]\n",
    "        keys = jax.random.split(key, num=num_copies)\n",
    "\n",
    "        # get train and test datasets\n",
    "        train_set, test_set = get_datasets()\n",
    "\n",
    "        # pick a model\n",
    "        model, params = pick_model(keys[1], cfg)\n",
    "\n",
    "        # pick an optimizer\n",
    "        state = pick_optim(cfg, model, params)\n",
    "\n",
    "        # pick a scheduler\n",
    "        # TODO: implement choice of scheduler\n",
    "\n",
    "        # pick a loss function\n",
    "        # TODO: implement choice of loss function\n",
    "\n",
    "        # get dataset info for training loop (number of steps per epoch)\n",
    "        train_set_size = len(train_set[\"image\"])\n",
    "        steps_per_epoch = train_set_size // cfg[\"training\"][\"batch_size\"]\n",
    "\n",
    "        perms = jax.random.permutation(keys[0], train_set_size)\n",
    "        perms = perms[\n",
    "            : steps_per_epoch * cfg[\"training\"][\"batch_size\"]\n",
    "        ]  # skip incomplete batch\n",
    "        perms = perms.reshape((steps_per_epoch, cfg[\"training\"][\"batch_size\"]))\n",
    "\n",
    "        epoch_loss = []\n",
    "        epoch_accuracy = []\n",
    "\n",
    "        # Loop over the training epochs\n",
    "        for epoch in range(cfg[\"training\"][\"num_epochs\"]):\n",
    "            # start_time = time.time()\n",
    "\n",
    "            for perm in perms:\n",
    "                train_data = train_set[\"image\"][perm, ...]\n",
    "                train_labels = train_set[\"label\"][perm, ...]\n",
    "                train_data = preprocess_data(cfg, train_data)\n",
    "                # train_labels = preprocess_labels(cfg, train_labels)\n",
    "                grads, loss, accuracy = apply_model(\n",
    "                    state=state, data=train_data, labels=train_labels\n",
    "                )\n",
    "                state = update_model(state, grads)\n",
    "                epoch_loss.append(loss)\n",
    "                epoch_accuracy.append(accuracy)\n",
    "\n",
    "            # epoch_time = time.time() - start_time\n",
    "\n",
    "            # train loss for current epoch\n",
    "            train_loss = jnp.mean(epoch_loss)\n",
    "            train_accuracy = jnp.mean(epoch_accuracy)\n",
    "\n",
    "            # test loss for current epoch\n",
    "            _, test_loss, test_accuracy = apply_model(\n",
    "                state=state, data=test_set[\"image\"], labels=test_set[\"label\"]\n",
    "            )\n",
    "\n",
    "            # TODO: add logging of metrics\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: (32, 28, 28)\n",
      "raveled image shape: (32, 784)\n",
      "windowed raveled image shape: (32, 780, 5)\n"
     ]
    }
   ],
   "source": [
    "# fake data\n",
    "height_dim = 28\n",
    "width_dim = 28\n",
    "batch_size = 32\n",
    "input_size = 5\n",
    "x = jax.random.randint(\n",
    "    jax.random.PRNGKey(1701), (batch_size, height_dim, width_dim), 0, 255\n",
    ")\n",
    "print(f\"image shape: {x.shape}\")\n",
    "x = vmap(jnp.ravel, in_axes=0)(x)\n",
    "print(f\"raveled image shape: {x.shape}\")\n",
    "x = vmap(moving_window, in_axes=(0, None))(x, input_size)\n",
    "print(f\"windowed raveled image shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [--help] [--hydra-help] [--version]\n",
      "                             [--cfg {job,hydra,all}] [--resolve]\n",
      "                             [--package PACKAGE] [--run] [--multirun]\n",
      "                             [--shell-completion] [--config-path CONFIG_PATH]\n",
      "                             [--config-name CONFIG_NAME]\n",
      "                             [--config-dir CONFIG_DIR]\n",
      "                             [--experimental-rerun EXPERIMENTAL_RERUN]\n",
      "                             [--info [{all,config,defaults,defaults-tree,plugins,searchpath}]]\n",
      "                             [overrides [overrides ...]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.cache/pypoetry/virtualenvs/s4mer-pkg-jZnBSgjq-py3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "state = recurrent_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check [this](https://github.com/deepmind/optax/blob/master/examples/quick_start.ipynb) out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to [this](https://github.com/manifest/flax-extra/blob/48efe1f1515893289b44646977bf5049a340b6c8/docs/notebooks/combinators.ipynb), [this](https://github.com/romanak/pyprobml/blob/65c82b9b43d2100cbc7c59e766161ee801c0f85f/notebooks/book1/15/rnn_jax.ipynb), [this](https://github.com/probml/pyprobml/blob/71d98dcdd3798525353eb1bfb9851b47e9d64bde/notebooks/book1/15/rnn_jax.ipynb) and [this](https://github.com/probml/probml-notebooks/blob/36cb173afce3f4a07a7b475cf8a7937025a60465/notebooks-d2l/rnn_jax.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('s4mer-pkg-jZnBSgjq-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a81e05d1d7f7eae781698b7c1b81c0d771335201ebad1d81045cb177cef974b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
